{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68750c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path=\"/mnt/e/env/ts/datas/data/data_long/ft_normal/bingo5/by_unique_id/N1.csv\"\n",
    "df = pd.read_csv(path)  \n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "futr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# NeuralForecast Auto(TFT / PatchTST) フル実装（seed修正）\n",
    "# ================================================\n",
    "import os, json, time, warnings, inspect, random\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoTFT, AutoPatchTST\n",
    "from neuralforecast.losses.pytorch import SMAPE\n",
    "\n",
    "# ---------------------------\n",
    "# ユーザ指定パラメータ\n",
    "# ---------------------------\n",
    "DATA_CSV = \"/mnt/e/env/ts/datas/data/data_long/ft_normal/bingo5/by_unique_id/N1.csv\"\n",
    "TRIALS   = 1\n",
    "SEED     = 1029\n",
    "H        = 1\n",
    "ARTIFACTS_ROOT = \"/mnt/e/env/ts/ts-mlops-skeleton/nf_auto_runs\"\n",
    "FREQ = \"D\"\n",
    "AUTO_GENERATE_FUTR_EXOG = False\n",
    "\n",
    "# --- 再現性（Lightning/torch/np/python を一括固定）---\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "# =========================================================\n",
    "# ユーティリティ\n",
    "# =========================================================\n",
    "def ensure_dir(path: str) -> str:\n",
    "    os.makedirs(path, exist_ok=True); return path\n",
    "\n",
    "def now_str():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def to_datetime_col(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    out = df.copy(); out[col] = pd.to_datetime(out[col], errors=\"coerce\"); return out\n",
    "\n",
    "def to_numeric_col(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    out = df.copy(); out[col] = pd.to_numeric(out[col], errors=\"coerce\"); return out\n",
    "\n",
    "def preprocess_df(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw.copy()\n",
    "    if \"unique_id\" not in df.columns:\n",
    "        df[\"unique_id\"] = \"series_0\"\n",
    "    df = to_datetime_col(df, \"ds\")\n",
    "    df = to_numeric_col(df, \"y\")\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"ds\", \"y\"])\n",
    "    dropped = before - len(df)\n",
    "    if dropped > 0:\n",
    "        print(f\"[info] 前処理: ds/y の変換で {dropped} 行を除去しました。\")\n",
    "    df = df.sort_values([\"unique_id\", \"ds\"])\n",
    "    df = df.drop_duplicates(subset=[\"unique_id\", \"ds\"], keep=\"last\").reset_index(drop=True)\n",
    "    gap = df.groupby(\"unique_id\")[\"ds\"].diff().dropna()\n",
    "    if not gap.empty and (gap != pd.Timedelta(days=1)).any():\n",
    "        warnings.warn(\"日次ギャップが検出されました。\")\n",
    "    return df\n",
    "\n",
    "def split_exog_by_prefix(df: pd.DataFrame):\n",
    "    futr_cols = [c for c in df.columns if c.startswith(\"futr_\")]\n",
    "    hist_cols = [c for c in df.columns if c.startswith(\"hist_\")]\n",
    "    stat_cols = [c for c in df.columns if c.startswith(\"stat_\")]\n",
    "    return futr_cols, hist_cols, stat_cols\n",
    "\n",
    "# ---- 指標 ----\n",
    "def _to_arr(x): return np.asarray(x, dtype=float)\n",
    "def smape(y, yhat, eps=1e-8): y=_to_arr(y);yhat=_to_arr(yhat);return 100*np.mean(2*np.abs(yhat-y)/(np.abs(y)+np.abs(yhat)+eps))\n",
    "def mae(y, yhat): y=_to_arr(y);yhat=_to_arr(yhat);return float(np.mean(np.abs(yhat-y)))\n",
    "def mape(y, yhat, eps=1e-8): y=_to_arr(y);yhat=_to_arr(yhat);den=np.clip(np.abs(y),eps,None);return 100*np.mean(np.abs((yhat-y)/den))\n",
    "def rmse(y, yhat): y=_to_arr(y);yhat=_to_arr(yhat);return float(np.sqrt(np.mean((yhat-y)**2)))\n",
    "\n",
    "def compute_metrics(df_pred: pd.DataFrame, model_cols):\n",
    "    rows=[]\n",
    "    for m in model_cols:\n",
    "        y=df_pred[\"y\"].values; yhat=df_pred[m].values\n",
    "        rows.append({\"model\":m,\"SMAPE\":smape(y,yhat),\"MAE\":mae(y,yhat),\"MAPE\":mape(y,yhat),\"RMSE\":rmse(y,yhat),\"n\":len(df_pred)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_last_window(cv_df: pd.DataFrame, model_cols, out_png: str, uid: str = None):\n",
    "    if \"cutoff\" not in cv_df.columns:\n",
    "        print(\"[warn] cutoff がないため描画スキップ\"); return\n",
    "    last_cutoff = cv_df[\"cutoff\"].max()\n",
    "    sub = cv_df[cv_df[\"cutoff\"] == last_cutoff]\n",
    "    if uid is not None:\n",
    "        pick = sub[sub[\"unique_id\"] == uid]\n",
    "        if not pick.empty: sub = pick\n",
    "    sub = sub.sort_values([\"unique_id\",\"ds\"])\n",
    "    if sub[\"unique_id\"].nunique() > 1:\n",
    "        sub = sub[sub[\"unique_id\"] == sub[\"unique_id\"].iloc[0]]\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(sub[\"ds\"], sub[\"y\"], label=\"y\", linewidth=2)\n",
    "    for m in model_cols: plt.plot(sub[\"ds\"], sub[m], label=m, linewidth=1)\n",
    "    plt.title(f\"Last CV Window @ cutoff={last_cutoff}\")\n",
    "    plt.xlabel(\"ds\"); plt.ylabel(\"value\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
    "    print(f\"[info] 予測可視化を保存: {out_png}\")\n",
    "\n",
    "# =========================================================\n",
    "# 1) データ読込 & 前処理\n",
    "# =========================================================\n",
    "print(f\"[info] CSV を読込中: {DATA_CSV}\")\n",
    "raw = pd.read_csv(DATA_CSV)\n",
    "print(f\"[info] loaded shape: {raw.shape}\")\n",
    "print(f\"[info] columns: {list(raw.columns)[:10]} ...\")\n",
    "\n",
    "df = preprocess_df(raw)\n",
    "futr_cols, hist_cols, stat_cols = split_exog_by_prefix(df)\n",
    "print(f\"[info] exog sizes (raw): futr={len(futr_cols)} hist={len(hist_cols)} stat={len(stat_cols)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 2) static_df 構築（非数値→factorize）＆ df から stat_* を削除\n",
    "# =========================================================\n",
    "static_df = None\n",
    "if len(stat_cols):\n",
    "    static_df = df[[\"unique_id\", *stat_cols]].drop_duplicates()\n",
    "    for c in stat_cols:\n",
    "        if not np.issubdtype(static_df[c].dtype, np.number):\n",
    "            codes, uniques = pd.factorize(static_df[c], sort=True)\n",
    "            static_df[c] = codes.astype(np.int32)\n",
    "            print(f\"[info] static_df: 非数値列をコード化 -> {c} （{len(uniques)}カテゴリ）\")\n",
    "    df = df.drop(columns=stat_cols)\n",
    "    print(f\"[info] df から stat_* を削除: {len(stat_cols)} 列\")\n",
    "\n",
    "# 最新の futr/hist を df から再抽出（stat は df に存在しない）\n",
    "futr_cols, hist_cols, _ = split_exog_by_prefix(df)\n",
    "print(f\"[info] exog sizes (after drop stat): futr={len(futr_cols)} hist={len(hist_cols)} stat={len(stat_cols)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 3) futr_/hist_ の非数値対策（非数値は強制数値化 or 全欠損ならドロップ）\n",
    "# =========================================================\n",
    "protected = {\"unique_id\",\"ds\",\"y\"}\n",
    "temporal_cols = [c for c in df.columns if c not in protected]\n",
    "drop_cols=[]\n",
    "for c in temporal_cols:\n",
    "    if not np.issubdtype(df[c].dtype, np.number):\n",
    "        coerced = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if coerced.isna().all():\n",
    "            drop_cols.append(c)\n",
    "        else:\n",
    "            df[c] = coerced.fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0.0)\n",
    "            print(f\"[info] temporal: {c} を数値化（欠損は前後詰め→0）\")\n",
    "if drop_cols:\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    print(f\"[info] temporal: 数値化不能のためドロップ -> {drop_cols}\")\n",
    "\n",
    "# 存在確認で最終リスト\n",
    "futr_cols = [c for c in futr_cols if c in df.columns]\n",
    "hist_cols = [c for c in hist_cols if c in df.columns]\n",
    "print(f\"[info] exog sizes (final): futr={len(futr_cols)} hist={len(hist_cols)} stat={len(stat_cols)}\")\n",
    "print(f\"[info] 学習に使う futr = {len(futr_cols)}, hist = {len(hist_cols)}, stat = {len(stat_cols)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4) Auto モデル設定（config(trial) に外生を渡す）※ seedは入れない\n",
    "# =========================================================\n",
    "def tft_config(trial):\n",
    "    return {\n",
    "        \"input_size\":   trial.suggest_categorical(\"input_size\",   [H*3, H*4, H*5]),\n",
    "        \"hidden_size\":  trial.suggest_categorical(\"hidden_size\",  [64, 128]),\n",
    "        \"dropout\":      trial.suggest_float(\"dropout\", 0.0, 0.2),\n",
    "        \"learning_rate\":trial.suggest_float(\"learning_rate\", 1e-4, 3e-3, log=True),\n",
    "        \"max_steps\":    400,\n",
    "        \"futr_exog_list\": futr_cols,\n",
    "        \"hist_exog_list\": hist_cols,\n",
    "        \"stat_exog_list\": stat_cols,  # static_df と対応\n",
    "        # ここに \"seed\": SEED を入れないこと（Lightning Trainerに渡ってエラー化）\n",
    "    }\n",
    "\n",
    "def patchtst_config(trial):\n",
    "    return {\n",
    "        \"input_size\":   trial.suggest_categorical(\"input_size\", [H*3, H*4, H*5]),\n",
    "        \"patch_len\":    trial.suggest_categorical(\"patch_len\",  [7, 14, 21, 28]),\n",
    "        \"stride\":       trial.suggest_categorical(\"stride\",     [1, 2, 4]),\n",
    "        \"n_layers\":     trial.suggest_categorical(\"n_layers\",   [2, 3]),\n",
    "        \"d_model\":      trial.suggest_categorical(\"d_model\",    [64, 128]),\n",
    "        \"n_heads\":      trial.suggest_categorical(\"n_heads\",    [2, 4]),\n",
    "        \"d_ff\":         trial.suggest_categorical(\"d_ff\",       [256, 512, 1024]),\n",
    "        \"dropout\":      trial.suggest_float(\"dropout\", 0.0, 0.2),\n",
    "        \"learning_rate\":trial.suggest_float(\"learning_rate\", 1e-4, 3e-3, log=True),\n",
    "        \"max_steps\":    400,\n",
    "        \"futr_exog_list\": futr_cols,\n",
    "        \"hist_exog_list\": hist_cols,\n",
    "        \"stat_exog_list\": stat_cols,\n",
    "        # ここも seed を入れない\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 5) trials の渡し先を環境で自動切替\n",
    "# =========================================================\n",
    "fit_accepts_n_trials = \"n_trials\" in inspect.signature(NeuralForecast.fit).parameters\n",
    "\n",
    "if fit_accepts_n_trials:\n",
    "    models = [\n",
    "        AutoTFT(h=H, loss=SMAPE(), backend=\"optuna\", config=tft_config, verbose=True),\n",
    "        AutoPatchTST(h=H, loss=SMAPE(), backend=\"optuna\", config=patchtst_config, verbose=True),\n",
    "    ]\n",
    "    nf = NeuralForecast(models=models, freq=FREQ)\n",
    "    print(f\"[info] 学習開始: n_trials={TRIALS}, val_size={H*2}, freq={FREQ}（fit に n_trials）\")\n",
    "    nf.fit(df=df, static_df=static_df, val_size=H*2, n_trials=TRIALS)\n",
    "else:\n",
    "    models = [\n",
    "        AutoTFT(h=H, loss=SMAPE(), backend=\"optuna\", config=tft_config, num_samples=TRIALS, verbose=True),\n",
    "        AutoPatchTST(h=H, loss=SMAPE(), backend=\"optuna\", config=patchtst_config, num_samples=TRIALS, verbose=True),\n",
    "    ]\n",
    "    nf = NeuralForecast(models=models, freq=FREQ)\n",
    "    print(f\"[info] 学習開始: num_samples={TRIALS}, val_size={H*2}, freq={FREQ}（__init__ に num_samples）\")\n",
    "    nf.fit(df=df, static_df=static_df, val_size=H*2)\n",
    "\n",
    "print(\"[info] 学習完了\")\n",
    "\n",
    "# =========================================================\n",
    "# 6) 成果物保存\n",
    "# =========================================================\n",
    "run_dir   = ensure_dir(os.path.join(ARTIFACTS_ROOT, f\"run_{now_str()}\"))\n",
    "models_dir= ensure_dir(os.path.join(run_dir, \"models\"))\n",
    "plots_dir = ensure_dir(os.path.join(run_dir, \"plots\"))\n",
    "tables_dir= ensure_dir(os.path.join(run_dir, \"tables\"))\n",
    "\n",
    "meta = {\n",
    "    \"DATA_CSV\": DATA_CSV, \"TRIALS\": TRIALS, \"SEED\": SEED, \"H\": H, \"FREQ\": FREQ,\n",
    "    \"futr_exog_list\": futr_cols, \"hist_exog_list\": hist_cols, \"stat_exog_list\": stat_cols,\n",
    "    \"models\": [m.__class__.__name__ for m in models],\n",
    "    \"fit_accepts_n_trials\": fit_accepts_n_trials,\n",
    "}\n",
    "with open(os.path.join(run_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "nf.save(models_dir)\n",
    "print(f\"[info] モデルを保存しました: {models_dir}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7) ロード確認\n",
    "# =========================================================\n",
    "nf_loaded = NeuralForecast.load(models_dir)\n",
    "print(\"[info] モデルをロードしました。\")\n",
    "\n",
    "# =========================================================\n",
    "# 8) CV → 指標・可視化\n",
    "# =========================================================\n",
    "print(\"[info] 交差検証（Rolling Origin）を実行します...\")\n",
    "try:\n",
    "    cv_df = nf_loaded.cross_validation(df=df, static_df=static_df, n_windows=3, step_size=H, h=H)\n",
    "except TypeError:\n",
    "    cv_df = nf_loaded.cross_validation(df=df, static_df=static_df, n_windows=3, step_size=H, max_horizon=H)\n",
    "\n",
    "fixed_cols = {\"unique_id\",\"ds\",\"y\",\"cutoff\"}\n",
    "model_cols = [c for c in cv_df.columns if c not in fixed_cols]\n",
    "cv_path = os.path.join(tables_dir, \"cv_predictions.csv\")\n",
    "cv_df.to_csv(cv_path, index=False); print(f\"[info] CV 予測テーブルを保存: {cv_path}\")\n",
    "\n",
    "metrics_df = compute_metrics(cv_df, model_cols)\n",
    "metrics_path = os.path.join(tables_dir, \"metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False); print(f\"[info] 評価指標を保存: {metrics_path}\")\n",
    "print(metrics_df)\n",
    "\n",
    "plot_last_window(cv_df, model_cols, out_png=os.path.join(plots_dir, \"last_window.png\"))\n",
    "\n",
    "# =========================================================\n",
    "# 9) 将来予測（任意）\n",
    "# =========================================================\n",
    "if AUTO_GENERATE_FUTR_EXOG:\n",
    "    pass\n",
    "\n",
    "print(f\"[done] 成果物ルート: {run_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# NeuralForecast Auto(TFT / PatchTST) 最小設定版\n",
    "# ================================================\n",
    "import os, json, time, warnings, inspect, random, math\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- OOM 断片化対策（torch import 前に）----\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoTFT, AutoPatchTST\n",
    "from neuralforecast.losses.pytorch import SMAPE\n",
    "\n",
    "# ---------------------------\n",
    "# ユーザ指定パラメータ\n",
    "# ---------------------------\n",
    "DATA_CSV = \"/mnt/e/env/ts/datas/data/data_long/ft_normal/bingo5/by_unique_id/N1.csv\"\n",
    "TRIALS   = 16\n",
    "SEED     = 1029\n",
    "H        = 28\n",
    "ARTIFACTS_ROOT = \"/mnt/e/env/ts/ts-mlops-skeleton/nf_auto_runs\"\n",
    "FREQ = \"D\"\n",
    "\n",
    "# ---------------------------\n",
    "# Auto設定パラメータ（最小構成）\n",
    "# ---------------------------\n",
    "LOSS = SMAPE()\n",
    "VALID_LOSS = None\n",
    "SEARCH_ALG = \"TPESampler\"  # Optuna: TPESampler, Ray: BasicVariantGenerator\n",
    "BACKEND = \"optuna\"\n",
    "CALLBACKS = None\n",
    "LOCAL_SCALER_TYPE = \"standard\"\n",
    "EARLY_STOP_PATIENCE_STEPS = 20  # 適切な早期停止（20ステップで改善なければ停止）\n",
    "VERBOSE = True\n",
    "\n",
    "# リソース設定\n",
    "CPUS = 4\n",
    "GPUS = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "# 外生変数の最大数（省メモリ）\n",
    "TOPK_HIST = 32\n",
    "TOPK_FUTR = 16\n",
    "\n",
    "# ---- 再現性固定 ----\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "# ---- 便利関数 ----\n",
    "def ensure_dir(path: str) -> str:\n",
    "    os.makedirs(path, exist_ok=True); return path\n",
    "\n",
    "def now_str():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def to_datetime_col(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    out = df.copy(); out[col] = pd.to_datetime(out[col], errors=\"coerce\"); return out\n",
    "\n",
    "def to_numeric_col(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    out = df.copy(); out[col] = pd.to_numeric(out[col], errors=\"coerce\"); return out\n",
    "\n",
    "def preprocess_df(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw.copy()\n",
    "    if \"unique_id\" not in df.columns:\n",
    "        df[\"unique_id\"] = \"series_0\"\n",
    "    df = to_datetime_col(df, \"ds\")\n",
    "    df = to_numeric_col(df, \"y\")\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"ds\", \"y\"])\n",
    "    if before - len(df) > 0:\n",
    "        print(f\"[info] 前処理: ds/y の変換で {before-len(df)} 行を除去しました。\")\n",
    "    df = df.sort_values([\"unique_id\", \"ds\"]).drop_duplicates(subset=[\"unique_id\",\"ds\"], keep=\"last\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    gap = df.groupby(\"unique_id\")[\"ds\"].diff().dropna()\n",
    "    if not gap.empty and (gap != pd.Timedelta(days=1)).any():\n",
    "        warnings.warn(\"日次ギャップが検出されました。\")\n",
    "    return df\n",
    "\n",
    "def split_exog_by_prefix(df: pd.DataFrame):\n",
    "    futr_cols = [c for c in df.columns if c.startswith(\"futr_\")]\n",
    "    hist_cols = [c for c in df.columns if c.startswith(\"hist_\")]\n",
    "    stat_cols = [c for c in df.columns if c.startswith(\"stat_\")]\n",
    "    return futr_cols, hist_cols, stat_cols\n",
    "\n",
    "def factorize_static(static_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = static_df.copy()\n",
    "    for c in out.columns:\n",
    "        if c == \"unique_id\": \n",
    "            continue\n",
    "        if not np.issubdtype(out[c].dtype, np.number):\n",
    "            codes, uniques = pd.factorize(out[c], sort=True)\n",
    "            out[c] = codes.astype(np.int32)\n",
    "            print(f\"[info] static_df: 非数値列をコード化 -> {c} （{len(uniques)}カテゴリ）\")\n",
    "    return out\n",
    "\n",
    "def coerce_temporal_numeric(df: pd.DataFrame, protected=(\"unique_id\",\"ds\",\"y\")):\n",
    "    temporal_cols = [c for c in df.columns if c not in protected]\n",
    "    drop_cols=[]\n",
    "    for c in temporal_cols:\n",
    "        if not np.issubdtype(df[c].dtype, np.number):\n",
    "            coerced = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if coerced.isna().all():\n",
    "                drop_cols.append(c)\n",
    "            else:\n",
    "                df[c] = coerced.fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0.0)\n",
    "                print(f\"[info] temporal: {c} を数値化（欠損は前後詰め→0）\")\n",
    "    if drop_cols:\n",
    "        df.drop(columns=drop_cols, inplace=True)\n",
    "        print(f\"[info] temporal: 数値化不能のためドロップ -> {drop_cols}\")\n",
    "    return df\n",
    "\n",
    "def select_topk_features(df: pd.DataFrame, feature_cols, k: int, target_col=\"y\", min_std=1e-12):\n",
    "    if k <= 0 or len(feature_cols) == 0:\n",
    "        return []\n",
    "    std = df[feature_cols].std(numeric_only=True)\n",
    "    keep = std[std > min_std].index.tolist()\n",
    "    if not keep:\n",
    "        return []\n",
    "    corrs = {}\n",
    "    y = df[target_col].astype(float)\n",
    "    for c in keep:\n",
    "        x = df[c].astype(float)\n",
    "        valid = x.notna() & y.notna()\n",
    "        if valid.sum() < 3:\n",
    "            continue\n",
    "        xc = x[valid]; yc = y[valid]\n",
    "        if xc.std() < min_std:\n",
    "            continue\n",
    "        corrs[c] = float(abs(np.corrcoef(xc, yc)[0,1]))\n",
    "    if not corrs:\n",
    "        return keep[:k]\n",
    "    ranked = sorted(corrs.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    picked = [c for c,_ in ranked[:k]]\n",
    "    if len(picked) < k:\n",
    "        rest = [c for c in keep if c not in picked]\n",
    "        picked += rest[:(k-len(picked))]\n",
    "    return picked\n",
    "\n",
    "def lightning_precision():\n",
    "    try:\n",
    "        major = int(pl.__version__.split(\".\")[0])\n",
    "    except Exception:\n",
    "        major = 2\n",
    "    return \"16-mixed\" if major >= 2 else 16\n",
    "\n",
    "def plot_last_window(cv_df: pd.DataFrame, model_cols, out_png: str, uid: str = None):\n",
    "    if \"cutoff\" not in cv_df.columns:\n",
    "        print(\"[warn] cutoff がないため描画スキップ\"); return\n",
    "    last_cutoff = cv_df[\"cutoff\"].max()\n",
    "    sub = cv_df[cv_df[\"cutoff\"] == last_cutoff]\n",
    "    if uid is not None:\n",
    "        pick = sub[sub[\"unique_id\"] == uid]\n",
    "        if not pick.empty: sub = pick\n",
    "    sub = sub.sort_values([\"unique_id\",\"ds\"])\n",
    "    if sub[\"unique_id\"].nunique() > 1:\n",
    "        sub = sub[sub[\"unique_id\"] == sub[\"unique_id\"].iloc[0]]\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(sub[\"ds\"], sub[\"y\"], label=\"y\", linewidth=2)\n",
    "    for m in model_cols:\n",
    "        plt.plot(sub[\"ds\"], sub[m], label=m, linewidth=1)\n",
    "    plt.title(f\"Last CV Window @ cutoff={last_cutoff}\")\n",
    "    plt.xlabel(\"ds\"); plt.ylabel(\"value\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
    "    print(f\"[info] 予測可視化を保存: {out_png}\")\n",
    "\n",
    "# =========================================================\n",
    "# 1) データ読込 & 前処理\n",
    "# =========================================================\n",
    "print(f\"[info] CSV を読込中: {DATA_CSV}\")\n",
    "raw = pd.read_csv(DATA_CSV)\n",
    "print(f\"[info] loaded shape: {raw.shape}\")\n",
    "print(f\"[info] columns: {list(raw.columns)[:10]} ...\")\n",
    "\n",
    "df = preprocess_df(raw)\n",
    "futr_cols, hist_cols, stat_cols = split_exog_by_prefix(df)\n",
    "print(f\"[info] exog sizes (raw): futr={len(futr_cols)} hist={len(hist_cols)} stat={len(stat_cols)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 2) static_df 構築（非数値→factorize）＆ df から stat_* を削除\n",
    "# =========================================================\n",
    "static_df = None\n",
    "if len(stat_cols):\n",
    "    static_df = df[[\"unique_id\", *stat_cols]].drop_duplicates()\n",
    "    static_df = factorize_static(static_df)\n",
    "    df = df.drop(columns=stat_cols)\n",
    "    print(f\"[info] df から stat_* を削除: {len(stat_cols)} 列\")\n",
    "\n",
    "futr_cols, hist_cols, _ = split_exog_by_prefix(df)\n",
    "\n",
    "# =========================================================\n",
    "# 3) 時系列側の非数値→数値化\n",
    "# =========================================================\n",
    "df = coerce_temporal_numeric(df)\n",
    "\n",
    "futr_cols = [c for c in futr_cols if c in df.columns]\n",
    "hist_cols = [c for c in hist_cols if c in df.columns]\n",
    "print(f\"[info] exog sizes (final): futr={len(futr_cols)} hist={len(hist_cols)} stat={(0 if static_df is None else static_df.shape[1]-1)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4) 外生 Top-K 選抜（省メモリ）\n",
    "# =========================================================\n",
    "hist_sel = select_topk_features(df, hist_cols, k=TOPK_HIST, target_col=\"y\")\n",
    "futr_sel = select_topk_features(df, futr_cols, k=TOPK_FUTR, target_col=\"y\")\n",
    "print(f\"[info] 選抜: hist={len(hist_sel)}/{len(hist_cols)} → {len(hist_sel)} 列使用\")\n",
    "print(f\"[info] 選抜: futr={len(futr_sel)}/{len(futr_cols)} → {len(futr_sel)} 列使用\")\n",
    "\n",
    "stat_sel = [] if static_df is None else [c for c in static_df.columns if c!=\"unique_id\"]\n",
    "\n",
    "# =========================================================\n",
    "# 5) Auto モデル設定（最小構成）\n",
    "# =========================================================\n",
    "prec = lightning_precision()\n",
    "trainer_kwargs = dict(\n",
    "    accelerator=\"gpu\" if GPUS > 0 else \"cpu\",\n",
    "    devices=GPUS if GPUS > 0 else None,\n",
    "    precision=prec,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=VERBOSE,\n",
    ")\n",
    "\n",
    "# AutoModelの設定は最小限に（h以外）\n",
    "def tft_config(trial):\n",
    "    return {\n",
    "        \"loss\": LOSS,\n",
    "        \"valid_loss\": VALID_LOSS,\n",
    "        \"scaler_type\": LOCAL_SCALER_TYPE,\n",
    "        \"early_stop_patience_steps\": EARLY_STOP_PATIENCE_STEPS,\n",
    "        \"futr_exog_list\": futr_sel,\n",
    "        \"hist_exog_list\": hist_sel,\n",
    "        \"stat_exog_list\": stat_sel,\n",
    "        \"trainer_kwargs\": trainer_kwargs,\n",
    "    }\n",
    "\n",
    "def patchtst_config(trial):\n",
    "    return {\n",
    "        \"loss\": LOSS,\n",
    "        \"valid_loss\": VALID_LOSS,\n",
    "        \"scaler_type\": LOCAL_SCALER_TYPE,\n",
    "        \"early_stop_patience_steps\": EARLY_STOP_PATIENCE_STEPS,\n",
    "        \"futr_exog_list\": futr_sel,\n",
    "        \"hist_exog_list\": hist_sel,\n",
    "        \"stat_exog_list\": stat_sel,\n",
    "        \"trainer_kwargs\": trainer_kwargs,\n",
    "    }\n",
    "\n",
    "# NF の fit に n_trials があるかどうかで分岐\n",
    "fit_accepts_n_trials = \"n_trials\" in inspect.signature(NeuralForecast.fit).parameters\n",
    "\n",
    "def build_nf(trials=TRIALS):\n",
    "    if fit_accepts_n_trials:\n",
    "        models = [\n",
    "            AutoTFT(\n",
    "                h=H,  # 必須引数\n",
    "                backend=BACKEND,\n",
    "                config=tft_config,\n",
    "                search_alg=SEARCH_ALG,\n",
    "                callbacks=CALLBACKS,\n",
    "                verbose=VERBOSE\n",
    "            ),\n",
    "            AutoPatchTST(\n",
    "                h=H,  # 必須引数\n",
    "                backend=BACKEND,\n",
    "                config=patchtst_config,\n",
    "                search_alg=SEARCH_ALG,\n",
    "                callbacks=CALLBACKS,\n",
    "                verbose=VERBOSE\n",
    "            ),\n",
    "        ]\n",
    "        nf = NeuralForecast(models=models, freq=FREQ)\n",
    "        return nf, dict(n_trials=trials)\n",
    "    else:\n",
    "        models = [\n",
    "            AutoTFT(\n",
    "                h=H,  # 必須引数\n",
    "                backend=BACKEND,\n",
    "                config=tft_config,\n",
    "                num_samples=trials,\n",
    "                search_alg=SEARCH_ALG,\n",
    "                callbacks=CALLBACKS,\n",
    "                verbose=VERBOSE\n",
    "            ),\n",
    "            AutoPatchTST(\n",
    "                h=H,  # 必須引数\n",
    "                backend=BACKEND,\n",
    "                config=patchtst_config,\n",
    "                num_samples=trials,\n",
    "                search_alg=SEARCH_ALG,\n",
    "                callbacks=CALLBACKS,\n",
    "                verbose=VERBOSE\n",
    "            ),\n",
    "        ]\n",
    "        nf = NeuralForecast(models=models, freq=FREQ)\n",
    "        return nf, {}\n",
    "\n",
    "# =========================================================\n",
    "# 6) 学習（OOM 時は自動フォールバック）\n",
    "# =========================================================\n",
    "def try_fit(nf, fit_kwargs):\n",
    "    print(f\"[info] 学習開始: trials={TRIALS}, val_size={H*2}, freq={FREQ}\")\n",
    "    print(f\"[info] early_stop={EARLY_STOP_PATIENCE_STEPS}, scaler={LOCAL_SCALER_TYPE}\")\n",
    "    if fit_accepts_n_trials:\n",
    "        nf.fit(df=df, static_df=static_df, val_size=H*2, **fit_kwargs)\n",
    "    else:\n",
    "        nf.fit(df=df, static_df=static_df, val_size=H*2)\n",
    "\n",
    "nf, fit_kwargs = build_nf(TRIALS)\n",
    "try:\n",
    "    try_fit(nf, fit_kwargs)\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if \"CUDA out of memory\" in msg or isinstance(e, torch.cuda.OutOfMemoryError):\n",
    "        print(\"[warn] CUDA OOM 検出。縮小構成でリトライします...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # フォールバック: 外生変数をさらに削減\n",
    "        TOPK_HIST_FALLBACK = 16\n",
    "        TOPK_FUTR_FALLBACK = 8\n",
    "        hist_sel = select_topk_features(df, hist_cols, k=TOPK_HIST_FALLBACK, target_col=\"y\")\n",
    "        futr_sel = select_topk_features(df, futr_cols, k=TOPK_FUTR_FALLBACK, target_col=\"y\")\n",
    "        print(f\"[info] フォールバック選抜: hist={len(hist_sel)} / futr={len(futr_sel)}\")\n",
    "\n",
    "        # モデル再構築（試行数も削減）\n",
    "        nf, fit_kwargs = build_nf(max(4, TRIALS//2))\n",
    "        try_fit(nf, fit_kwargs)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"[info] 学習完了\")\n",
    "\n",
    "# =========================================================\n",
    "# 7) 成果物保存\n",
    "# =========================================================\n",
    "run_dir   = ensure_dir(os.path.join(ARTIFACTS_ROOT, f\"run_{now_str()}\"))\n",
    "models_dir= ensure_dir(os.path.join(run_dir, \"models\"))\n",
    "plots_dir = ensure_dir(os.path.join(run_dir, \"plots\"))\n",
    "tables_dir= ensure_dir(os.path.join(run_dir, \"tables\"))\n",
    "\n",
    "meta = {\n",
    "    \"DATA_CSV\": DATA_CSV,\n",
    "    \"TRIALS\": TRIALS,\n",
    "    \"SEED\": SEED,\n",
    "    \"H\": H,\n",
    "    \"FREQ\": FREQ,\n",
    "    \"BACKEND\": BACKEND,\n",
    "    \"SEARCH_ALG\": SEARCH_ALG,\n",
    "    \"LOSS\": str(LOSS),\n",
    "    \"VALID_LOSS\": str(VALID_LOSS),\n",
    "    \"LOCAL_SCALER_TYPE\": LOCAL_SCALER_TYPE,\n",
    "    \"EARLY_STOP_PATIENCE_STEPS\": EARLY_STOP_PATIENCE_STEPS,\n",
    "    \"CPUS\": CPUS,\n",
    "    \"GPUS\": GPUS,\n",
    "    \"futr_exog_list\": futr_sel,\n",
    "    \"hist_exog_list\": hist_sel,\n",
    "    \"stat_exog_list\": stat_sel,\n",
    "    \"fit_accepts_n_trials\": fit_accepts_n_trials,\n",
    "    \"precision\": prec,\n",
    "}\n",
    "with open(os.path.join(run_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "nf.save(models_dir)\n",
    "print(f\"[info] モデルを保存しました: {models_dir}\")\n",
    "\n",
    "# =========================================================\n",
    "# 8) ロード確認\n",
    "# =========================================================\n",
    "nf_loaded = NeuralForecast.load(models_dir)\n",
    "print(\"[info] モデルをロードしました。\")\n",
    "\n",
    "# =========================================================\n",
    "# 9) 交差検証 → 指標・可視化\n",
    "# =========================================================\n",
    "print(\"[info] 交差検証（Rolling Origin）を実行します...\")\n",
    "try:\n",
    "    cv_df = nf_loaded.cross_validation(df=df, static_df=static_df, n_windows=3, step_size=H, h=H)\n",
    "except TypeError:\n",
    "    cv_df = nf_loaded.cross_validation(df=df, static_df=static_df, n_windows=3, step_size=H, max_horizon=H)\n",
    "\n",
    "fixed_cols = {\"unique_id\",\"ds\",\"y\",\"cutoff\"}\n",
    "model_cols = [c for c in cv_df.columns if c not in fixed_cols]\n",
    "cv_path = os.path.join(tables_dir, \"cv_predictions.csv\")\n",
    "cv_df.to_csv(cv_path, index=False)\n",
    "print(f\"[info] CV 予測テーブルを保存: {cv_path}\")\n",
    "\n",
    "# 簡易メトリクス\n",
    "def _to_arr(x): return np.asarray(x, dtype=float)\n",
    "def smape_np(y,yhat,eps=1e-8): y=_to_arr(y);yhat=_to_arr(yhat);return 100*np.mean(2*np.abs(yhat-y)/(np.abs(y)+np.abs(yhat)+eps))\n",
    "def mae_np(y,yhat): y=_to_arr(y);yhat=_to_arr(yhat);return float(np.mean(np.abs(yhat-y)))\n",
    "def mape_np(y,yhat,eps=1e-8): y=_to_arr(y);yhat=_to_arr(yhat);den=np.clip(np.abs(y),eps,None);return 100*np.mean(np.abs((yhat-y)/den))\n",
    "def rmse_np(y,yhat): y=_to_arr(y);yhat=_to_arr(yhat);return float(np.sqrt(np.mean((yhat-y)**2)))\n",
    "\n",
    "rows=[]\n",
    "for m in model_cols:\n",
    "    y=cv_df[\"y\"].values; yhat=cv_df[m].values\n",
    "    rows.append({\n",
    "        \"model\":m,\n",
    "        \"SMAPE\":smape_np(y,yhat),\n",
    "        \"MAE\":mae_np(y,yhat),\n",
    "        \"MAPE\":mape_np(y,yhat),\n",
    "        \"RMSE\":rmse_np(y,yhat),\n",
    "        \"n\":len(cv_df)\n",
    "    })\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "metrics_path = os.path.join(tables_dir, \"metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"[info] 評価指標を保存: {metrics_path}\")\n",
    "print(metrics_df)\n",
    "\n",
    "plot_last_window(cv_df, model_cols, out_png=os.path.join(plots_dir, \"last_window.png\"))\n",
    "\n",
    "print(f\"[done] 成果物ルート: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0cc779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1029\n",
      "/home/az/miniconda3/envs/nc/lib/python3.11/site-packages/torch/__init__.py:1628: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:45.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/tmp/ipykernel_365392/3762487492.py:75: UserWarning: 等間隔でないタイムスタンプが検出されました。freq 指定または欠損補完を確認してください。\n",
      "  warnings.warn(\"等間隔でないタイムスタンプが検出されました。freq 指定または欠損補完を確認してください。\")\n",
      "[I 2025-11-12 07:52:04,089] A new study created in memory with name: no-name-eca17002-db25-4948-a0be-f2d3f1773b4b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] CSV を読込中: /mnt/e/env/ts/datas/data/data_long/ft_normal/bingo5/by_unique_id/N1.csv\n",
      "[info] loaded shape: (364, 162)\n",
      "[info] exog sizes (raw): futr=34 hist=122 stat=2\n",
      "[info] static_df: 非数値列をコード化 -> stat_ds_quarteryear\n",
      "[info] static_df: 非数値列をコード化 -> stat_ds_month_lbl\n",
      "[info] 選抜: hist=32/122 | futr=16/34 | stat=2\n",
      "[cap] TFT exog -> F=True H=True S=True\n",
      "[cap] PatchTST exog -> F=False H=False S=False\n",
      "[info] 学習開始: trials=1, val_size=2, freq=D\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741d5f5ea24f4b3caf89c6a9061c72e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cap] TFT exog -> F=True H=True S=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/az/miniconda3/envs/nc/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                    | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------------------\n",
      "0 | loss                    | SMAPE                    | 0      | train\n",
      "1 | padder_train            | ConstantPad1d            | 0      | train\n",
      "2 | scaler                  | TemporalNorm             | 0      | train\n",
      "3 | embedding               | TFTEmbedding             | 26.1 K | train\n",
      "4 | static_encoder          | StaticCovariateEncoder   | 1.8 M  | train\n",
      "5 | temporal_encoder        | TemporalCovariateEncoder | 23.6 M | train\n",
      "6 | temporal_fusion_decoder | TemporalFusionDecoder    | 1.1 M  | train\n",
      "7 | output_adapter          | Linear                   | 257    | train\n",
      "-----------------------------------------------------------------------------\n",
      "26.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.5 M    Total params\n",
      "105.951   Total estimated model params size (MB)\n",
      "646       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43cd5b13fc646c29f3b687187bf7573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02561eb35b49442e8a066495de0b30a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4c6013e7184f8bbb9ab2521fe94260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119f2a0b7be641129e3fc2fe53943630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b096e7410c4abf806a2d919c0a4468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/az/miniconda3/envs/nc/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                    | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------------------\n",
      "0 | loss                    | SMAPE                    | 0      | train\n",
      "1 | padder_train            | ConstantPad1d            | 0      | train\n",
      "2 | scaler                  | TemporalNorm             | 0      | train\n",
      "3 | embedding               | TFTEmbedding             | 26.1 K | train\n",
      "4 | static_encoder          | StaticCovariateEncoder   | 1.8 M  | train\n",
      "5 | temporal_encoder        | TemporalCovariateEncoder | 23.6 M | train\n",
      "6 | temporal_fusion_decoder | TemporalFusionDecoder    | 1.1 M  | train\n",
      "7 | output_adapter          | Linear                   | 257    | train\n",
      "-----------------------------------------------------------------------------\n",
      "26.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.5 M    Total params\n",
      "105.951   Total estimated model params size (MB)\n",
      "646       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 07:52:22,974] Trial 0 finished with value: 1.6655426025390625 and parameters: {'input_size': 4, 'learning_rate': 0.002134570940152967, 'hidden_size': 256, 'n_head': 2, 'dropout': 0.14874923353338593, 'batch_size': 128}. Best is trial 0 with value: 1.6655426025390625.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7eccb2bf577430983f89c39001bd254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82408a81ca6747a8ad5ca2fb46c37330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0767e6b3c0114e72aa9316401b105799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f42f1b059054d03b7fa2bbc8da6d489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415bcfe1b8014219925191d9cb0c84c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 07:52:41,127] A new study created in memory with name: no-name-4a596da5-8dec-41cd-81b4-64e3e4b14a90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141ae466782b4b4b86af6d901b314ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/az/miniconda3/envs/nc/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name         | Type              | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss         | SMAPE             | 0      | train\n",
      "1 | padder_train | ConstantPad1d     | 0      | train\n",
      "2 | scaler       | TemporalNorm      | 0      | train\n",
      "3 | model        | PatchTST_backbone | 399 K  | train\n",
      "-----------------------------------------------------------\n",
      "399 K     Trainable params\n",
      "3         Non-trainable params\n",
      "399 K     Total params\n",
      "1.600     Total estimated model params size (MB)\n",
      "90        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cap] PatchTST exog -> F=False H=False S=False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c84ff1fca84a999cbe3537cbb0d9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd407e3471c94401960e3fc1e3142e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11b4b36dd414a2cb54f3bbce5835d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ee9ab564844639a79306c197baeb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98036a3e9ce4f1386aa9f1a7f9855f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1d2d2966eb4dc6b703e1cb609d0cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa9f9a46125449497535490b2e1894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f970d687981f4c18a83ba72208ee3b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbe35920c4041d681d37c8e8fb88e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/az/miniconda3/envs/nc/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name         | Type              | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss         | SMAPE             | 0      | train\n",
      "1 | padder_train | ConstantPad1d     | 0      | train\n",
      "2 | scaler       | TemporalNorm      | 0      | train\n",
      "3 | model        | PatchTST_backbone | 399 K  | train\n",
      "-----------------------------------------------------------\n",
      "399 K     Trainable params\n",
      "3         Non-trainable params\n",
      "399 K     Total params\n",
      "1.600     Total estimated model params size (MB)\n",
      "90        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 07:52:43,731] Trial 0 finished with value: 1.095440149307251 and parameters: {'input_size': 4, 'd_model': 64, 'n_heads': 2, 'd_ff': 256, 'patch_len': 16, 'stride': 16, 'dropout': 0.0374688791481127, 'learning_rate': 0.002440608669830101, 'batch_size': 64}. Best is trial 0 with value: 1.095440149307251.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb370640cf0e474b9e9af29057d7a2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b06bec369f4652982d19d0a1421ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1372872b359c4188b20862c5d98caeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b613a98b0e48f68e277fa847def57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfde1128add54b35a4e762b254033d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f8905982c3463b8ba277fd49d970c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d51f48927d40809f2f802c1cefba8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650ecd0da729479b860e832933483acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e28115ec584408aa0d392fbd05f2b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Seed set to 1\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/az/miniconda3/envs/nc/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                    | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------------------\n",
      "0 | loss                    | SMAPE                    | 0      | train\n",
      "1 | padder_train            | ConstantPad1d            | 0      | train\n",
      "2 | scaler                  | TemporalNorm             | 0      | train\n",
      "3 | embedding               | TFTEmbedding             | 26.1 K | train\n",
      "4 | static_encoder          | StaticCovariateEncoder   | 1.8 M  | train\n",
      "5 | temporal_encoder        | TemporalCovariateEncoder | 23.6 M | train\n",
      "6 | temporal_fusion_decoder | TemporalFusionDecoder    | 1.1 M  | train\n",
      "7 | output_adapter          | Linear                   | 257    | train\n",
      "-----------------------------------------------------------------------------\n",
      "26.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.5 M    Total params\n",
      "105.951   Total estimated model params size (MB)\n",
      "646       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] 学習完了\n",
      "[info] モデルを保存しました: nf_auto_runs/run_20251112-075246/models\n",
      "[info] モデルをロードしました。\n",
      "[info] 交差検証（Rolling Origin）を実行します...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56e9b9324b84c58aeb91b82ddc5a81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf7587f9e984f1b92692568439cd221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1c46d7eb40499a9bfdb3ead2978f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ea7e61e30413d83a1a5dc5aa1f305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc58767f66a44e0c85b1ca31b07fbe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53fed0fc86949e4b5188cf3c1f44e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e6c048eb9a4ea3955426e4620fbde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b206b162544193853d829471aa6fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa87d831e52b46ec8b9bea079b715baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed2a1f93e6a466d82a1277f3ba5fa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type              | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss         | SMAPE             | 0      | train\n",
      "1 | padder_train | ConstantPad1d     | 0      | train\n",
      "2 | scaler       | TemporalNorm      | 0      | train\n",
      "3 | model        | PatchTST_backbone | 399 K  | train\n",
      "-----------------------------------------------------------\n",
      "399 K     Trainable params\n",
      "3         Non-trainable params\n",
      "399 K     Total params\n",
      "1.600     Total estimated model params size (MB)\n",
      "90        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235052e626314a82bf6877f5ddbd8490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce40b891596422b94519dacc6c0d80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab6133a2519497a84c9d5d651460fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8c8b7123c3405190b9b37ac765d8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8f01c66ad547b9a9e3f76536732fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311d3aa22d3b4c1295371de09bf40b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] CV 予測テーブルを保存: nf_auto_runs/run_20251112-075246/tables/cv_predictions.csv\n",
      "[info] 評価指標を保存: nf_auto_runs/run_20251112-075246/tables/metrics.csv\n",
      "          model      SMAPE       MAE       MAPE      RMSE  n\n",
      "0       AutoTFT        NaN       NaN        NaN       NaN  3\n",
      "1  AutoPatchTST  25.273531  1.123584  31.792946  1.388669  3\n",
      "[info] 予測可視化を保存: nf_auto_runs/run_20251112-075246/plots/last_window.png\n",
      "[done] 成果物ルート: nf_auto_runs/run_20251112-075246\n"
     ]
    }
   ],
   "source": [
    "# NF Auto — 汎用EXOG適応＆署名フィルタ改修版（CVでval_size明示＋ES一時無効フォールバック付き）\n",
    "# - モデル能力に応じて F/H/S を自動で付与/除去\n",
    "# - モデル __init__ 署名＋Trainer 署名で未知引数を除去（安全）\n",
    "# - 主要パラの簡易シノニム変換でバージョン差に耐性\n",
    "# - 交差検証でも val_size を明示、必要時は EarlyStopping を一時無効化\n",
    "\n",
    "import os, json, time, warnings, random, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import optuna\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoTFT, AutoPatchTST\n",
    "from neuralforecast.models import TFT, PatchTST\n",
    "from neuralforecast.losses.pytorch import SMAPE\n",
    "\n",
    "# =============================\n",
    "# 0) 実行パラメータ\n",
    "# =============================\n",
    "path= \"/mnt/e/env/ts/datas/data/data_long/ft_normal/bingo5/by_unique_id/N1.csv\"\n",
    "\n",
    "DATA_CSV = os.environ.get(\"NF_DATA_CSV\", path)\n",
    "TRIALS   = int(os.environ.get(\"NF_TRIAL_NUM_SAMPLES\", 1))\n",
    "SEED     = int(os.environ.get(\"NF_SEED\", 1029))\n",
    "H        = int(os.environ.get(\"NF_H\", 1))\n",
    "ARTIFACTS_ROOT = os.environ.get(\"NF_ARTIFACTS_ROOT\", \"nf_auto_runs\")\n",
    "FREQ = os.environ.get(\"NF_FREQ\", \"D\")\n",
    "\n",
    "LOSS = SMAPE()\n",
    "BACKEND = \"optuna\"\n",
    "SEARCH_ALG = optuna.samplers.TPESampler(seed=SEED)\n",
    "EARLY_STOP_PATIENCE_STEPS = int(os.environ.get(\"NF_EARLY_STOP\", 2))\n",
    "VERBOSE = True\n",
    "TOPK_HIST = int(os.environ.get(\"NF_TOPK_HIST\", 32))\n",
    "TOPK_FUTR = int(os.environ.get(\"NF_TOPK_FUTR\", 16))\n",
    "\n",
    "CPUS = -1\n",
    "GPUS = -1\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); pl.seed_everything(SEED, workers=True)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# =============================\n",
    "# 1) 前処理ユーティリティ\n",
    "# =============================\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "def now_str(): return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def to_datetime_col(df, col): out=df.copy(); out[col]=pd.to_datetime(out[col], errors=\"coerce\"); return out\n",
    "\n",
    "def to_numeric_col(df, col): out=df.copy(); out[col]=pd.to_numeric(out[col], errors=\"coerce\"); return out\n",
    "\n",
    "\n",
    "def preprocess_df(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw.copy()\n",
    "    if \"unique_id\" not in df.columns: df[\"unique_id\"]=\"series_0\"\n",
    "    df = to_datetime_col(df, \"ds\"); df = to_numeric_col(df, \"y\")\n",
    "    before=len(df); df=df.dropna(subset=[\"ds\",\"y\"]) \n",
    "    if before-len(df)>0: print(f(\"[info] 前処理: ds/y の変換で {before-len(df)} 行を除去しました。\"))\n",
    "    df=df.sort_values([\"unique_id\",\"ds\"]).drop_duplicates(subset=[\"unique_id\",\"ds\"], keep=\"last\").reset_index(drop=True)\n",
    "    gap=df.groupby(\"unique_id\")[\"ds\"].diff().dropna()\n",
    "    if not gap.empty:\n",
    "        if (gap.dt.days.fillna(0)!=1).any() and FREQ==\"D\":\n",
    "            warnings.warn(\"等間隔でないタイムスタンプが検出されました。freq 指定または欠損補完を確認してください。\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_exog_by_prefix(df):\n",
    "    futr=[c for c in df.columns if c.startswith(\"futr_\")]\n",
    "    hist=[c for c in df.columns if c.startswith(\"hist_\")]\n",
    "    stat=[c for c in df.columns if c.startswith(\"stat_\")]\n",
    "    return futr,hist,stat\n",
    "\n",
    "\n",
    "def factorize_static(static_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out=static_df.copy()\n",
    "    for c in out.columns:\n",
    "        if c==\"unique_id\": continue\n",
    "        if not np.issubdtype(out[c].dtype, np.number):\n",
    "            codes, _ = pd.factorize(out[c], sort=True)\n",
    "            out[c]=codes.astype(np.int32)\n",
    "            print(f\"[info] static_df: 非数値列をコード化 -> {c}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def coerce_temporal_numeric(df: pd.DataFrame, protected=(\"unique_id\",\"ds\",\"y\")):\n",
    "    temporal=[c for c in df.columns if c not in protected]\n",
    "    drop=[]\n",
    "    for c in temporal:\n",
    "        if not np.issubdtype(df[c].dtype, np.number):\n",
    "            coerced=pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if coerced.isna().all():\n",
    "                drop.append(c)\n",
    "            else:\n",
    "                df[c]=coerced.fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0.0)\n",
    "                print(f\"[info] temporal: {c} を数値化（欠損は前後詰め→0）\")\n",
    "    if drop:\n",
    "        df.drop(columns=drop, inplace=True)\n",
    "        print(f\"[info] temporal: 数値化不能のためドロップ -> {drop}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_topk_features(df, feature_cols, k: int, target_col=\"y\", min_std=1e-12):\n",
    "    if k<=0 or len(feature_cols)==0: return []\n",
    "    std=df[feature_cols].std(numeric_only=True)\n",
    "    keep=std[std>min_std].index.tolist()\n",
    "    if not keep: return []\n",
    "    corrs={}; y=df[target_col].astype(float)\n",
    "    for c in keep:\n",
    "        x=df[c].astype(float); valid=x.notna() & y.notna()\n",
    "        if valid.sum()<3: continue\n",
    "        xc=x[valid]; yc=y[valid]\n",
    "        if xc.std()<min_std: continue\n",
    "        corrs[c]=float(abs(np.corrcoef(xc,yc)[0,1]))\n",
    "    if not corrs: return keep[:k]\n",
    "    ranked=sorted(corrs.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    picked=[c for c,_ in ranked[:k]]\n",
    "    if len(picked)<k:\n",
    "        rest=[c for c in keep if c not in picked]; picked+=rest[:(k-len(picked))]\n",
    "    return picked\n",
    "\n",
    "\n",
    "def lightning_precision():\n",
    "    try: major=int(pl.__version__.split(\".\")[0])\n",
    "    except: major=2\n",
    "    return \"16-mixed\" if major>=2 else 16\n",
    "\n",
    "# =============================\n",
    "# 2) モデル能力→EXOG 自動適応\n",
    "# =============================\n",
    "\n",
    "MODEL_EXOG_FALLBACK = {\n",
    "    \"Autoformer\": dict(F=True, H=False, S=False),\n",
    "    \"BiTCN\": dict(F=True, H=True, S=True),\n",
    "    \"DeepAR\": dict(F=True, H=False, S=True),\n",
    "    \"DeepNPTS\": dict(F=True, H=True, S=True),\n",
    "    \"DilatedRNN\": dict(F=True, H=True, S=True),\n",
    "    \"FEDformer\": dict(F=True, H=False, S=False),\n",
    "    \"GRU\": dict(F=True, H=True, S=True),\n",
    "    \"HINT\": dict(F=True, H=True, S=True),\n",
    "    \"Informer\": dict(F=True, H=False, S=False),\n",
    "    \"iTransformer\": dict(F=False, H=False, S=False),\n",
    "    \"KAN\": dict(F=True, H=True, S=True),\n",
    "    \"LSTM\": dict(F=True, H=True, S=True),\n",
    "    \"MLP\": dict(F=True, H=True, S=True),\n",
    "    \"MLPMultivariate\": dict(F=True, H=True, S=True),\n",
    "    \"NBEATS\": dict(F=False, H=False, S=False),\n",
    "    \"NBEATSx\": dict(F=True, H=True, S=True),\n",
    "    \"NHITS\": dict(F=True, H=True, S=True),\n",
    "    \"NLinear\": dict(F=False, H=False, S=False),\n",
    "    \"PatchTST\": dict(F=False, H=False, S=False),\n",
    "    \"RMoK\": dict(F=False, H=False, S=False),\n",
    "    \"RNN\": dict(F=True, H=True, S=True),\n",
    "    \"SOFTS\": dict(F=False, H=False, S=False),\n",
    "    \"StemGNN\": dict(F=False, H=False, S=False),\n",
    "    \"TCN\": dict(F=True, H=True, S=True),\n",
    "    \"TFT\": dict(F=True, H=True, S=True),\n",
    "    \"TiDE\": dict(F=True, H=True, S=True),\n",
    "    \"TimeMixer\": dict(F=False, H=False, S=False),\n",
    "    \"TimeLLM\": dict(F=False, H=False, S=False),\n",
    "    \"TimesNet\": dict(F=True, H=False, S=False),\n",
    "    \"TimeXer\": dict(F=True, H=False, S=False),\n",
    "    \"TSMixer\": dict(F=False, H=False, S=False),\n",
    "    \"TSMixerx\": dict(F=True, H=True, S=True),\n",
    "    \"VanillaTransformer\": dict(F=True, H=False, S=False),\n",
    "}\n",
    "\n",
    "\n",
    "def exog_capabilities(model_cls, fallback_key: str):\n",
    "    f = bool(getattr(model_cls, \"EXOGENOUS_FUTR\", MODEL_EXOG_FALLBACK.get(fallback_key, {}).get(\"F\", False)))\n",
    "    h = bool(getattr(model_cls, \"EXOGENOUS_HIST\", MODEL_EXOG_FALLBACK.get(fallback_key, {}).get(\"H\", False)))\n",
    "    s = bool(getattr(model_cls, \"EXOGENOUS_STAT\", MODEL_EXOG_FALLBACK.get(fallback_key, {}).get(\"S\", False)))\n",
    "    return dict(F=f, H=h, S=s)\n",
    "\n",
    "\n",
    "def attach_exog(config: dict, model_cls, fallback_key: str, futr_sel, hist_sel, stat_sel):\n",
    "    caps = exog_capabilities(model_cls, fallback_key)\n",
    "    config.update({\n",
    "        \"futr_exog_list\": futr_sel if caps[\"F\"] else [],\n",
    "        \"hist_exog_list\": hist_sel if caps[\"H\"] else [],\n",
    "        \"stat_exog_list\": stat_sel if caps[\"S\"] else [],\n",
    "    })\n",
    "    return config, caps\n",
    "\n",
    "# --- 署名フィルタとシノニム補正 ---\n",
    "\n",
    "TRAINER_ALLOWED = (set(inspect.signature(pl.Trainer.__init__).parameters.keys()) - {\"self\"}) if hasattr(pl, \"Trainer\") else set()\n",
    "ALWAYS_ALLOWED = {\"futr_exog_list\",\"hist_exog_list\",\"stat_exog_list\"}\n",
    "\n",
    "def _apply_param_synonyms(cfg: dict, model_cls) -> dict:\n",
    "    try:\n",
    "        params = set(inspect.signature(model_cls.__init__).parameters.keys()) - {\"self\"}\n",
    "    except Exception:\n",
    "        params = set()\n",
    "\n",
    "    out = dict(cfg)\n",
    "\n",
    "    def rename(src, dst):\n",
    "        if src in out and src not in params and dst in params:\n",
    "            out[dst] = out.pop(src)\n",
    "\n",
    "    rename(\"n_heads\", \"n_head\")\n",
    "    rename(\"n_head\", \"n_heads\")\n",
    "    rename(\"d_model\", \"d\")\n",
    "    rename(\"d_ff\", \"ff_dim\")\n",
    "    rename(\"learning_rate\", \"lr\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def filter_kwargs_by_signature(config: dict, model_cls):\n",
    "    try:\n",
    "        model_params = set(inspect.signature(model_cls.__init__).parameters.keys())\n",
    "    except Exception:\n",
    "        model_params = set()\n",
    "    model_params.discard(\"self\")\n",
    "\n",
    "    cfg = _apply_param_synonyms(config, model_cls)\n",
    "\n",
    "    allowed = model_params | TRAINER_ALLOWED | ALWAYS_ALLOWED\n",
    "    filtered = {k: v for k, v in cfg.items() if k in allowed}\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3) データ読み込みと外生選抜\n",
    "# =============================\n",
    "print(f\"[info] CSV を読込中: {DATA_CSV}\")\n",
    "raw = pd.read_csv(DATA_CSV)\n",
    "print(f\"[info] loaded shape: {raw.shape}\")\n",
    "\n",
    "df = preprocess_df(raw)\n",
    "futr_cols, hist_cols, stat_cols = split_exog_by_prefix(df)\n",
    "print(f\"[info] exog sizes (raw): futr={len(futr_cols)} hist={len(hist_cols)} stat={len(stat_cols)}\")\n",
    "\n",
    "static_df=None\n",
    "if len(stat_cols):\n",
    "    static_df = df[[\"unique_id\", *stat_cols]].drop_duplicates()\n",
    "    static_df = factorize_static(static_df)\n",
    "    df = df.drop(columns=stat_cols)\n",
    "\n",
    "# 型整備\n",
    "futr_cols, hist_cols, _ = split_exog_by_prefix(df)\n",
    "df = coerce_temporal_numeric(df)\n",
    "\n",
    "# 再確認\n",
    "futr_cols = [c for c in futr_cols if c in df.columns]\n",
    "hist_cols = [c for c in hist_cols if c in df.columns]\n",
    "stat_sel = [] if static_df is None else [c for c in static_df.columns if c!=\"unique_id\"]\n",
    "\n",
    "# Top-K\n",
    "hist_sel = select_topk_features(df, hist_cols, k=TOPK_HIST, target_col=\"y\")\n",
    "futr_sel = select_topk_features(df, futr_cols, k=TOPK_FUTR, target_col=\"y\")\n",
    "print(f\"[info] 選抜: hist={len(hist_sel)}/{len(hist_cols)} | futr={len(futr_sel)}/{len(futr_cols)} | stat={len(stat_sel)}\")\n",
    "\n",
    "# =============================\n",
    "# 4) Auto 構成（PL引数はフラットで渡す→内部で安全に抽出）\n",
    "# =============================\n",
    "prec = lightning_precision()\n",
    "PL_FLAT = dict(\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=prec,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "def tft_config(trial: optuna.trial.Trial):\n",
    "    cfg = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [2*H, 3*H, 4*H]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 3e-3, log=True),\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [64, 128, 256]),\n",
    "        \"n_head\": trial.suggest_categorical(\"n_head\", [2, 4, 8]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.3),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n",
    "        \"max_steps\": int(os.environ.get(\"NF_MAX_STEPS\", 800)),\n",
    "        \"val_check_steps\": int(os.environ.get(\"NF_VAL_CHECK_STEPS\", 50)),\n",
    "        \"early_stop_patience_steps\": EARLY_STOP_PATIENCE_STEPS,\n",
    "        **PL_FLAT,\n",
    "    }\n",
    "    cfg, caps = attach_exog(cfg, TFT, \"TFT\", futr_sel, hist_sel, stat_sel)\n",
    "    cfg = filter_kwargs_by_signature(cfg, TFT)\n",
    "    print(f\"[cap] TFT exog -> F={caps['F']} H={caps['H']} S={caps['S']}\")\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def patchtst_config(trial: optuna.trial.Trial):\n",
    "    cfg = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [2*H, 3*H, 4*H]),\n",
    "        \"d_model\": trial.suggest_categorical(\"d_model\", [64, 128, 192]),\n",
    "        \"n_heads\": trial.suggest_categorical(\"n_heads\", [2, 4, 8]),\n",
    "        \"d_ff\": trial.suggest_categorical(\"d_ff\", [128, 256, 512]),\n",
    "        \"patch_len\": trial.suggest_categorical(\"patch_len\", [8, 16, 32]),\n",
    "        \"stride\": trial.suggest_categorical(\"stride\", [8, 16]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.2),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 3e-3, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n",
    "        \"max_steps\": int(os.environ.get(\"NF_MAX_STEPS\", 800)),\n",
    "        \"val_check_steps\": int(os.environ.get(\"NF_VAL_CHECK_STEPS\", 50)),\n",
    "        \"early_stop_patience_steps\": EARLY_STOP_PATIENCE_STEPS,\n",
    "        **PL_FLAT,\n",
    "    }\n",
    "    cfg, caps = attach_exog(cfg, PatchTST, \"PatchTST\", futr_sel, hist_sel, stat_sel)\n",
    "    cfg = filter_kwargs_by_signature(cfg, PatchTST)\n",
    "    print(f\"[cap] PatchTST exog -> F={caps['F']} H={caps['H']} S={caps['S']}\")\n",
    "    return cfg\n",
    "\n",
    "\n",
    "models = [\n",
    "    AutoTFT(h=H, loss=LOSS, backend=BACKEND, config=tft_config, search_alg=SEARCH_ALG, num_samples=TRIALS, verbose=VERBOSE),\n",
    "    AutoPatchTST(h=H, loss=LOSS, backend=BACKEND, config=patchtst_config, search_alg=SEARCH_ALG, num_samples=TRIALS, verbose=VERBOSE),\n",
    "]\n",
    "\n",
    "nf = NeuralForecast(models=models, freq=FREQ, local_scaler_type=\"standard\")\n",
    "\n",
    "# =============================\n",
    "# 5) 学習\n",
    "# =============================\n",
    "\n",
    "def try_fit():\n",
    "    val_size = max(2*H, H)  # 余裕を持たせる\n",
    "    print(f\"[info] 学習開始: trials={TRIALS}, val_size={val_size}, freq={FREQ}\")\n",
    "    nf.fit(df=df, static_df=static_df, val_size=val_size)\n",
    "\n",
    "try:\n",
    "    try_fit()\n",
    "except Exception as e:\n",
    "    msg=str(e)\n",
    "    if \"CUDA out of memory\" in msg or isinstance(e, torch.cuda.OutOfMemoryError):\n",
    "        print(\"[warn] CUDA OOM 検出。縮小構成でリトライします...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        hist_small = select_topk_features(df, hist_cols, k=max(8, TOPK_HIST//2), target_col=\"y\")\n",
    "        futr_small = select_topk_features(df, futr_cols, k=max(4, TOPK_FUTR//2), target_col=\"y\")\n",
    "        # 再バインド（能力を再評価）\n",
    "        hist_sel, futr_sel = hist_small, futr_small\n",
    "        try_fit()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"[info] 学習完了\")\n",
    "\n",
    "# =============================\n",
    "# 6) 成果物保存・CV・指標\n",
    "# =============================\n",
    "run_dir   = ensure_dir(os.path.join(ARTIFACTS_ROOT, f\"run_{now_str()}\"))\n",
    "models_dir= ensure_dir(os.path.join(run_dir, \"models\"))\n",
    "plots_dir = ensure_dir(os.path.join(run_dir, \"plots\"))\n",
    "tables_dir= ensure_dir(os.path.join(run_dir, \"tables\"))\n",
    "\n",
    "meta = {\n",
    "    \"DATA_CSV\": DATA_CSV, \"TRIALS\": TRIALS, \"SEED\": SEED, \"H\": H, \"FREQ\": FREQ,\n",
    "    \"BACKEND\": BACKEND, \"SEARCH_ALG\": type(SEARCH_ALG).__name__, \"LOSS\": str(LOSS),\n",
    "    \"LOCAL_SCALER_TYPE\": \"standard\", \"EARLY_STOP_PATIENCE_STEPS\": EARLY_STOP_PATIENCE_STEPS,\n",
    "    \"CPUS\": CPUS, \"GPUS\": GPUS,\n",
    "    \"futr_exog_list\": futr_sel, \"hist_exog_list\": hist_sel, \"stat_exog_list\": stat_sel,\n",
    "    \"precision\": prec,\n",
    "}\n",
    "with open(os.path.join(run_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "nf.save(models_dir)\n",
    "print(f\"[info] モデルを保存しました: {models_dir}\")\n",
    "\n",
    "nf_loaded = NeuralForecast.load(models_dir)\n",
    "print(\"[info] モデルをロードしました。\")\n",
    "\n",
    "print(\"[info] 交差検証（Rolling Origin）を実行します...\")\n",
    "val_size_cv = max(2*H, H)\n",
    "try:\n",
    "    # PL2 系では ES の監視名が 'ptl/val_loss' になる実装が多いが、\n",
    "    # そもそも val loader が無いと生成されないため、val_size を必ず明示する\n",
    "    cv_df = nf_loaded.cross_validation(\n",
    "        df=df, static_df=static_df,\n",
    "        n_windows=3, step_size=H, h=H,\n",
    "        val_size=val_size_cv\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    # EarlyStopping が監視指標未出力で落ちた場合のフォールバック：CV 中のみ ES を無効化して再実行\n",
    "    if \"Early stopping conditioned on metric\" in str(e):\n",
    "        print(\"[warn] CV中の EarlyStopping 監視メトリクス未検出。CVに限り EarlyStopping を一時無効化して再実行します。\")\n",
    "        for m in nf_loaded.models:\n",
    "            if hasattr(m, \"early_stop_patience_steps\"):\n",
    "                try:\n",
    "                    m.early_stop_patience_steps = None\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if hasattr(m, \"trainer_kwargs\"):\n",
    "                try:\n",
    "                    m.trainer_kwargs.pop(\"early_stop_patience_steps\", None)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        cv_df = nf_loaded.cross_validation(\n",
    "            df=df, static_df=static_df,\n",
    "            n_windows=3, step_size=H, h=H,\n",
    "            val_size=val_size_cv\n",
    "        )\n",
    "    else:\n",
    "        raise\n",
    "except TypeError:\n",
    "    # バージョン差：max_horizon 名称の互換\n",
    "    try:\n",
    "        cv_df = nf_loaded.cross_validation(\n",
    "            df=df, static_df=static_df,\n",
    "            n_windows=3, step_size=H, max_horizon=H,\n",
    "            val_size=val_size_cv\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        if \"Early stopping conditioned on metric\" in str(e):\n",
    "            print(\"[warn] CV中の EarlyStopping 監視メトリクス未検出。CVに限り EarlyStopping を一時無効化して再実行します。\")\n",
    "            for m in nf_loaded.models:\n",
    "                if hasattr(m, \"early_stop_patience_steps\"):\n",
    "                    try:\n",
    "                        m.early_stop_patience_steps = None\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if hasattr(m, \"trainer_kwargs\"):\n",
    "                    try:\n",
    "                        m.trainer_kwargs.pop(\"early_stop_patience_steps\", None)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            cv_df = nf_loaded.cross_validation(\n",
    "                df=df, static_df=static_df,\n",
    "                n_windows=3, step_size=H, max_horizon=H,\n",
    "                val_size=val_size_cv\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "fixed_cols = {\"unique_id\",\"ds\",\"y\",\"cutoff\"}\n",
    "model_cols = [c for c in cv_df.columns if c not in fixed_cols]\n",
    "cv_path = os.path.join(tables_dir, \"cv_predictions.csv\")\n",
    "cv_df.to_csv(cv_path, index=False)\n",
    "print(f\"[info] CV 予測テーブルを保存: {cv_path}\")\n",
    "\n",
    "# 指標\n",
    "_def_to_arr = lambda x: np.asarray(x, dtype=float)\n",
    "\n",
    "def smape_np(y,yhat,eps=1e-8): y=_def_to_arr(y);yhat=_def_to_arr(yhat);return 100*np.mean(2*np.abs(yhat-y)/(np.abs(y)+np.abs(yhat)+eps))\n",
    "\n",
    "def mae_np(y,yhat): y=_def_to_arr(y);yhat=_def_to_arr(yhat);return float(np.mean(np.abs(yhat-y)))\n",
    "\n",
    "def mape_np(y,yhat,eps=1e-8): y=_def_to_arr(y);yhat=_def_to_arr(yhat);den=np.clip(np.abs(y),eps,None);return 100*np.mean(np.abs((yhat-y)/den))\n",
    "\n",
    "def rmse_np(y,yhat): y=_def_to_arr(y);yhat=_def_to_arr(yhat);return float(np.sqrt(np.mean((yhat-y)**2)))\n",
    "\n",
    "rows=[]\n",
    "for m in model_cols:\n",
    "    y=cv_df[\"y\"].values; yhat=cv_df[m].values\n",
    "    rows.append({\"model\":m, \"SMAPE\":smape_np(y,yhat), \"MAE\":mae_np(y,yhat), \"MAPE\":mape_np(y,yhat), \"RMSE\":rmse_np(y,yhat), \"n\":len(cv_df)})\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "metrics_path = os.path.join(tables_dir, \"metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"[info] 評価指標を保存: {metrics_path}\\n{metrics_df}\")\n",
    "\n",
    "# 描画\n",
    "if not cv_df.empty:\n",
    "    try:\n",
    "        last = cv_df[\"cutoff\"].max()\n",
    "        sub = cv_df[cv_df[\"cutoff\"] == last]\n",
    "        if sub[\"unique_id\"].nunique() > 1:\n",
    "            sub = sub[sub[\"unique_id\"] == sub[\"unique_id\"].iloc[0]]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.plot(sub[\"ds\"], sub[\"y\"], label=\"y\", linewidth=2)\n",
    "        for m in model_cols: plt.plot(sub[\"ds\"], sub[m], label=m, linewidth=1)\n",
    "        plt.title(f\"Last CV Window @ cutoff={last}\")\n",
    "        plt.xlabel(\"ds\"); plt.ylabel(\"value\"); plt.legend(); plt.tight_layout()\n",
    "        out_png = os.path.join(plots_dir, \"last_window.png\")\n",
    "        plt.savefig(out_png); plt.close()\n",
    "        print(f\"[info] 予測可視化を保存: {out_png}\")\n",
    "    except Exception:\n",
    "        print(\"[warn] 可視化に失敗しました（処理を継続します）。\")\n",
    "\n",
    "print(f\"[done] 成果物ルート: {run_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09322df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "[Model 1] AutoTFT\n",
      "=================\n",
      "name                                      type                      value\n",
      "-------------------------------------------------------------------------\n",
      "CHECKPOINT_HYPER_PARAMS_KEY               str                       hyper_parameters\n",
      "CHECKPOINT_HYPER_PARAMS_NAME              str                       hparams_name\n",
      "CHECKPOINT_HYPER_PARAMS_TYPE              str                       hparams_type\n",
      "EXOGENOUS_FUTR                            bool                      True\n",
      "EXOGENOUS_HIST                            bool                      True\n",
      "EXOGENOUS_STAT                            bool                      True\n",
      "MULTIVARIATE                              bool                      False\n",
      "RECURRENT                                 bool                      False\n",
      "T_destination                             TypeVar                   TypeVar: ~T_destination\n",
      "alias                                     NoneType                  None\n",
      "allow_zero_length_dataloader_with_multiple_devices  bool                      False\n",
      "automatic_optimization                    bool                      True\n",
      "backend                                   str                       optuna\n",
      "call_super_init                           bool                      False\n",
      "callbacks                                 NoneType                  None\n",
      "cls_model                                 type                      type: <class 'neuralforecast.models.tft.TFT'>\n",
      "config                                    function                  function: <function BaseAuto.__init__.<locals>.config_f at 0x7354e4fc8a40>\n",
      "cpus                                      int                       32\n",
      "current_epoch                             int                       0\n",
      "default_config                            dict                      dict(len=11, keys=['input_size_multiplier', 'h', 'hidden_size', 'n_head', 'learning_rate', 'scaler_type', 'max_steps', 'batch_size']…)\n",
      "device                                    device                    device: device(type='cpu')\n",
      "device_mesh                               NoneType                  None\n",
      "dtype                                     dtype                     dtype: torch.float32\n",
      "dump_patches                              bool                      False\n",
      "early_stop_patience_steps                 int                       1\n",
      "example_input_array                       NoneType                  None\n",
      "fabric                                    NoneType                  None\n",
      "futr_exog_list                            list                      list(len=16)\n",
      "global_rank                               int                       0\n",
      "global_step                               int                       0\n",
      "gpus                                      int                       1\n",
      "h                                         int                       1\n",
      "hist_exog_list                            list                      list(len=32)\n",
      "hparams                                   AttributeDict             dict(len=14, keys=['cls_model', 'h', 'loss', 'valid_loss', 'config', 'search_alg', 'num_samples', 'cpus']…)\n",
      "hparams_initial                           AttributeDict             dict(len=14, keys=['cls_model', 'h', 'loss', 'valid_loss', 'config', 'search_alg', 'num_samples', 'cpus']…)\n",
      "local_rank                                int                       0\n",
      "logger                                    NoneType                  None\n",
      "loggers                                   list                      list(len=0)\n",
      "loss                                      SMAPE                     SMAPE: SMAPE()\n",
      "model                                     TFT                       TFT: TFT\n",
      "num_samples                               int                       1\n",
      "on_gpu                                    bool                      False\n",
      "prepare_data_per_node                     bool                      True\n",
      "refit_with_val                            bool                      True\n",
      "results                                   Study                     Study: <optuna.study.study.Study object at 0x7354e4f3f590>\n",
      "search_alg                                TPESampler                TPESampler: <optuna.samplers._tpe.sampler.TPESampler object at 0x7354e4e42290>\n",
      "stat_exog_list                            list                      list(len=2)\n",
      "strict_loading                            bool                      True\n",
      "trainer                                   str                       <attr_error: AutoTFT is not attached to a `Trainer`.>\n",
      "training                                  bool                      True\n",
      "valid_loss                                SMAPE                     SMAPE: SMAPE()\n",
      "verbose                                   bool                      True\n",
      "======================\n",
      "[Model 1] tuned -> TFT\n",
      "======================\n",
      "name                                      type                      value\n",
      "-------------------------------------------------------------------------\n",
      "CHECKPOINT_HYPER_PARAMS_KEY               str                       hyper_parameters\n",
      "CHECKPOINT_HYPER_PARAMS_NAME              str                       hparams_name\n",
      "CHECKPOINT_HYPER_PARAMS_TYPE              str                       hparams_type\n",
      "EXOGENOUS_FUTR                            bool                      True\n",
      "EXOGENOUS_HIST                            bool                      True\n",
      "EXOGENOUS_STAT                            bool                      True\n",
      "MULTIVARIATE                              bool                      False\n",
      "RECURRENT                                 bool                      False\n",
      "T_destination                             TypeVar                   TypeVar: ~T_destination\n",
      "alias                                     NoneType                  None\n",
      "allow_zero_length_dataloader_with_multiple_devices  bool                      False\n",
      "automatic_optimization                    bool                      True\n",
      "batch_size                                int                       128\n",
      "call_super_init                           bool                      False\n",
      "current_epoch                             str                       <attr_error: 'TFT' object has no attribute 'current_epoch'>\n",
      "dataloader_kwargs                         NoneType                  None\n",
      "decompose_forecast                        bool                      False\n",
      "device                                    device                    device: device(type='cpu')\n",
      "device_mesh                               NoneType                  None\n",
      "drop_last_loader                          bool                      False\n",
      "dtype                                     dtype                     dtype: torch.float32\n",
      "dump_patches                              bool                      False\n",
      "early_stop_patience_steps                 int                       2\n",
      "embedding                                 TFTEmbedding              TFTEmbedding: TFTEmbedding()\n",
      "example_input_array                       NoneType                  None\n",
      "example_length                            int                       5\n",
      "exclude_insample_y                        bool                      False\n",
      "fabric                                    NoneType                  None\n",
      "futr_exog_list                            list                      list(len=16)\n",
      "futr_exog_size                            int                       16\n",
      "global_rank                               str                       <attr_error: 'TFT' object has no attribute 'global_rank'>\n",
      "global_step                               str                       <attr_error: 'TFT' object has no attribute 'global_step'>\n",
      "grn_activation                            str                       ELU\n",
      "h                                         int                       1\n",
      "hist_exog_list                            list                      list(len=32)\n",
      "hist_exog_size                            int                       32\n",
      "horizon_backup                            int                       1\n",
      "hparams                                   AttributeDict             dict(len=48, keys=['h', 'input_size', 'loss', 'valid_loss', 'learning_rate', 'max_steps', 'val_check_steps', 'batch_size']…)\n",
      "hparams_initial                           AttributeDict             dict(len=48, keys=['h', 'input_size', 'loss', 'valid_loss', 'learning_rate', 'max_steps', 'val_check_steps', 'batch_size']…)\n",
      "inference_windows_batch_size              int                       1024\n",
      "input_size                                int                       4\n",
      "input_size_backup                         int                       4\n",
      "interpretability_params                   dict                      dict(len=4, keys=['history_vsn_wgts', 'future_vsn_wgts', 'static_encoder_sparse_weights', 'attn_wts'])\n",
      "learning_rate                             float                     0.002134570940152967\n",
      "local_rank                                str                       <attr_error: 'TFT' object has no attribute 'local_rank'>\n",
      "logger                                    str                       <attr_error: 'TFT' object has no attribute 'logger'>\n",
      "loggers                                   str                       <attr_error: 'TFT' object has no attribute 'loggers'>\n",
      "loss                                      SMAPE                     SMAPE: SMAPE()\n",
      "lr_decay_steps                            float                     100000000.0\n",
      "lr_scheduler                              NoneType                  None\n",
      "lr_scheduler_kwargs                       dict                      dict(len=0, keys=[])\n",
      "max_steps                                 int                       800\n",
      "metrics                                   dict                      dict(len=5, keys=['train_loss', 'train_loss_step', 'train_loss_epoch', 'valid_loss', 'ptl/val_loss'])\n",
      "min_insample_fraction                     float                     0.0\n",
      "min_outsample_fraction                    float                     0.0\n",
      "n_predicts                                int                       1\n",
      "n_rnn_layers                              int                       1\n",
      "n_samples                                 int                       100\n",
      "n_series                                  int                       1\n",
      "num_lr_decays                             int                       -1\n",
      "on_gpu                                    bool                      False\n",
      "optimizer                                 NoneType                  None\n",
      "optimizer_kwargs                          dict                      dict(len=0, keys=[])\n",
      "output_adapter                            Linear                    Linear: Linear(in_features=256, out_features=1, bias=True)\n",
      "padder_train                              ConstantPad1d             ConstantPad1d: ConstantPad1d(padding=(0, 1), value=0.0)\n",
      "predict_horizon                           int                       1\n",
      "prepare_data_per_node                     bool                      True\n",
      "random_seed                               int                       1\n",
      "rnn_type                                  str                       lstm\n",
      "scaler                                    TemporalNorm              TemporalNorm: TemporalNorm()\n",
      "start_padding_enabled                     bool                      False\n",
      "stat_exog_list                            list                      list(len=2)\n",
      "stat_exog_size                            int                       2\n",
      "static_encoder                            StaticCovariateEncoder    StaticCovariateEncoder: StaticCovariateEncoder(   (vsn): VariableSelectionNetwork(     (joint_grn): GRN(       (layer_norm): MaybeLayerNorm(         (ln): LayerNorm((2,), eps=0.001, elementwise_affine=True)       )       (li…\n",
      "step_size                                 int                       1\n",
      "strict_loading                            bool                      True\n",
      "temporal_encoder                          TemporalCovariateEncoder  TemporalCovariateEncoder: TemporalCovariateEncoder(   (history_vsn): VariableSelectionNetwork(     (joint_grn): GRN(       (layer_norm): MaybeLayerNorm(         (ln): LayerNorm((49,), eps=0.001, elementwise_affine=True)       …\n",
      "temporal_fusion_decoder                   TemporalFusionDecoder     TemporalFusionDecoder: TemporalFusionDecoder(   (enrichment_grn): GRN(     (layer_norm): MaybeLayerNorm(       (ln): LayerNorm((256,), eps=0.001, elementwise_affine=True)     )     (lin_a): Linear(in_features=256, out_featu…\n",
      "test_size                                 int                       0\n",
      "tgt_size                                  int                       1\n",
      "train_trajectories                        list                      list(len=150)\n",
      "trainer                                   str                       <attr_error: 'TFT' object has no attribute 'trainer'>\n",
      "trainer_kwargs                            dict                      dict(len=10, keys=['accelerator', 'devices', 'precision', 'enable_checkpointing', 'logger', 'enable_progress_bar', 'max_steps', 'callbacks']…)\n",
      "training                                  bool                      True\n",
      "val_check_steps                           int                       50\n",
      "val_size                                  int                       2\n",
      "valid_batch_size                          int                       128\n",
      "valid_loss                                SMAPE                     SMAPE: SMAPE()\n",
      "valid_trajectories                        list                      list(len=4)\n",
      "validation_step_outputs                   list                      list(len=0)\n",
      "windows_batch_size                        int                       1024\n",
      "\n",
      "[exog lists]\n",
      "  futr_exog_list: ['futr_p30.4375_k1_cos', 'futr_p30.4375_k1_sin', 'futr_hol_days_until', 'futr_ds_quarterend', 'futr_ds_qday', 'futr_ds_days_in_month', 'futr_p365.25_k3_sin', 'futr_p365.25_k1_cos', 'futr_ds_half', 'futr_hol_is_holiday', 'futr_ds_quarter', 'futr_p365.25_k1_sin', 'futr_p30.4375_k2_cos', 'futr_ds_yday', 'futr_ds_yweek', 'futr_ds_month']\n",
      "  hist_exog_list: ['hist_trend_drawdown', 'hist_trend_drawup', 'hist_roll_z_w90', 'hist_roll_z_w60', 'hist_roll_z_w30', 'hist_roll_z_w21', 'hist_roll_z_w14', 'hist_roll_z_w7', 'hist_ewm_mean_s3', 'hist_roll_z_w3', 'hist_diff_2', 'hist_stl_resid', 'hist_diff_28', 'hist_diff_14', 'hist_diff_30', 'hist_diff_60', 'hist_ft_diff1', 'hist_diff_3', 'hist_diff_21', 'hist_diff_7', 'hist_ewm_mean_s7', 'hist_pct_2', 'hist_pct_3', 'hist_pct_28', 'hist_pct_30', 'hist_stl_seasonal', 'hist_pct_14', 'hist_roll_mean_w3', 'hist_pct_1', 'hist_pct_60', 'hist_pct_21', 'hist_roll_q25_w3']\n",
      "  stat_exog_list: ['stat_ds_quarteryear', 'stat_ds_month_lbl']\n",
      "\n",
      "\n",
      "======================\n",
      "[Model 2] AutoPatchTST\n",
      "======================\n",
      "name                                      type                      value\n",
      "-------------------------------------------------------------------------\n",
      "CHECKPOINT_HYPER_PARAMS_KEY               str                       hyper_parameters\n",
      "CHECKPOINT_HYPER_PARAMS_NAME              str                       hparams_name\n",
      "CHECKPOINT_HYPER_PARAMS_TYPE              str                       hparams_type\n",
      "EXOGENOUS_FUTR                            bool                      False\n",
      "EXOGENOUS_HIST                            bool                      False\n",
      "EXOGENOUS_STAT                            bool                      False\n",
      "MULTIVARIATE                              bool                      False\n",
      "RECURRENT                                 bool                      False\n",
      "T_destination                             TypeVar                   TypeVar: ~T_destination\n",
      "alias                                     NoneType                  None\n",
      "allow_zero_length_dataloader_with_multiple_devices  bool                      False\n",
      "automatic_optimization                    bool                      True\n",
      "backend                                   str                       optuna\n",
      "call_super_init                           bool                      False\n",
      "callbacks                                 NoneType                  None\n",
      "cls_model                                 type                      type: <class 'neuralforecast.models.patchtst.PatchTST'>\n",
      "config                                    function                  function: <function BaseAuto.__init__.<locals>.config_f at 0x7354e4fc87c0>\n",
      "cpus                                      int                       32\n",
      "current_epoch                             int                       0\n",
      "default_config                            dict                      dict(len=13, keys=['input_size_multiplier', 'h', 'hidden_size', 'n_heads', 'patch_len', 'learning_rate', 'scaler_type', 'revin']…)\n",
      "device                                    device                    device: device(type='cpu')\n",
      "device_mesh                               NoneType                  None\n",
      "dtype                                     dtype                     dtype: torch.float32\n",
      "dump_patches                              bool                      False\n",
      "early_stop_patience_steps                 int                       1\n",
      "example_input_array                       NoneType                  None\n",
      "fabric                                    NoneType                  None\n",
      "futr_exog_list                            list                      list(len=0)\n",
      "global_rank                               int                       0\n",
      "global_step                               int                       0\n",
      "gpus                                      int                       1\n",
      "h                                         int                       1\n",
      "hist_exog_list                            list                      list(len=0)\n",
      "hparams                                   AttributeDict             dict(len=14, keys=['cls_model', 'h', 'loss', 'valid_loss', 'config', 'search_alg', 'num_samples', 'cpus']…)\n",
      "hparams_initial                           AttributeDict             dict(len=14, keys=['cls_model', 'h', 'loss', 'valid_loss', 'config', 'search_alg', 'num_samples', 'cpus']…)\n",
      "local_rank                                int                       0\n",
      "logger                                    NoneType                  None\n",
      "loggers                                   list                      list(len=0)\n",
      "loss                                      SMAPE                     SMAPE: SMAPE()\n",
      "model                                     PatchTST                  PatchTST: PatchTST\n",
      "num_samples                               int                       1\n",
      "on_gpu                                    bool                      False\n",
      "prepare_data_per_node                     bool                      True\n",
      "refit_with_val                            bool                      True\n",
      "results                                   Study                     Study: <optuna.study.study.Study object at 0x7354e037e810>\n",
      "search_alg                                TPESampler                TPESampler: <optuna.samplers._tpe.sampler.TPESampler object at 0x7354e4e42490>\n",
      "stat_exog_list                            list                      list(len=0)\n",
      "strict_loading                            bool                      True\n",
      "trainer                                   str                       <attr_error: AutoPatchTST is not attached to a `Trainer`.>\n",
      "training                                  bool                      True\n",
      "valid_loss                                SMAPE                     SMAPE: SMAPE()\n",
      "verbose                                   bool                      True\n",
      "===========================\n",
      "[Model 2] tuned -> PatchTST\n",
      "===========================\n",
      "name                                      type                      value\n",
      "-------------------------------------------------------------------------\n",
      "CHECKPOINT_HYPER_PARAMS_KEY               str                       hyper_parameters\n",
      "CHECKPOINT_HYPER_PARAMS_NAME              str                       hparams_name\n",
      "CHECKPOINT_HYPER_PARAMS_TYPE              str                       hparams_type\n",
      "EXOGENOUS_FUTR                            bool                      False\n",
      "EXOGENOUS_HIST                            bool                      False\n",
      "EXOGENOUS_STAT                            bool                      False\n",
      "MULTIVARIATE                              bool                      False\n",
      "RECURRENT                                 bool                      False\n",
      "T_destination                             TypeVar                   TypeVar: ~T_destination\n",
      "alias                                     NoneType                  None\n",
      "allow_zero_length_dataloader_with_multiple_devices  bool                      False\n",
      "automatic_optimization                    bool                      True\n",
      "batch_size                                int                       64\n",
      "call_super_init                           bool                      False\n",
      "current_epoch                             str                       <attr_error: 'PatchTST' object has no attribute 'current_epoch'>\n",
      "dataloader_kwargs                         NoneType                  None\n",
      "decompose_forecast                        bool                      False\n",
      "device                                    device                    device: device(type='cpu')\n",
      "device_mesh                               NoneType                  None\n",
      "drop_last_loader                          bool                      False\n",
      "dtype                                     dtype                     dtype: torch.float32\n",
      "dump_patches                              bool                      False\n",
      "early_stop_patience_steps                 int                       2\n",
      "example_input_array                       NoneType                  None\n",
      "exclude_insample_y                        bool                      False\n",
      "fabric                                    NoneType                  None\n",
      "futr_exog_list                            list                      list(len=0)\n",
      "futr_exog_size                            int                       0\n",
      "global_rank                               str                       <attr_error: 'PatchTST' object has no attribute 'global_rank'>\n",
      "global_step                               str                       <attr_error: 'PatchTST' object has no attribute 'global_step'>\n",
      "h                                         int                       1\n",
      "hist_exog_list                            list                      list(len=0)\n",
      "hist_exog_size                            int                       0\n",
      "horizon_backup                            int                       1\n",
      "hparams                                   AttributeDict             dict(len=56, keys=['h', 'input_size', 'loss', 'valid_loss', 'learning_rate', 'max_steps', 'val_check_steps', 'batch_size']…)\n",
      "hparams_initial                           AttributeDict             dict(len=56, keys=['h', 'input_size', 'loss', 'valid_loss', 'learning_rate', 'max_steps', 'val_check_steps', 'batch_size']…)\n",
      "inference_windows_batch_size              int                       1024\n",
      "input_size                                int                       4\n",
      "input_size_backup                         int                       4\n",
      "learning_rate                             float                     0.002440608669830101\n",
      "local_rank                                str                       <attr_error: 'PatchTST' object has no attribute 'local_rank'>\n",
      "logger                                    str                       <attr_error: 'PatchTST' object has no attribute 'logger'>\n",
      "loggers                                   str                       <attr_error: 'PatchTST' object has no attribute 'loggers'>\n",
      "loss                                      SMAPE                     SMAPE: SMAPE()\n",
      "lr_decay_steps                            float                     100000000.0\n",
      "lr_scheduler                              NoneType                  None\n",
      "lr_scheduler_kwargs                       dict                      dict(len=0, keys=[])\n",
      "max_steps                                 int                       800\n",
      "metrics                                   dict                      dict(len=5, keys=['train_loss', 'train_loss_step', 'train_loss_epoch', 'valid_loss', 'ptl/val_loss'])\n",
      "min_insample_fraction                     float                     0.0\n",
      "min_outsample_fraction                    float                     0.0\n",
      "model                                     PatchTST_backbone         PatchTST_backbone: PatchTST_backbone(   (revin_layer): RevIN()   (padding_patch_layer): ReplicationPad1d((0, 16))   (backbone): TSTiEncoder(     (W_P): Linear(in_features=16, out_features=128, bias=True)     (dropout): …\n",
      "n_predicts                                int                       1\n",
      "n_samples                                 int                       100\n",
      "n_series                                  int                       1\n",
      "num_lr_decays                             int                       -1\n",
      "on_gpu                                    bool                      False\n",
      "optimizer                                 NoneType                  None\n",
      "optimizer_kwargs                          dict                      dict(len=0, keys=[])\n",
      "padder_train                              ConstantPad1d             ConstantPad1d: ConstantPad1d(padding=(0, 1), value=0.0)\n",
      "predict_horizon                           int                       1\n",
      "prepare_data_per_node                     bool                      True\n",
      "random_seed                               int                       1\n",
      "scaler                                    TemporalNorm              TemporalNorm: TemporalNorm()\n",
      "start_padding_enabled                     bool                      False\n",
      "stat_exog_list                            list                      list(len=0)\n",
      "stat_exog_size                            int                       0\n",
      "step_size                                 int                       1\n",
      "strict_loading                            bool                      True\n",
      "test_size                                 int                       0\n",
      "train_trajectories                        list                      list(len=350)\n",
      "trainer                                   str                       <attr_error: 'PatchTST' object has no attribute 'trainer'>\n",
      "trainer_kwargs                            dict                      dict(len=10, keys=['accelerator', 'devices', 'precision', 'enable_checkpointing', 'logger', 'enable_progress_bar', 'max_steps', 'callbacks']…)\n",
      "training                                  bool                      True\n",
      "val_check_steps                           int                       50\n",
      "val_size                                  int                       2\n",
      "valid_batch_size                          int                       64\n",
      "valid_loss                                SMAPE                     SMAPE: SMAPE()\n",
      "valid_trajectories                        list                      list(len=8)\n",
      "validation_step_outputs                   list                      list(len=0)\n",
      "windows_batch_size                        int                       1024\n",
      "\n",
      "[exog lists]\n",
      "  futr_exog_list: []\n",
      "  hist_exog_list: []\n",
      "  stat_exog_list: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === モデルのプロパティ値を安全に一覧表示するユーティリティ ===\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    torch = None  # torch が無い環境でも動くように\n",
    "\n",
    "SIMPLE_TYPES = (str, int, float, bool, type(None))\n",
    "\n",
    "def _summarize_value(v, maxlen=200):\n",
    "    \"\"\"値を安全・短く要約して表示用に整形\"\"\"\n",
    "    try:\n",
    "        # 素朴な型はそのまま（長すぎる文字列は詰める）\n",
    "        if isinstance(v, SIMPLE_TYPES):\n",
    "            s = str(v)\n",
    "            return (s[:maxlen] + \"…\") if len(s) > maxlen else s\n",
    "\n",
    "        # numpy\n",
    "        if isinstance(v, np.ndarray):\n",
    "            return f\"np.ndarray(shape={v.shape}, dtype={v.dtype})\"\n",
    "\n",
    "        # pandas\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            return f\"DataFrame(shape={v.shape}, columns={list(v.columns)[:6]}{'…' if v.shape[1]>6 else ''})\"\n",
    "        if isinstance(v, pd.Series):\n",
    "            return f\"Series(len={len(v)}, name={v.name}, dtype={v.dtype})\"\n",
    "\n",
    "        # torch\n",
    "        if torch is not None:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                dev = v.device if hasattr(v, 'device') else 'cpu'\n",
    "                return f\"torch.Tensor(shape={tuple(v.shape)}, dtype={v.dtype}, device={dev})\"\n",
    "            if hasattr(v, \"__class__\") and v.__class__.__module__.startswith(\"torch.optim\"):\n",
    "                return f\"{v.__class__.__name__}(param_groups={len(getattr(v,'param_groups',[]))})\"\n",
    "\n",
    "        # リスト／タプル／セット／辞書はサイズだけ\n",
    "        if isinstance(v, (list, tuple, set)):\n",
    "            return f\"{type(v).__name__}(len={len(v)})\"\n",
    "        if isinstance(v, dict):\n",
    "            keys = list(v.keys())[:8]\n",
    "            return f\"dict(len={len(v)}, keys={keys}{'…' if len(v)>8 else ''})\"\n",
    "\n",
    "        # その他のオブジェクトは型名＋一部repr\n",
    "        r = repr(v)\n",
    "        r = r.replace(\"\\n\", \" \")\n",
    "        if len(r) > maxlen:\n",
    "            r = r[:maxlen] + \"…\"\n",
    "        return f\"{v.__class__.__name__}: {r}\"\n",
    "    except Exception as e:\n",
    "        return f\"<summarize_error: {e}>\"\n",
    "\n",
    "def collect_properties(obj, include_private=False):\n",
    "    \"\"\"\n",
    "    obj の「公開属性（__dict__）＋ @property を含む」名前→値の辞書を作る。\n",
    "    値の取得で例外が出ても飲み込んで続行。\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    out = {}\n",
    "\n",
    "    # 1) 実インスタンス属性\n",
    "    for k, v in getattr(obj, \"__dict__\", {}).items():\n",
    "        if not include_private and k.startswith(\"_\"):\n",
    "            continue\n",
    "        seen.add(k)\n",
    "        out[k] = v\n",
    "\n",
    "    # 2) @property で定義された属性\n",
    "    try:\n",
    "        for name, member in inspect.getmembers(type(obj)):\n",
    "            if not isinstance(member, property):\n",
    "                continue\n",
    "            if not include_private and name.startswith(\"_\"):\n",
    "                continue\n",
    "            if name in seen:\n",
    "                continue\n",
    "            try:\n",
    "                out[name] = getattr(obj, name)\n",
    "            except Exception as e:\n",
    "                out[name] = f\"<property_error: {e}>\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) dir で拾えるその他の公開属性（callable を除く）\n",
    "    for name in dir(obj):\n",
    "        if not include_private and name.startswith(\"_\"):\n",
    "            continue\n",
    "        if name in seen:\n",
    "            continue\n",
    "        try:\n",
    "            val = getattr(obj, name)\n",
    "        except Exception as e:\n",
    "            val = f\"<attr_error: {e}>\"\n",
    "        # 関数やメソッドは除外\n",
    "        if inspect.ismethod(val) or inspect.isfunction(val):\n",
    "            continue\n",
    "        out[name] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "def print_properties(obj, title=None, sort=True, include_private=False):\n",
    "    \"\"\"プロパティを表形式でプリント（名前 / 型 / 要約）\"\"\"\n",
    "    if title:\n",
    "        print(\"=\"*len(title))\n",
    "        print(title)\n",
    "        print(\"=\"*len(title))\n",
    "    props = collect_properties(obj, include_private=include_private)\n",
    "    items = list(props.items())\n",
    "    if sort:\n",
    "        items.sort(key=lambda kv: kv[0])\n",
    "\n",
    "    name_w = max(8, min(40, max((len(k) for k,_ in items), default=8)))\n",
    "    type_w = 24\n",
    "\n",
    "    header = f\"{'name'.ljust(name_w)}  {'type'.ljust(type_w)}  value\"\n",
    "    print(header)\n",
    "    print(\"-\"*len(header))\n",
    "    for k, v in items:\n",
    "        t = type(v).__name__\n",
    "        s = _summarize_value(v)\n",
    "        print(f\"{k.ljust(name_w)}  {t.ljust(type_w)}  {s}\")\n",
    "\n",
    "def print_nf_models_properties(nf):\n",
    "    \"\"\"\n",
    "    NeuralForecast インスタンスから各モデルを辿って表示。\n",
    "    Auto系モデルの場合は、チューニング後の実体（m.model）があればそれも併せて表示。\n",
    "    \"\"\"\n",
    "    for i, m in enumerate(getattr(nf, \"models\", []), start=1):\n",
    "        print_properties(m, title=f\"[Model {i}] {m.__class__.__name__}\")\n",
    "        # AutoTFT/AutoPatchTST などは、最適化後に .model に実体が入る\n",
    "        tuned = getattr(m, \"model\", None)\n",
    "        if tuned is not None and tuned is not m:\n",
    "            print_properties(tuned, title=f\"[Model {i}] tuned -> {tuned.__class__.__name__}\")\n",
    "\n",
    "        # よく見る追加情報（存在すれば）\n",
    "        trkw = getattr(m, \"trainer_kwargs\", None)\n",
    "        if trkw is not None:\n",
    "            print_properties(trkw, title=f\"[Model {i}] trainer_kwargs (dict)\")\n",
    "        futr = getattr(m, \"futr_exog_list\", None)\n",
    "        hist = getattr(m, \"hist_exog_list\", None)\n",
    "        stat = getattr(m, \"stat_exog_list\", None)\n",
    "        if any(x is not None for x in (futr, hist, stat)):\n",
    "            print(\"\\n[exog lists]\")\n",
    "            if futr is not None: print(f\"  futr_exog_list: {futr}\")\n",
    "            if hist is not None: print(f\"  hist_exog_list: {hist}\")\n",
    "            if stat is not None: print(f\"  stat_exog_list: {stat}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# === 使い方（学習後 or ロード後）===\n",
    "# 例: nf = NeuralForecast(models=[...], ...)\n",
    "# nf.fit(...)\n",
    "\n",
    "# 全モデルのプロパティ一覧を表示\n",
    "print_nf_models_properties(nf)\n",
    "\n",
    "# 単一モデルだけ見たい場合:\n",
    "# print_properties(nf.models[0], title=\"First model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
