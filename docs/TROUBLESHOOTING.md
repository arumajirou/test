# è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ - ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## ğŸ“— ç›®æ¬¡

- [ã‚¯ã‚¤ãƒƒã‚¯è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ](#ã‚¯ã‚¤ãƒƒã‚¯è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ)
- [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å•é¡Œ](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å•é¡Œ)
- [ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®å•é¡Œ](#ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®å•é¡Œ)
- [ãƒ¡ãƒ¢ãƒªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å•é¡Œ](#ãƒ¡ãƒ¢ãƒªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å•é¡Œ)
- [æœ€é©åŒ–ã®å•é¡Œ](#æœ€é©åŒ–ã®å•é¡Œ)
- [MLflowã®å•é¡Œ](#mlflowã®å•é¡Œ)
- [GPUé–¢é€£ã®å•é¡Œ](#gpué–¢é€£ã®å•é¡Œ)
- [ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ¥ã‚¬ã‚¤ãƒ‰](#ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ¥ã‚¬ã‚¤ãƒ‰)
- [ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•](#ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•)
- [FAQ](#faq)

---

## ã‚¯ã‚¤ãƒƒã‚¯è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å•é¡Œã«ç›´é¢ã—ãŸã‚‰ã€ã¾ãšä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š

```python
# diagnostic_check.py - åŒ…æ‹¬çš„ãªè¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
import sys
import torch
import pandas as pd
import numpy as np

def run_diagnostics():
    """ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œ"""
    
    print("=" * 80)
    print("ğŸ” SYSTEM DIAGNOSTICS")
    print("=" * 80)
    
    # 1. Pythonç’°å¢ƒ
    print("\n1. Python Environment:")
    print(f"   Python version: {sys.version}")
    print(f"   Python executable: {sys.executable}")
    
    # 2. å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
    print("\n2. Required Packages:")
    try:
        import neuralforecast
        print(f"   âœ… neuralforecast: {neuralforecast.__version__}")
    except ImportError:
        print("   âŒ neuralforecast: NOT INSTALLED")
    
    try:
        import optuna
        print(f"   âœ… optuna: {optuna.__version__}")
    except ImportError:
        print("   âŒ optuna: NOT INSTALLED")
    
    try:
        import ray
        print(f"   âœ… ray: {ray.__version__}")
    except ImportError:
        print("   âŒ ray: NOT INSTALLED")
    
    try:
        import mlflow
        print(f"   âœ… mlflow: {mlflow.__version__}")
    except ImportError:
        print("   âŒ mlflow: NOT INSTALLED")
    
    # 3. PyTorch ã¨CUDA
    print("\n3. PyTorch & CUDA:")
    print(f"   PyTorch version: {torch.__version__}")
    print(f"   CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   GPU count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"      Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
    
    # 4. ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹
    print("\n4. System Resources:")
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"   Total RAM: {mem.total / 1e9:.2f} GB")
        print(f"   Available RAM: {mem.available / 1e9:.2f} GB")
        print(f"   CPU count: {psutil.cpu_count()}")
        disk = psutil.disk_usage('/')
        print(f"   Disk free: {disk.free / 1e9:.2f} GB")
    except ImportError:
        print("   âš ï¸  psutil not installed (optional)")
    
    # 5. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    print("\n5. Project Modules:")
    modules = [
        'auto_model_factory',
        'validation',
        'search_algorithm_selector',
        'model_characteristics',
        'mlflow_integration',
        'logging_config'
    ]
    
    for module in modules:
        try:
            __import__(module)
            print(f"   âœ… {module}")
        except ImportError as e:
            print(f"   âŒ {module}: {e}")
    
    print("\n" + "=" * 80)
    print("âœ… DIAGNOSTICS COMPLETE")
    print("=" * 80)

if __name__ == "__main__":
    run_diagnostics()
```

**ä½¿ç”¨æ–¹æ³•**:
```bash
python diagnostic_check.py
```

---

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å•é¡Œ

### å•é¡Œ: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼

#### ç—‡çŠ¶
```python
ImportError: No module named 'neuralforecast'
ImportError: No module named 'optuna'
```

#### è§£æ±ºç­–

**ã‚¹ãƒ†ãƒƒãƒ—1**: ä»®æƒ³ç’°å¢ƒã®ç¢ºèª
```bash
# ä»®æƒ³ç’°å¢ƒãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª
which python
# ã¾ãŸã¯
python --version
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install --upgrade neuralforecast
pip install --upgrade optuna
pip install --upgrade 'ray[tune]'
pip install --upgrade mlflow
pip install --upgrade pytorch-lightning

# GPUç‰ˆPyTorchï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: ä¾å­˜é–¢ä¿‚ã®ç¢ºèª
```bash
pip check
```

---

### å•é¡Œ: CUDA/GPUèªè­˜ã‚¨ãƒ©ãƒ¼

#### ç—‡çŠ¶
```python
RuntimeError: CUDA out of memory
RuntimeError: No CUDA GPUs are available
```

#### è§£æ±ºç­–

**è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:
```python
import torch

print("CUDA available:", torch.cuda.is_available())
print("CUDA version:", torch.version.cuda)
print("GPU count:", torch.cuda.device_count())

if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}:", torch.cuda.get_device_name(i))
        props = torch.cuda.get_device_properties(i)
        print(f"  Total memory: {props.total_memory / 1e9:.2f} GB")
        print(f"  Available memory: {(props.total_memory - torch.cuda.memory_allocated(i)) / 1e9:.2f} GB")
else:
    print("âš ï¸ CUDA not available")
    print("Solution:")
    print("1. Check NVIDIA driver: nvidia-smi")
    print("2. Reinstall CUDA-compatible PyTorch")
    print("3. Or use CPU-only mode: gpus=0")
```

**è§£æ±ºæ³•1**: ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ç¢ºèª
```bash
# NVIDIA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã¨CUDAã®ç¢ºèª
nvidia-smi

# CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆã£ãŸPyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# CUDA 11.8ã®å ´åˆ:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**è§£æ±ºæ³•2**: CPU-onlyãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
```python
auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    gpus=0  # GPUã‚’ä½¿ç”¨ã—ãªã„
)
```

---

## ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®å•é¡Œ

### å•é¡Œ: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼

#### ç—‡çŠ¶
```
ValidationError: Missing required column: 'ds'
ValidationError: Invalid date format in column 'ds'
ValidationError: Missing values found in column 'y'
```

#### è§£æ±ºç­–

**è¨ºæ–­ã¨ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:
```python
import pandas as pd
import numpy as np
from datetime import datetime

def diagnose_and_fix_dataset(df, fix=False):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å•é¡Œã‚’è¨ºæ–­ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ä¿®æ­£"""
    
    print("ğŸ” Diagnosing dataset...\n")
    
    issues = []
    fixes = {}
    
    # 1. å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯
    required_cols = ['unique_id', 'ds', 'y']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        issues.append(f"Missing columns: {missing_cols}")
        print(f"âŒ Missing required columns: {missing_cols}")
        print(f"   Available columns: {list(df.columns)}")
    else:
        print("âœ… All required columns present")
    
    # 2. ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒã‚§ãƒƒã‚¯
    if 'ds' in df.columns:
        if not pd.api.types.is_datetime64_any_dtype(df['ds']):
            issues.append("Column 'ds' is not datetime type")
            print("âŒ 'ds' column is not datetime")
            print(f"   Current type: {df['ds'].dtype}")
            
            if fix:
                try:
                    df['ds'] = pd.to_datetime(df['ds'])
                    fixes['ds_converted'] = True
                    print("   âœ… Fixed: Converted to datetime")
                except Exception as e:
                    print(f"   âš ï¸  Could not convert: {e}")
        else:
            print("âœ… 'ds' column is datetime type")
    
    if 'y' in df.columns:
        if not pd.api.types.is_numeric_dtype(df['y']):
            issues.append("Column 'y' is not numeric")
            print("âŒ 'y' column is not numeric")
            print(f"   Current type: {df['y'].dtype}")
            
            if fix:
                try:
                    df['y'] = pd.to_numeric(df['y'])
                    fixes['y_converted'] = True
                    print("   âœ… Fixed: Converted to numeric")
                except Exception as e:
                    print(f"   âš ï¸  Could not convert: {e}")
        else:
            print("âœ… 'y' column is numeric type")
    
    # 3. æ¬ æå€¤ã®ãƒã‚§ãƒƒã‚¯
    if df.isnull().any().any():
        null_counts = df.isnull().sum()
        null_cols = null_counts[null_counts > 0]
        issues.append(f"Missing values found")
        print(f"\nâŒ Missing values detected:")
        for col, count in null_cols.items():
            print(f"   {col}: {count} ({count/len(df)*100:.2f}%)")
        
        if fix:
            df_before = len(df)
            df = df.dropna()
            df_after = len(df)
            fixes['rows_dropped'] = df_before - df_after
            print(f"   âœ… Fixed: Dropped {df_before - df_after} rows with missing values")
    else:
        print("\nâœ… No missing values")
    
    # 4. é‡è¤‡ã®ãƒã‚§ãƒƒã‚¯
    if 'unique_id' in df.columns and 'ds' in df.columns:
        duplicates = df.duplicated(subset=['unique_id', 'ds']).sum()
        if duplicates > 0:
            issues.append(f"{duplicates} duplicate records")
            print(f"\nâŒ Found {duplicates} duplicate records")
            
            if fix:
                df = df.drop_duplicates(subset=['unique_id', 'ds'])
                fixes['duplicates_removed'] = duplicates
                print(f"   âœ… Fixed: Removed {duplicates} duplicates")
        else:
            print("\nâœ… No duplicates found")
    
    # 5. ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®ãƒã‚§ãƒƒã‚¯
    if 'y' in df.columns:
        y_stats = df['y'].describe()
        print(f"\nğŸ“Š Value statistics:")
        print(f"   Min: {y_stats['min']:.2f}")
        print(f"   Max: {y_stats['max']:.2f}")
        print(f"   Mean: {y_stats['mean']:.2f}")
        print(f"   Std: {y_stats['std']:.2f}")
        
        # æ¥µç«¯ãªå¤–ã‚Œå€¤ã®è­¦å‘Š
        Q1 = df['y'].quantile(0.25)
        Q3 = df['y'].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df['y'] < Q1 - 3*IQR) | (df['y'] > Q3 + 3*IQR)]
        
        if len(outliers) > 0:
            print(f"   âš ï¸  {len(outliers)} extreme outliers detected")
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    if not issues:
        print("âœ… Dataset is clean and ready to use!")
    else:
        print(f"âŒ Found {len(issues)} issue(s)")
        if fix and fixes:
            print(f"âœ… Applied {len(fixes)} fix(es)")
    print("=" * 60)
    
    return df if fix else None, issues, fixes

# ä½¿ç”¨ä¾‹
df = pd.read_csv('your_data.csv')

# è¨ºæ–­ã®ã¿
diagnose_and_fix_dataset(df, fix=False)

# è¨ºæ–­ã¨ä¿®æ­£
df_fixed, issues, fixes = diagnose_and_fix_dataset(df, fix=True)
if df_fixed is not None:
    df_fixed.to_csv('data_fixed.csv', index=False)
    print("\nğŸ’¾ Fixed dataset saved to 'data_fixed.csv'")
```

---

### å•é¡Œ: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ãŒå¤§ãã™ãã‚‹

#### ç—‡çŠ¶
```
ValidationWarning: Forecast horizon (h=365) is very large compared to dataset size
```

#### è§£æ±ºç­–

**æ¨å¥¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:
- äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ `h` ã¯ã€1ç³»åˆ—ã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã®10-20%ä»¥ä¸‹ãŒç†æƒ³
- é•·æœŸäºˆæ¸¬ãŒå¿…è¦ãªå ´åˆã¯ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†

**ä¿®æ­£ä¾‹**:
```python
import pandas as pd

df = pd.read_csv('data.csv')

# ç³»åˆ—ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’ç¢ºèª
points_per_series = df.groupby('unique_id').size()
min_points = points_per_series.min()
max_points = points_per_series.max()

print(f"Data points per series:")
print(f"  Min: {min_points}")
print(f"  Max: {max_points}")
print(f"  Mean: {points_per_series.mean():.0f}")

# æ¨å¥¨äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
recommended_h = int(min_points * 0.15)  # 15%
print(f"\nRecommended h: {recommended_h}")

# é•·æœŸäºˆæ¸¬ãŒå¿…è¦ãªå ´åˆã®å¯¾ç­–
if recommended_h < 30:  # ä¾‹: 30æ—¥åˆ†ã®äºˆæ¸¬ãŒå¿…è¦
    print("\nâš ï¸  Need more historical data for long-term forecasting")
    print("Options:")
    print("1. Collect more historical data")
    print("2. Use hierarchical forecasting")
    print("3. Split into multiple shorter-horizon forecasts")
```

---

## ãƒ¡ãƒ¢ãƒªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å•é¡Œ

### å•é¡Œ: OOM (Out of Memory) ã‚¨ãƒ©ãƒ¼

#### ç—‡çŠ¶
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
MemoryError: Unable to allocate array with shape...
```

#### è§£æ±ºç­–

**è§£æ±ºæ³•1**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å‰Šæ¸›
```python
from ray import tune

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„è¨­å®š
memory_efficient_config = {
    'batch_size': tune.choice([16, 32]),  # å°ã•ã„ãƒãƒƒãƒ
    'input_size': tune.choice([7, 14]),   # çŸ­ã„ãƒ«ãƒƒã‚¯ãƒãƒƒã‚¯
    'hidden_size': tune.choice([128, 256])  # å°ã•ã„ãƒ¢ãƒ‡ãƒ«
}

auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    config=memory_efficient_config,
    gpus=1
)
```

**è§£æ±ºæ³•2**: GPUãƒ¡ãƒ¢ãƒªã®ã‚¯ãƒªã‚¢
```python
import torch
import gc

def clear_gpu_memory():
    """GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        print("âœ… GPU memory cleared")
        
        # ç©ºããƒ¡ãƒ¢ãƒªã®ç¢ºèª
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1e9
            cached = torch.cuda.memory_reserved(i) / 1e9
            total = torch.cuda.get_device_properties(i).total_memory / 1e9
            free = total - allocated
            print(f"GPU {i}: {free:.2f}GB free / {total:.2f}GB total")

# ä½¿ç”¨å‰ã«ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
clear_gpu_memory()

auto_model = create_auto_model(...)
```

**è§£æ±ºæ³•3**: ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
```python
import pandas as pd

def subsample_data(df, fraction=0.5, random_state=42):
    """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
    series_ids = df['unique_id'].unique()
    n_sample = int(len(series_ids) * fraction)
    
    sampled_ids = pd.Series(series_ids).sample(
        n=n_sample,
        random_state=random_state
    )
    
    df_sampled = df[df['unique_id'].isin(sampled_ids)]
    
    print(f"Subsampled: {len(df_sampled)} rows ({len(df)} original)")
    print(f"Series: {n_sample} / {len(series_ids)}")
    
    return df_sampled

# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
if len(df) > 100000:
    df_train = subsample_data(df, fraction=0.5)
else:
    df_train = df
```

**è§£æ±ºæ³•4**: è¤‡æ•°GPUã®ä½¿ç”¨
```python
from auto_model_factory import OptimizationConfig

# ãƒãƒ«ãƒGPUè¨­å®š
opt_config = OptimizationConfig(
    backend="ray",  # Rayã¯è¤‡æ•°GPUã«å„ªã‚Œã¦ã„ã‚‹
    num_samples=30,
    cpus=16,
    gpus=4,  # è¤‡æ•°GPU
    use_pruning=True
)

# ãƒ¡ãƒ¢ãƒªãŒ4ã¤ã®GPUã«åˆ†æ•£ã•ã‚Œã‚‹
auto_model = create_auto_model(
    model_name="TFT",
    h=24,
    dataset=df,
    optimization_config=opt_config
)
```

---

### å•é¡Œ: å­¦ç¿’ãŒé…ã„

#### ç—‡çŠ¶
- æœ€é©åŒ–ã«äºˆæƒ³ä»¥ä¸Šã®æ™‚é–“ãŒã‹ã‹ã‚‹
- GPUãŒ100%ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„

#### è§£æ±ºç­–

**è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:
```python
import time
import torch
from auto_model_factory import create_auto_model

def measure_optimization_speed(df, model_name, num_samples=5):
    """æœ€é©åŒ–é€Ÿåº¦ã‚’æ¸¬å®š"""
    
    print(f"ğŸ” Measuring speed for {model_name}...")
    print(f"   Trials: {num_samples}")
    print(f"   GPU: {torch.cuda.is_available()}")
    
    start_time = time.time()
    
    auto_model = create_auto_model(
        model_name=model_name,
        h=7,
        dataset=df,
        num_samples=num_samples,
        gpus=1 if torch.cuda.is_available() else 0,
        verbose=False
    )
    
    elapsed = time.time() - start_time
    time_per_trial = elapsed / num_samples
    
    print(f"\nğŸ“Š Results:")
    print(f"   Total time: {elapsed:.1f}s")
    print(f"   Time per trial: {time_per_trial:.1f}s")
    print(f"   Estimated for 50 trials: {time_per_trial * 50 / 60:.1f} min")
    
    return time_per_trial

# ä½¿ç”¨ä¾‹
df = pd.read_csv('data.csv')
measure_optimization_speed(df, "NHITS", num_samples=5)
```

**é«˜é€ŸåŒ–ç­–**:

1. **è©¦è¡Œå›æ•°ã®æœ€é©åŒ–**:
```python
from search_algorithm_selector import recommend_num_samples, ModelComplexity, DatasetSize

# ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸæ¨å¥¨å€¤ã‚’ä½¿ç”¨
num_samples, explanation = recommend_num_samples(
    model_complexity=ModelComplexity.MODERATE,
    dataset_size=DatasetSize.MEDIUM,
    time_budget_hours=1.0  # 1æ™‚é–“ä»¥å†…
)

print(f"Recommended trials: {num_samples}")
```

2. **æ—©æœŸåœæ­¢ã®æ´»ç”¨**:
```python
from auto_model_factory import OptimizationConfig

opt_config = OptimizationConfig(
    backend="optuna",
    num_samples=50,
    use_pruning=True,  # è¦‹è¾¼ã¿ã®ãªã„è©¦è¡Œã‚’æ—©æœŸåœæ­¢
    time_budget_hours=2.0  # æ™‚é–“åˆ¶é™
)
```

3. **ä¸¦åˆ—å®Ÿè¡Œ**:
```python
# Ray Tuneã§ä¸¦åˆ—å®Ÿè¡Œ
opt_config = OptimizationConfig(
    backend="ray",
    num_samples=40,
    cpus=16,  # å¤šãã®CPU
    gpus=2    # è¤‡æ•°GPU
)

# Optunaã§ã‚‚ä¸¦åˆ—å¯èƒ½ï¼ˆå®Ÿé¨“çš„ï¼‰
import optuna
optuna.create_study(n_jobs=4)  # 4ä¸¦åˆ—
```

4. **ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é–‹å§‹**:
```python
# ã¾ãšè»½é‡ãƒ¢ãƒ‡ãƒ«ã§è¨­å®šã‚’ç¢ºèª
quick_model = create_auto_model(
    model_name="MLP",  # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«
    h=7,
    dataset=df,
    num_samples=10
)

# è‰¯å¥½ãªã‚‰æœ¬æ ¼çš„ãªãƒ¢ãƒ‡ãƒ«
final_model = create_auto_model(
    model_name="TFT",
    h=7,
    dataset=df,
    num_samples=50
)
```

---

## æœ€é©åŒ–ã®å•é¡Œ

### å•é¡Œ: æœ€é©åŒ–ãŒåæŸã—ãªã„

#### ç—‡çŠ¶
- è©¦è¡Œã‚’é‡ã­ã¦ã‚‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæ”¹å–„ã—ãªã„
- æœ€è‰¯ã‚¹ã‚³ã‚¢ãŒå¤‰ã‚ã‚‰ãªã„
- æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãŒé«˜æ­¢ã¾ã‚Š

#### è§£æ±ºç­–

**è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:
```python
import mlflow
import pandas as pd
import matplotlib.pyplot as plt

def analyze_optimization_history(experiment_name):
    """æœ€é©åŒ–å±¥æ­´ã‚’åˆ†æ"""
    
    # MLflowã‹ã‚‰å±¥æ­´ã‚’å–å¾—
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if not experiment:
        print(f"âŒ Experiment '{experiment_name}' not found")
        return
    
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["start_time ASC"]
    )
    
    if len(runs) == 0:
        print("âŒ No runs found")
        return
    
    print(f"ğŸ“Š Optimization Analysis for '{experiment_name}'")
    print(f"   Total trials: {len(runs)}")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡ºï¼ˆæå¤±å€¤ãªã©ï¼‰
    metric_cols = [col for col in runs.columns if col.startswith('metrics.')]
    
    if len(metric_cols) == 0:
        print("âš ï¸  No metrics found")
        return
    
    # æœ€åˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
    metric_col = metric_cols[0]
    metric_name = metric_col.replace('metrics.', '')
    
    values = runs[metric_col].dropna()
    
    print(f"\nğŸ“ˆ Metric: {metric_name}")
    print(f"   Best: {values.min():.4f}")
    print(f"   Worst: {values.max():.4f}")
    print(f"   Mean: {values.mean():.4f}")
    print(f"   Std: {values.std():.4f}")
    
    # åæŸã®åˆ¤å®š
    # æœ€åˆã®10è©¦è¡Œã¨æœ€å¾Œã®10è©¦è¡Œã‚’æ¯”è¼ƒ
    if len(values) >= 20:
        first_10 = values.iloc[:10].mean()
        last_10 = values.iloc[-10:].mean()
        improvement = (first_10 - last_10) / first_10 * 100
        
        print(f"\nğŸ¯ Convergence:")
        print(f"   First 10 trials avg: {first_10:.4f}")
        print(f"   Last 10 trials avg: {last_10:.4f}")
        print(f"   Improvement: {improvement:.1f}%")
        
        if improvement < 5:
            print("\nâš ï¸  Warning: Poor convergence detected")
            print("Possible causes:")
            print("1. Search space too large")
            print("2. Learning rate issues")
            print("3. Model too complex for data")
            print("4. Need more trials")
    
    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 6))
    plt.plot(values.values)
    plt.axhline(values.min(), color='r', linestyle='--', label='Best')
    plt.xlabel('Trial')
    plt.ylabel(metric_name)
    plt.title(f'Optimization History: {experiment_name}')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'{experiment_name}_history.png')
    print(f"\nğŸ’¾ Plot saved to '{experiment_name}_history.png'")

# ä½¿ç”¨ä¾‹
analyze_optimization_history("my_experiment")
```

**è§£æ±ºç­–**:

1. **æ¢ç´¢ç©ºé–“ã‚’ç‹­ã‚ã‚‹**:
```python
from ray import tune

# åºƒã™ãã‚‹æ¢ç´¢ç©ºé–“ï¼ˆåæŸã—ã«ãã„ï¼‰
wide_config = {
    'max_steps': tune.choice([500, 1000, 2000, 3000, 5000]),
    'learning_rate': tune.loguniform(1e-6, 1e-1),
    'batch_size': tune.choice([16, 32, 64, 128, 256, 512])
}

# ç„¦ç‚¹ã‚’çµã£ãŸæ¢ç´¢ç©ºé–“ï¼ˆåæŸã—ã‚„ã™ã„ï¼‰
focused_config = {
    'max_steps': tune.choice([1000, 2000]),  # ç‹­ã„ç¯„å›²
    'learning_rate': tune.loguniform(1e-4, 1e-2),  # ç¾å®Ÿçš„ãªç¯„å›²
    'batch_size': tune.choice([64, 128])  # 2ã¤ã®é¸æŠè‚¢
}
```

2. **è©¦è¡Œå›æ•°ã‚’å¢—ã‚„ã™**:
```python
# ä¸ååˆ†
auto_model = create_auto_model(..., num_samples=10)

# ã‚ˆã‚Šè‰¯ã„
auto_model = create_auto_model(..., num_samples=50)

# å¾¹åº•çš„
auto_model = create_auto_model(..., num_samples=100)
```

3. **ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã™**:
```python
from search_algorithm_selector import SearchAlgorithmSelector, ModelComplexity, DatasetSize

selector = SearchAlgorithmSelector(backend="optuna")

# ç•°ãªã‚‹æˆ¦ç•¥ã‚’è©¦ã™
strategies = []

# TPEï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
strategy_tpe = selector.select_algorithm(
    model_complexity=ModelComplexity.MODERATE,
    dataset_size=DatasetSize.MEDIUM,
    num_samples=50
)
strategies.append(("TPE", strategy_tpe))

# CMA-ESï¼ˆé€£ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¼·ã„ï¼‰
strategy_cmaes = selector.select_algorithm(
    model_complexity=ModelComplexity.SIMPLE,
    dataset_size=DatasetSize.SMALL,
    num_samples=50
)
strategies.append(("CMA-ES", strategy_cmaes))

for name, strategy in strategies:
    print(f"Testing {name}: {strategy.algorithm_name}")
```

4. **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰é–‹å§‹**:
```python
# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãªã—ã§è©¦ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯èª¿æ•´æ¸ˆã¿ï¼‰
auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    config=None,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨
    num_samples=50
)
```

---

### å•é¡Œ: ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã™ã‚‹

#### ç—‡çŠ¶
```
OptimizationError: All trials failed
RuntimeError: Trial execution failed
```

#### è§£æ±ºç­–

**ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ**:
```python
import logging
from auto_model_factory import create_auto_model

# è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
logging.basicConfig(level=logging.DEBUG)

try:
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,
        num_samples=1,  # ã¾ãš1è©¦è¡Œã ã‘
        verbose=True
    )
except Exception as e:
    print(f"âŒ Error: {e}")
    import traceback
    traceback.print_exc()
```

**ä¸€èˆ¬çš„ãªåŸå› ã¨è§£æ±º**:

1. **ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œ**:
```python
from validation import DataValidator

# ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼
validator = DataValidator()
result = validator.validate_dataset(df)

if not result.is_valid:
    print("Data issues:")
    for error in result.errors:
        print(f"  - {error}")
```

2. **è¨­å®šã®å•é¡Œ**:
```python
from validation import ConfigValidator

# è¨­å®šã‚’æ¤œè¨¼
validator = ConfigValidator(strict_mode=True)
result = validator.validate_backend_config(
    backend="optuna",
    config=my_config,
    num_samples=50,
    cpus=4,
    gpus=1
)

if not result.is_valid:
    print("Config issues:")
    for error in result.errors:
        print(f"  - {error}")
    
    if result.corrected_config:
        print("Suggested corrections:")
        print(result.corrected_config)
```

3. **ãƒªã‚½ãƒ¼ã‚¹ã®å•é¡Œ**:
```python
# ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®šã§è©¦ã™
minimal_config = {
    'max_steps': 100,  # éå¸¸ã«å°‘ãªã„
    'batch_size': 32,
    'learning_rate': 0.001
}

auto_model = create_auto_model(
    model_name="MLP",  # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«
    h=7,
    dataset=df.head(1000),  # ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šæ¸›
    config=minimal_config,
    num_samples=1,
    gpus=0  # CPUã®ã¿
)
```

---

## MLflowã®å•é¡Œ

### å•é¡Œ: MLflowã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ããªã„

#### ç—‡çŠ¶
```
MlflowException: Connection refused
RequestException: Connection error
```

#### è§£æ±ºç­–

**è¨ºæ–­ã¨è§£æ±º**:
```python
from validation import ConfigValidator
import mlflow
import requests

def diagnose_mlflow_connection(tracking_uri="http://localhost:5000"):
    """MLflowæ¥ç¶šã‚’è¨ºæ–­"""
    
    print(f"ğŸ” Diagnosing MLflow connection: {tracking_uri}\n")
    
    # 1. ã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”ç¢ºèª
    try:
        response = requests.get(tracking_uri, timeout=5)
        print(f"âœ… Server responded: {response.status_code}")
    except requests.exceptions.ConnectionError:
        print("âŒ Connection refused")
        print("Solutions:")
        print("1. Start MLflow server: mlflow ui --host 0.0.0.0 --port 5000")
        print("2. Check if port is already in use: netstat -an | grep 5000")
        print("3. Try different port: mlflow ui --port 5001")
        return False
    except requests.exceptions.Timeout:
        print("âŒ Connection timeout")
        print("Server may be overloaded or network issues")
        return False
    
    # 2. MLflow APIã®ç¢ºèª
    try:
        mlflow.set_tracking_uri(tracking_uri)
        experiments = mlflow.search_experiments()
        print(f"âœ… MLflow API works: {len(experiments)} experiments found")
        return True
    except Exception as e:
        print(f"âŒ MLflow API error: {e}")
        return False

# è¨ºæ–­å®Ÿè¡Œ
is_connected = diagnose_mlflow_connection()

if not is_connected:
    print("\nğŸ’¡ Workaround: Disable MLflow")
    print("auto_model = create_auto_model(..., use_mlflow=False)")
```

**MLflowã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•**:
```bash
# åŸºæœ¬çš„ãªèµ·å‹•
mlflow ui

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ãƒˆ
mlflow ui --port 5001

# å¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã™ã‚‹
mlflow ui --host 0.0.0.0 --port 5000

# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¹ãƒˆã‚¢ã‚’æŒ‡å®š
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

---

### å•é¡Œ: MLflowã®å®Ÿé¨“ãŒé‡è¤‡ã™ã‚‹

#### ç—‡çŠ¶
- åŒã˜åå‰ã®å®Ÿé¨“ãŒè¤‡æ•°ä½œæˆã•ã‚Œã‚‹
- å®Ÿé¨“ã®ç®¡ç†ãŒæ··ä¹±ã™ã‚‹

#### è§£æ±ºç­–

```python
import mlflow

def get_or_create_experiment(experiment_name):
    """å®Ÿé¨“ã‚’å–å¾—ã¾ãŸã¯ä½œæˆï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰"""
    
    # æ—¢å­˜ã®å®Ÿé¨“ã‚’æ¤œç´¢
    experiment = mlflow.get_experiment_by_name(experiment_name)
    
    if experiment is None:
        # å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        experiment_id = mlflow.create_experiment(experiment_name)
        print(f"âœ… Created new experiment: {experiment_name}")
    else:
        experiment_id = experiment.experiment_id
        print(f"âœ… Using existing experiment: {experiment_name}")
    
    mlflow.set_experiment(experiment_name)
    return experiment_id

# ä½¿ç”¨ä¾‹
experiment_id = get_or_create_experiment("my_forecasting_project")

# ã“ã‚Œã§å®‰å…¨ã«å®Ÿé¨“ã‚’å®Ÿè¡Œ
with mlflow.start_run():
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,
        use_mlflow=True,
        verbose=True
    )
```

---

## GPUé–¢é€£ã®å•é¡Œ

### å•é¡Œ: GPUãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯

#### ç—‡çŠ¶
- å®Ÿè¡Œã‚’é‡ã­ã‚‹ã¨GPUãƒ¡ãƒ¢ãƒªãŒè§£æ”¾ã•ã‚Œãªã„
- 2å›ç›®ä»¥é™ã®å®Ÿè¡Œã§OOMã‚¨ãƒ©ãƒ¼

#### è§£æ±ºç­–

```python
import torch
import gc

def reset_gpu_state():
    """GPUã®çŠ¶æ…‹ã‚’å®Œå…¨ã«ãƒªã‚»ãƒƒãƒˆ"""
    
    if not torch.cuda.is_available():
        print("No GPU available")
        return
    
    print("ğŸ”„ Resetting GPU state...")
    
    # ã™ã¹ã¦ã®GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    for i in range(torch.cuda.device_count()):
        with torch.cuda.device(i):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    
    # Pythonã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    gc.collect()
    
    print("âœ… GPU state reset complete")
    
    # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’è¡¨ç¤º
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1e9
        reserved = torch.cuda.memory_reserved(i) / 1e9
        print(f"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")

# å®Ÿé¨“ã®é–“ã«å®Ÿè¡Œ
reset_gpu_state()

auto_model = create_auto_model(...)

reset_gpu_state()  # å®Ÿé¨“å¾Œã‚‚ãƒªã‚»ãƒƒãƒˆ
```

**ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢**:
```python
import subprocess

def run_optimization_in_subprocess(script_path):
    """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§æœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªå®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ï¼‰"""
    
    result = subprocess.run(
        ['python', script_path],
        capture_output=True,
        text=True
    )
    
    print("STDOUT:", result.stdout)
    print("STDERR:", result.stderr)
    print("Return code:", result.returncode)
    
    return result.returncode == 0

# ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# optimization_script.py ã‚’ä½œæˆã—ã¦ä½¿ç”¨
success = run_optimization_in_subprocess('optimization_script.py')
```

---

## ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ¥ã‚¬ã‚¤ãƒ‰

### "ValueError: Unknown model name"

**åŸå› **: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«å

**è§£æ±ºæ³•**:
```python
from model_characteristics import MODEL_CATALOG

# ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
supported_models = list(MODEL_CATALOG.keys())
print("Supported models:", supported_models)

# æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã‚’ä½¿ç”¨
auto_model = create_auto_model(
    model_name="NHITS",  # æ­£ã—ã„åå‰
    # model_name="nhits",  # âŒ å°æ–‡å­—ã¯ä¸å¯
    # model_name="N-HITS",  # âŒ ãƒã‚¤ãƒ•ãƒ³ã¯ä¸å¯
    h=7,
    dataset=df
)
```

---

### "RuntimeError: Expected tensor for argument"

**åŸå› **: ãƒ‡ãƒ¼ã‚¿å‹ã®ä¸ä¸€è‡´

**è§£æ±ºæ³•**:
```python
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª
print(df.dtypes)

# æ­£ã—ã„å‹ã«å¤‰æ›
df['unique_id'] = df['unique_id'].astype(str)
df['ds'] = pd.to_datetime(df['ds'])
df['y'] = pd.to_numeric(df['y'], errors='coerce')

# NaNã‚’å‡¦ç†
df = df.dropna()

# å†è©¦è¡Œ
auto_model = create_auto_model(...)
```

---

### "KeyError: 'unique_id'"

**åŸå› **: å¿…é ˆã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„

**è§£æ±ºæ³•**:
```python
# ã‚«ãƒ©ãƒ åã‚’ç¢ºèª
print("Columns:", df.columns.tolist())

# ã‚«ãƒ©ãƒ åã‚’ãƒªãƒãƒ¼ãƒ 
df = df.rename(columns={
    'series_id': 'unique_id',  # ä¾‹
    'date': 'ds',
    'value': 'y'
})

# å†è©¦è¡Œ
auto_model = create_auto_model(...)
```

---

## ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•

### ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒãƒƒã‚°

```python
import pandas as pd
from auto_model_factory import create_auto_model
import logging

# 1. ãƒ­ã‚®ãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# 2. ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
print("Step 1: Loading data")
df = pd.read_csv('data.csv')
print(f"  Loaded: {len(df)} rows")
print(f"  Columns: {df.columns.tolist()}")
print(f"  Data types:\n{df.dtypes}")

# 3. ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³
print("\nStep 2: Cleaning data")
df['ds'] = pd.to_datetime(df['ds'])
df = df.dropna()
print(f"  After cleaning: {len(df)} rows")

# 4. å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ
print("\nStep 3: Small-scale test")
df_test = df.head(1000)  # æœ€åˆã®1000è¡Œã®ã¿
try:
    auto_model = create_auto_model(
        model_name="MLP",  # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«
        h=7,
        dataset=df_test,
        num_samples=1,  # 1è©¦è¡Œã®ã¿
        verbose=True,
        gpus=0  # CPUã®ã¿
    )
    print("  âœ… Small test passed")
except Exception as e:
    print(f"  âŒ Small test failed: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# 5. å¾ã€…ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
print("\nStep 4: Scaling up")
try:
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,  # å…¨ãƒ‡ãƒ¼ã‚¿
        num_samples=10,  # ã‚ˆã‚Šå¤šãã®è©¦è¡Œ
        verbose=True,
        gpus=1  # GPUä½¿ç”¨
    )
    print("  âœ… Full run passed")
except Exception as e:
    print(f"  âŒ Full run failed: {e}")
    traceback.print_exc()
```

---

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
import cProfile
import pstats
import io

def profile_optimization(df):
    """æœ€é©åŒ–ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    
    pr = cProfile.Profile()
    pr.enable()
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,
        num_samples=5,
        verbose=False
    )
    
    pr.disable()
    
    # çµæœã‚’è¡¨ç¤º
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(20)  # ä¸Šä½20ä»¶
    
    print(s.getvalue())

# ä½¿ç”¨
profile_optimization(df)
```

---

## FAQ

### Q1: æœ€åˆã«ä½•ã‚’ç¢ºèªã™ã¹ãã§ã™ã‹ï¼Ÿ

**A**: ã¾ãš`diagnostic_check.py`ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

---

### Q2: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç†è§£ã§ãã¾ã›ã‚“

**A**: 
1. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“ã‚’ã‚³ãƒ”ãƒ¼
2. ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã€Œã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ¥ã‚¬ã‚¤ãƒ‰ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢
3. è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®æœ€å¾Œã®è¡Œã‚’ç¢ºèª
4. ãã‚Œã§ã‚‚è§£æ±ºã—ãªã„å ´åˆã¯ã€GitHubã§Issueã‚’ä½œæˆ

---

### Q3: æœ€é©åŒ–ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¾ã™

**A**: 
1. `num_samples`ã‚’æ¸›ã‚‰ã™ï¼ˆ10-20ã«ï¼‰
2. `use_pruning=True`ã‚’è¨­å®š
3. `time_budget_hours`ã§æ™‚é–“åˆ¶é™ã‚’è¨­å®š
4. ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ï¼ˆMLPã€DLinearï¼‰ã‚’è©¦ã™

---

### Q4: ç²¾åº¦ãŒæœŸå¾…ã‚ˆã‚Šä½ã„

**A**:
1. ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
2. `num_samples`ã‚’å¢—ã‚„ã™ï¼ˆ50-100ã«ï¼‰
3. ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
4. `input_size`ã‚’èª¿æ•´
5. ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ã‚’ä½¿ç”¨

---

### Q5: GPU ãŒèªè­˜ã•ã‚Œã¾ã›ã‚“

**A**:
1. `nvidia-smi`ã§ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’ç¢ºèª
2. PyTorchã®CUDAç‰ˆã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
3. `torch.cuda.is_available()`ã§ç¢ºèª
4. æœ€å¾Œã®æ‰‹æ®µ: `gpus=0`ã§CPU-onlyãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨

---

## ã‚µãƒãƒ¼ãƒˆã‚’å—ã‘ã‚‹å‰ã«

å•é¡Œã‚’å ±å‘Šã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š

```python
# bug_report.py - ãƒã‚°ãƒ¬ãƒãƒ¼ãƒˆæƒ…å ±ã‚’åé›†
import sys
import torch
import pandas as pd

def generate_bug_report():
    """ãƒã‚°ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®æƒ…å ±ã‚’åé›†"""
    
    report = []
    report.append("=" * 80)
    report.append("BUG REPORT")
    report.append("=" * 80)
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    report.append("\n## System Information")
    report.append(f"Python version: {sys.version}")
    report.append(f"PyTorch version: {torch.__version__}")
    report.append(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        report.append(f"CUDA version: {torch.version.cuda}")
        report.append(f"GPU count: {torch.cuda.device_count()}")
    
    # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    report.append("\n## Package Versions")
    packages = ['neuralforecast', 'optuna', 'ray', 'mlflow', 'pandas', 'numpy']
    for pkg in packages:
        try:
            mod = __import__(pkg)
            version = getattr(mod, '__version__', 'unknown')
            report.append(f"{pkg}: {version}")
        except ImportError:
            report.append(f"{pkg}: NOT INSTALLED")
    
    # å•é¡Œã®èª¬æ˜ï¼ˆæ‰‹å‹•ã§ç·¨é›†ï¼‰
    report.append("\n## Problem Description")
    report.append("Please describe your problem here:")
    report.append("")
    
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆæ‰‹å‹•ã§ç·¨é›†ï¼‰
    report.append("\n## Error Message")
    report.append("Paste full error message and stack trace here:")
    report.append("")
    
    # å†ç¾ã‚³ãƒ¼ãƒ‰ï¼ˆæ‰‹å‹•ã§ç·¨é›†ï¼‰
    report.append("\n## Minimal Reproducible Code")
    report.append("```python")
    report.append("# Paste your code here")
    report.append("```")
    
    report_text = "\n".join(report)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open('bug_report.txt', 'w') as f:
        f.write(report_text)
    
    print("ğŸ“ Bug report template saved to 'bug_report.txt'")
    print("Please fill in the problem description, error message, and code")
    print("Then submit to GitHub Issues")
    
    return report_text

generate_bug_report()
```

---

## ğŸ†˜ æœ€çµ‚æ‰‹æ®µ

ã™ã¹ã¦ã®è§£æ±ºç­–ã‚’è©¦ã—ã¦ã‚‚å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆï¼š

1. **GitHubã§Issueã‚’ä½œæˆ**: ãƒã‚°ãƒ¬ãƒãƒ¼ãƒˆã‚’å«ã‚ã¦
2. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§è³ªå•**: Discussions ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§
3. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†ç¢ºèª**: [README](./README.md)ã€[TUTORIAL](./TUTORIAL.md)ã€[API_REFERENCE](./API_REFERENCE.md)

**å•é¡Œè§£æ±ºã®æ—…ã€å¿œæ´ã—ã¦ã„ã¾ã™ï¼ğŸš€**
