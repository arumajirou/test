# è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ - APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

## ğŸ“• ç›®æ¬¡

- [æ¦‚è¦](#æ¦‚è¦)
- [auto_model_factory.py](#auto_model_factorypy)
  - [create_auto_model()](#create_auto_model)
  - [AutoModelFactory](#automodelfactory)
  - [OptimizationConfig](#optimizationconfig)
  - [ModelCharacteristics](#modelcharacteristics)
- [validation.py](#validationpy)
  - [ConfigValidator](#configvalidator)
  - [DataValidator](#datavalidator)
  - [ValidationResult](#validationresult)
- [search_algorithm_selector.py](#search_algorithm_selectorpy)
  - [SearchAlgorithmSelector](#searchalgorithmselector)
  - [SearchStrategy](#searchstrategy)
  - [recommend_num_samples()](#recommend_num_samples)
- [mlflow_integration.py](#mlflow_integrationpy)
- [logging_config.py](#logging_configpy)
- [ãƒ‡ãƒ¼ã‚¿å‹å®šç¾©](#ãƒ‡ãƒ¼ã‚¿å‹å®šç¾©)

---

## æ¦‚è¦

ã“ã® APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§ã¯ã€è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ã™ã¹ã¦ã®å…¬é–‹API ã‚’è©³ç´°ã«èª¬æ˜ã—ã¾ã™ã€‚

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0  
**æœ€çµ‚æ›´æ–°**: 2025å¹´11æœˆ12æ—¥

---

## auto_model_factory.py

ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚è‡ªå‹•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®çµ±åˆç®¡ç†ã‚’æä¾›ã—ã¾ã™ã€‚

### create_auto_model()

**æ¦‚è¦**: æœ€ã‚‚ç°¡å˜ã«è‡ªå‹•æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ä¾¿åˆ©é–¢æ•°

#### ã‚·ã‚°ãƒãƒãƒ£

```python
def create_auto_model(
    model_name: str,
    h: int,
    dataset: pd.DataFrame,
    backend: str = "optuna",
    config: Optional[Dict[str, Any]] = None,
    loss: Optional[Any] = None,
    num_samples: Optional[int] = None,
    cpus: int = 4,
    gpus: int = 0,
    use_mlflow: bool = False,
    mlflow_tracking_uri: Optional[str] = None,
    mlflow_experiment_name: Optional[str] = None,
    use_pruning: bool = False,
    time_budget_hours: Optional[float] = None,
    random_seed: Optional[int] = None,
    verbose: bool = True
) -> Any:
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----|-----------|------|
| `model_name` | `str` | **å¿…é ˆ** | NeuralForecastãƒ¢ãƒ‡ãƒ«å (ä¾‹: "NHITS", "TFT", "DLinear") |
| `h` | `int` | **å¿…é ˆ** | äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼ˆä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰ |
| `dataset` | `pd.DataFrame` | **å¿…é ˆ** | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆã‚«ãƒ©ãƒ : unique_id, ds, yï¼‰ |
| `backend` | `str` | `"optuna"` | æœ€é©åŒ–ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ ("optuna" ã¾ãŸã¯ "ray") |
| `config` | `Dict[str, Any]` | `None` | ã‚«ã‚¹ã‚¿ãƒ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“ã€‚Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ |
| `loss` | `Any` | `None` | ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ã€‚Noneã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ |
| `num_samples` | `int` | `None` | è©¦è¡Œå›æ•°ã€‚Noneã®å ´åˆã¯è‡ªå‹•æ¨å¥¨å€¤ã‚’ä½¿ç”¨ |
| `cpus` | `int` | `4` | ä½¿ç”¨ã™ã‚‹CPUæ•° |
| `gpus` | `int` | `0` | ä½¿ç”¨ã™ã‚‹GPUæ•° |
| `use_mlflow` | `bool` | `False` | MLflowã§ã®å®Ÿé¨“è¿½è·¡ã‚’æœ‰åŠ¹åŒ– |
| `mlflow_tracking_uri` | `str` | `None` | MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚µãƒ¼ãƒãƒ¼ã®URI |
| `mlflow_experiment_name` | `str` | `None` | MLflowå®Ÿé¨“åã€‚Noneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆ |
| `use_pruning` | `bool` | `False` | æ—©æœŸåœæ­¢ï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã‚’æœ‰åŠ¹åŒ– |
| `time_budget_hours` | `float` | `None` | æœ€é©åŒ–ã®æ™‚é–“åˆ¶é™ï¼ˆæ™‚é–“å˜ä½ï¼‰ |
| `random_seed` | `int` | `None` | å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰ |
| `verbose` | `bool` | `True` | è©³ç´°ãªé€²æ—æƒ…å ±ã‚’è¡¨ç¤º |

#### è¿”ã‚Šå€¤

| å‹ | èª¬æ˜ |
|----|------|
| `NeuralForecast.Auto*` | æœ€é©åŒ–ã•ã‚ŒãŸNeuralForecast Autoãƒ¢ãƒ‡ãƒ« |

#### ä¾‹å¤–

| ä¾‹å¤– | æ¡ä»¶ |
|------|------|
| `ValueError` | ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«åã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰åã€ã¾ãŸã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| `ValidationError` | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¾ãŸã¯è¨­å®šã®æ¤œè¨¼å¤±æ•— |
| `RuntimeError` | æœ€é©åŒ–ä¸­ã®ã‚¨ãƒ©ãƒ¼ |

#### ä½¿ç”¨ä¾‹

**åŸºæœ¬çš„ãªä½¿ç”¨**:
```python
from auto_model_factory import create_auto_model
import pandas as pd

df = pd.read_csv('data.csv')

auto_model = create_auto_model(
    model_name="NHITS",
    h=24,
    dataset=df,
    num_samples=50
)

predictions = auto_model.predict(dataset=df)
```

**ã‚«ã‚¹ã‚¿ãƒ è¨­å®š**:
```python
from ray import tune

custom_config = {
    'max_steps': tune.choice([1000, 2000]),
    'learning_rate': tune.loguniform(1e-4, 1e-2),
    'batch_size': tune.choice([64, 128])
}

auto_model = create_auto_model(
    model_name="TFT",
    h=24,
    dataset=df,
    config=custom_config,
    backend="optuna",
    num_samples=100,
    gpus=2
)
```

**MLflowçµ±åˆ**:
```python
auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    use_mlflow=True,
    mlflow_tracking_uri="http://localhost:5000",
    mlflow_experiment_name="production_forecast",
    verbose=True
)
```

---

### AutoModelFactory

**æ¦‚è¦**: è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ä½œæˆã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚¯ãƒ©ã‚¹ã€‚ã‚ˆã‚Šç´°ã‹ã„åˆ¶å¾¡ãŒå¿…è¦ãªå ´åˆã«ä½¿ç”¨ã€‚

#### ã‚·ã‚°ãƒãƒãƒ£

```python
class AutoModelFactory:
    def __init__(
        self,
        model_name: str,
        h: int,
        optimization_config: OptimizationConfig = None
    )
```

#### ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----|-----------|------|
| `model_name` | `str` | **å¿…é ˆ** | NeuralForecastãƒ¢ãƒ‡ãƒ«å |
| `h` | `int` | **å¿…é ˆ** | äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ |
| `optimization_config` | `OptimizationConfig` | `None` | æœ€é©åŒ–è¨­å®šã€‚Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š |

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### create_auto_model()

è‡ªå‹•æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
def create_auto_model(
    self,
    dataset: pd.DataFrame,
    config: Optional[Dict[str, Any]] = None,
    loss: Optional[Any] = None
) -> Any:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `dataset`: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
- `config`: ã‚«ã‚¹ã‚¿ãƒ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“
- `loss`: ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°

**è¿”ã‚Šå€¤**: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«

##### get_optimization_summary()

æœ€é©åŒ–ã®æ¦‚è¦æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

```python
def get_optimization_summary(self) -> Dict[str, Any]:
```

**è¿”ã‚Šå€¤**: æœ€é©åŒ–ã®çµ±è¨ˆæƒ…å ±ã‚’å«ã‚€è¾æ›¸
- `model_name`: ãƒ¢ãƒ‡ãƒ«å
- `forecast_horizon`: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
- `backend`: ä½¿ç”¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- `num_samples`: è©¦è¡Œå›æ•°
- `selected_algorithm`: é¸æŠã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- `dataset_characteristics`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§

##### _create_default_config()

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```python
def _create_default_config(self) -> Dict[str, Any]:
```

**è¿”ã‚Šå€¤**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®è¾æ›¸

#### ä½¿ç”¨ä¾‹

**åŸºæœ¬çš„ãªä½¿ç”¨**:
```python
from auto_model_factory import AutoModelFactory, OptimizationConfig

# è¨­å®š
opt_config = OptimizationConfig(
    backend="optuna",
    num_samples=50,
    cpus=8,
    gpus=1,
    use_mlflow=True,
    verbose=True
)

# ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ä½œæˆ
factory = AutoModelFactory(
    model_name="TFT",
    h=24,
    optimization_config=opt_config
)

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
auto_model = factory.create_auto_model(dataset=df)

# æ¦‚è¦å–å¾—
summary = factory.get_optimization_summary()
print(summary)
```

**é«˜åº¦ãªä½¿ç”¨**:
```python
from ray import tune
from neuralforecast.losses.pytorch import MQLoss

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒ æå¤±
custom_config = {
    'max_steps': tune.choice([1000, 2000, 3000]),
    'learning_rate': tune.loguniform(1e-4, 1e-2)
}

custom_loss = MQLoss(level=[90])

factory = AutoModelFactory(
    model_name="NHITS",
    h=24,
    optimization_config=opt_config
)

auto_model = factory.create_auto_model(
    dataset=df,
    config=custom_config,
    loss=custom_loss
)
```

---

### OptimizationConfig

**æ¦‚è¦**: æœ€é©åŒ–è¨­å®šã‚’ç®¡ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹

#### ã‚·ã‚°ãƒãƒãƒ£

```python
@dataclass
class OptimizationConfig:
    backend: str = "optuna"
    num_samples: Optional[int] = None
    cpus: int = 4
    gpus: int = 0
    use_mlflow: bool = False
    mlflow_tracking_uri: Optional[str] = None
    mlflow_experiment_name: Optional[str] = None
    use_pruning: bool = False
    time_budget_hours: Optional[float] = None
    random_seed: Optional[int] = None
    verbose: bool = True
```

#### ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----|-----------|------|
| `backend` | `str` | `"optuna"` | æœ€é©åŒ–ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ ("optuna" ã¾ãŸã¯ "ray") |
| `num_samples` | `int` | `None` | è©¦è¡Œå›æ•°ã€‚Noneã®å ´åˆã¯è‡ªå‹•æ¨å¥¨ |
| `cpus` | `int` | `4` | ä½¿ç”¨ã™ã‚‹CPUæ•° |
| `gpus` | `int` | `0` | ä½¿ç”¨ã™ã‚‹GPUæ•° |
| `use_mlflow` | `bool` | `False` | MLflowçµ±åˆã‚’æœ‰åŠ¹åŒ– |
| `mlflow_tracking_uri` | `str` | `None` | MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI |
| `mlflow_experiment_name` | `str` | `None` | MLflowå®Ÿé¨“å |
| `use_pruning` | `bool` | `False` | æ—©æœŸåœæ­¢ã‚’æœ‰åŠ¹åŒ– |
| `time_budget_hours` | `float` | `None` | æ™‚é–“åˆ¶é™ï¼ˆæ™‚é–“å˜ä½ï¼‰ |
| `random_seed` | `int` | `None` | ä¹±æ•°ã‚·ãƒ¼ãƒ‰ |
| `verbose` | `bool` | `True` | è©³ç´°å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ– |

#### ä½¿ç”¨ä¾‹

```python
from auto_model_factory import OptimizationConfig

# åŸºæœ¬è¨­å®š
config = OptimizationConfig(
    backend="optuna",
    num_samples=50,
    cpus=8,
    gpus=2
)

# MLflowæœ‰åŠ¹åŒ–
config = OptimizationConfig(
    backend="optuna",
    num_samples=30,
    use_mlflow=True,
    mlflow_tracking_uri="http://localhost:5000",
    mlflow_experiment_name="my_experiment"
)

# æ™‚é–“åˆ¶é™ã¨ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
config = OptimizationConfig(
    backend="optuna",
    num_samples=None,  # è‡ªå‹•æ¨å¥¨
    use_pruning=True,
    time_budget_hours=2.0,
    random_seed=42
)
```

---

### ModelCharacteristics

**æ¦‚è¦**: ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã‚’å®šç¾©ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹

#### ã‚·ã‚°ãƒãƒãƒ£

```python
@dataclass
class ModelCharacteristics:
    name: str
    complexity: ModelComplexity
    recommended_input_size_range: Tuple[int, int]
    supports_exogenous: bool
    supports_static: bool
    typical_training_time_minutes: float
    memory_footprint_mb: float
```

#### ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å‹ | èª¬æ˜ |
|-----------|-----|------|
| `name` | `str` | ãƒ¢ãƒ‡ãƒ«å |
| `complexity` | `ModelComplexity` | ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘åº¦ (SIMPLE, MODERATE, COMPLEX) |
| `recommended_input_size_range` | `Tuple[int, int]` | æ¨å¥¨input_sizeã®ç¯„å›² |
| `supports_exogenous` | `bool` | å¤–ç”Ÿå¤‰æ•°ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‹ |
| `supports_static` | `bool` | é™çš„ç‰¹å¾´é‡ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‹ |
| `typical_training_time_minutes` | `float` | å…¸å‹çš„ãªå­¦ç¿’æ™‚é–“ï¼ˆåˆ†ï¼‰ |
| `memory_footprint_mb` | `float` | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰ |

#### MODEL_CATALOG

åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã‚’å«ã‚€è¾æ›¸ã€‚

```python
MODEL_CATALOG: Dict[str, ModelCharacteristics]
```

#### ä½¿ç”¨ä¾‹

```python
from model_characteristics import MODEL_CATALOG, ModelComplexity

# ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã®å–å¾—
nhits_char = MODEL_CATALOG["NHITS"]
print(f"Complexity: {nhits_char.complexity}")
print(f"Input size range: {nhits_char.recommended_input_size_range}")
print(f"Training time: {nhits_char.typical_training_time_minutes} min")

# è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
complex_models = [
    name for name, char in MODEL_CATALOG.items()
    if char.complexity == ModelComplexity.COMPLEX
]
print(f"Complex models: {complex_models}")

# å¤–ç”Ÿå¤‰æ•°ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«
exog_models = [
    name for name, char in MODEL_CATALOG.items()
    if char.supports_exogenous
]
print(f"Models supporting exogenous variables: {exog_models}")
```

---

## validation.py

è¨­å®šã€ç’°å¢ƒã€ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã‚’æä¾›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

### ConfigValidator

**æ¦‚è¦**: è¨­å®šã¨ç’°å¢ƒã®æ¤œè¨¼ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹

#### ã‚·ã‚°ãƒãƒãƒ£

```python
class ConfigValidator:
    def __init__(self, strict_mode: bool = False)
```

#### ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----|-----------|------|
| `strict_mode` | `bool` | `False` | å³æ ¼ãƒ¢ãƒ¼ãƒ‰ã€‚Trueã®å ´åˆã¯è­¦å‘Šã‚‚ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã† |

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### validate_backend_config()

ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®šã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

```python
def validate_backend_config(
    self,
    backend: str,
    config: Optional[Dict[str, Any]],
    num_samples: Optional[int],
    cpus: int,
    gpus: int
) -> ValidationResult:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `backend`: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å ("optuna" ã¾ãŸã¯ "ray")
- `config`: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
- `num_samples`: è©¦è¡Œå›æ•°
- `cpus`: CPUæ•°
- `gpus`: GPUæ•°

**è¿”ã‚Šå€¤**: `ValidationResult`

##### validate_model_config()

ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

```python
def validate_model_config(
    self,
    config: Dict[str, Any],
    model_class_name: str
) -> ValidationResult:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `config`: ãƒ¢ãƒ‡ãƒ«è¨­å®š
- `model_class_name`: ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹å

**è¿”ã‚Šå€¤**: `ValidationResult`

##### validate_environment()

å®Ÿè¡Œç’°å¢ƒã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

```python
def validate_environment(
    self,
    required_memory_gb: float = 4.0,
    required_disk_gb: float = 10.0
) -> ValidationResult:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `required_memory_gb`: å¿…è¦ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰
- `required_disk_gb`: å¿…è¦ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ï¼ˆGBï¼‰

**è¿”ã‚Šå€¤**: `ValidationResult`

##### validate_mlflow_config()

MLflowè¨­å®šã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

```python
def validate_mlflow_config(
    self,
    tracking_uri: Optional[str] = None,
    experiment_name: Optional[str] = None
) -> ValidationResult:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `tracking_uri`: MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI
- `experiment_name`: å®Ÿé¨“å

**è¿”ã‚Šå€¤**: `ValidationResult`

#### ä½¿ç”¨ä¾‹

```python
from validation import ConfigValidator

# ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ä½œæˆ
validator = ConfigValidator(strict_mode=False)

# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®šã®æ¤œè¨¼
result = validator.validate_backend_config(
    backend="optuna",
    config=my_config,
    num_samples=50,
    cpus=4,
    gpus=1
)

if not result.is_valid:
    print("Errors:", result.errors)
    print("Warnings:", result.warnings)
    if result.corrected_config:
        print("Suggested config:", result.corrected_config)

# ç’°å¢ƒã®æ¤œè¨¼
env_result = validator.validate_environment(
    required_memory_gb=8.0,
    required_disk_gb=20.0
)

# MLflowè¨­å®šã®æ¤œè¨¼
mlflow_result = validator.validate_mlflow_config(
    tracking_uri="http://localhost:5000",
    experiment_name="my_experiment"
)
```

---

### DataValidator

**æ¦‚è¦**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### validate_dataset()

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã¨å†…å®¹ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

```python
def validate_dataset(
    self,
    df: pd.DataFrame,
    required_columns: List[str] = ['unique_id', 'ds', 'y']
) -> ValidationResult:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `df`: æ¤œè¨¼ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- `required_columns`: å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆ

**è¿”ã‚Šå€¤**: `ValidationResult`

**æ¤œè¨¼é …ç›®**:
- å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨
- ãƒ‡ãƒ¼ã‚¿å‹ã®æ­£ç¢ºæ€§
- æ¬ æå€¤ã®æœ‰ç„¡
- é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æœ‰ç„¡
- æ™‚ç³»åˆ—ã®é€£ç¶šæ€§

##### validate_forecast_horizon()

äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

```python
def validate_forecast_horizon(
    self,
    df: pd.DataFrame,
    h: int
) -> ValidationResult:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `df`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- `h`: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³

**è¿”ã‚Šå€¤**: `ValidationResult`

#### ä½¿ç”¨ä¾‹

```python
from validation import DataValidator
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ä½œæˆ
validator = DataValidator()

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼
df = pd.read_csv('data.csv')
result = validator.validate_dataset(df)

if result.is_valid:
    print("âœ… Dataset is valid")
else:
    print("âŒ Dataset validation failed:")
    for error in result.errors:
        print(f"  - {error}")

# äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³æ¤œè¨¼
h = 24
horizon_result = validator.validate_forecast_horizon(df, h)

if not horizon_result.is_valid:
    print("âš ï¸ Forecast horizon may be too large")
    print(horizon_result.warnings)
```

---

### ValidationResult

**æ¦‚è¦**: æ¤œè¨¼çµæœã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹

#### ã‚·ã‚°ãƒãƒãƒ£

```python
@dataclass
class ValidationResult:
    is_valid: bool
    errors: List[str]
    warnings: List[str]
    corrected_config: Optional[Dict[str, Any]] = None
```

#### ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å‹ | èª¬æ˜ |
|-----------|-----|------|
| `is_valid` | `bool` | æ¤œè¨¼ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹ |
| `errors` | `List[str]` | ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ |
| `warnings` | `List[str]` | è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ |
| `corrected_config` | `Dict[str, Any]` | ä¿®æ­£ã•ã‚ŒãŸè¨­å®šï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰ |

---

### ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°

#### validate_all()

ã™ã¹ã¦ã®æ¤œè¨¼ã‚’ä¸€åº¦ã«å®Ÿè¡Œã—ã¾ã™ã€‚

```python
def validate_all(
    backend: str,
    config: Optional[Dict[str, Any]],
    num_samples: Optional[int],
    cpus: int,
    gpus: int,
    model_class_name: str,
    dataset: pd.DataFrame,
    h: int,
    strict_mode: bool = False
) -> Dict[str, Any]:
```

**è¿”ã‚Šå€¤**: å„ã‚«ãƒ†ã‚´ãƒªã®æ¤œè¨¼çµæœã‚’å«ã‚€è¾æ›¸

#### print_validation_results()

æ¤œè¨¼çµæœã‚’æ•´å½¢ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚

```python
def print_validation_results(result: ValidationResult) -> None:
```

---

## search_algorithm_selector.py

æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è‡ªå‹•é¸æŠã‚’æä¾›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

### SearchAlgorithmSelector

**æ¦‚è¦**: ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«åŸºã¥ã„ã¦æœ€é©ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸æŠ

#### ã‚·ã‚°ãƒãƒãƒ£

```python
class SearchAlgorithmSelector:
    def __init__(self, backend: str = "optuna")
```

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### select_algorithm()

æœ€é©ãªæ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸æŠã—ã¾ã™ã€‚

```python
def select_algorithm(
    self,
    model_complexity: ModelComplexity,
    dataset_size: DatasetSize,
    num_samples: int,
    config: Optional[Dict[str, Any]] = None,
    use_pruning: bool = False,
    random_seed: Optional[int] = None
) -> SearchStrategy:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `model_complexity`: ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘åº¦
- `dataset_size`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º
- `num_samples`: è©¦è¡Œå›æ•°
- `config`: æ¢ç´¢ç©ºé–“è¨­å®š
- `use_pruning`: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æœ‰åŠ¹åŒ–
- `random_seed`: ä¹±æ•°ã‚·ãƒ¼ãƒ‰

**è¿”ã‚Šå€¤**: `SearchStrategy`

##### get_optuna_sampler()

Optunaã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’å–å¾—ã—ã¾ã™ã€‚

```python
def get_optuna_sampler(
    self,
    strategy: SearchStrategy
) -> optuna.samplers.BaseSampler:
```

##### get_optuna_pruner()

Optunaãƒ—ãƒ«ãƒ¼ãƒŠãƒ¼ã‚’å–å¾—ã—ã¾ã™ã€‚

```python
def get_optuna_pruner(
    self,
    strategy: SearchStrategy
) -> Optional[optuna.pruners.BasePruner]:
```

##### get_ray_search_algorithm()

Ray Tuneæ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚

```python
def get_ray_search_algorithm(
    self,
    strategy: SearchStrategy
) -> Any:
```

#### ä½¿ç”¨ä¾‹

```python
from search_algorithm_selector import (
    SearchAlgorithmSelector,
    ModelComplexity,
    DatasetSize
)

# ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ä½œæˆ
selector = SearchAlgorithmSelector(backend="optuna")

# ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ
strategy = selector.select_algorithm(
    model_complexity=ModelComplexity.COMPLEX,
    dataset_size=DatasetSize.LARGE,
    num_samples=100,
    use_pruning=True,
    random_seed=42
)

print(f"Selected: {strategy.algorithm_name}")
print(f"Description: {strategy.description}")
print(f"Reason: {strategy.reason}")

# ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¨ãƒ—ãƒ«ãƒ¼ãƒŠãƒ¼ã‚’å–å¾—
sampler = selector.get_optuna_sampler(strategy)
pruner = selector.get_optuna_pruner(strategy)
```

---

### SearchStrategy

**æ¦‚è¦**: é¸æŠã•ã‚ŒãŸæ¢ç´¢æˆ¦ç•¥ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹

#### ã‚·ã‚°ãƒãƒãƒ£

```python
@dataclass
class SearchStrategy:
    algorithm_name: str
    description: str
    reason: str
    hyperparameters: Dict[str, Any]
    use_multivariate: bool = False
    use_pruning: bool = False
    pruning_config: Optional[Dict[str, Any]] = None
```

#### ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å‹ | èª¬æ˜ |
|-----------|-----|------|
| `algorithm_name` | `str` | ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å |
| `description` | `str` | ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª¬æ˜ |
| `reason` | `str` | é¸æŠç†ç”± |
| `hyperparameters` | `Dict[str, Any]` | ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| `use_multivariate` | `bool` | å¤šå¤‰é‡TPEã‚’ä½¿ç”¨ã™ã‚‹ã‹ |
| `use_pruning` | `bool` | ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹ |
| `pruning_config` | `Dict[str, Any]` | ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š |

---

### recommend_num_samples()

**æ¦‚è¦**: æœ€é©ãªè©¦è¡Œå›æ•°ã‚’æ¨å¥¨ã™ã‚‹é–¢æ•°

#### ã‚·ã‚°ãƒãƒãƒ£

```python
def recommend_num_samples(
    model_complexity: ModelComplexity,
    dataset_size: DatasetSize,
    search_complexity: SearchComplexity = SearchComplexity.MEDIUM,
    time_budget_hours: Optional[float] = None
) -> Tuple[int, str]:
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----|-----------|------|
| `model_complexity` | `ModelComplexity` | **å¿…é ˆ** | ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘åº¦ |
| `dataset_size` | `DatasetSize` | **å¿…é ˆ** | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º |
| `search_complexity` | `SearchComplexity` | `MEDIUM` | æ¢ç´¢ç©ºé–“ã®è¤‡é›‘åº¦ |
| `time_budget_hours` | `float` | `None` | æ™‚é–“åˆ¶é™ï¼ˆæ™‚é–“å˜ä½ï¼‰ |

#### è¿”ã‚Šå€¤

| è¦ç´  | å‹ | èª¬æ˜ |
|------|-----|------|
| `num_samples` | `int` | æ¨å¥¨è©¦è¡Œå›æ•° |
| `explanation` | `str` | æ¨å¥¨ç†ç”±ã®èª¬æ˜ |

#### ä½¿ç”¨ä¾‹

```python
from search_algorithm_selector import (
    recommend_num_samples,
    ModelComplexity,
    DatasetSize,
    SearchComplexity
)

# åŸºæœ¬çš„ãªæ¨å¥¨
num_samples, explanation = recommend_num_samples(
    model_complexity=ModelComplexity.MODERATE,
    dataset_size=DatasetSize.MEDIUM
)
print(f"Recommended: {num_samples} trials")
print(f"Reason: {explanation}")

# æ™‚é–“åˆ¶é™ä»˜ã
num_samples, explanation = recommend_num_samples(
    model_complexity=ModelComplexity.COMPLEX,
    dataset_size=DatasetSize.LARGE,
    search_complexity=SearchComplexity.HIGH,
    time_budget_hours=2.0
)
print(f"Within 2 hours: {num_samples} trials")
```

---

## mlflow_integration.py

MLflowã¨ã®çµ±åˆã‚’æä¾›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

### setup_mlflow()

MLflowã‚’è¨­å®šã—ã¾ã™ã€‚

```python
def setup_mlflow(
    tracking_uri: Optional[str] = None,
    experiment_name: Optional[str] = None
) -> str:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `tracking_uri`: MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI
- `experiment_name`: å®Ÿé¨“å

**è¿”ã‚Šå€¤**: å®Ÿé¨“ID

### log_optimization_results()

æœ€é©åŒ–çµæœã‚’MLflowã«è¨˜éŒ²ã—ã¾ã™ã€‚

```python
def log_optimization_results(
    params: Dict[str, Any],
    metrics: Dict[str, float],
    artifacts: Optional[Dict[str, str]] = None
) -> None:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `params`: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¾æ›¸
- `metrics`: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸
- `artifacts`: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®è¾æ›¸ï¼ˆãƒ‘ã‚¹ï¼‰

---

## logging_config.py

ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’æä¾›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

### setup_logging()

ãƒ­ã‚®ãƒ³ã‚°ã‚’è¨­å®šã—ã¾ã™ã€‚

```python
def setup_logging(
    level: str = "INFO",
    log_file: Optional[str] = None,
    format_string: Optional[str] = None
) -> logging.Logger:
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `level`: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« ("DEBUG", "INFO", "WARNING", "ERROR")
- `log_file`: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
- `format_string`: ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ–‡å­—åˆ—

**è¿”ã‚Šå€¤**: è¨­å®šã•ã‚ŒãŸLogger

---

## ãƒ‡ãƒ¼ã‚¿å‹å®šç¾©

### Enumå‹

#### ModelComplexity

```python
class ModelComplexity(Enum):
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
```

#### DatasetSize

```python
class DatasetSize(Enum):
    SMALL = "small"      # < 10,000 rows
    MEDIUM = "medium"    # 10,000 - 100,000 rows
    LARGE = "large"      # > 100,000 rows
```

#### SearchComplexity

```python
class SearchComplexity(Enum):
    LOW = "low"          # < 10 parameters
    MEDIUM = "medium"    # 10 - 20 parameters
    HIGH = "high"        # > 20 parameters
```

---

## ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚·ãƒ³ãƒ—ãƒ«ãªäºˆæ¸¬

```python
from auto_model_factory import create_auto_model

auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    num_samples=30
)
predictions = auto_model.predict(dataset=df)
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³2: è©³ç´°ãªåˆ¶å¾¡

```python
from auto_model_factory import AutoModelFactory, OptimizationConfig
from ray import tune

config = OptimizationConfig(
    backend="optuna",
    num_samples=50,
    gpus=2,
    use_mlflow=True
)

factory = AutoModelFactory(
    model_name="TFT",
    h=24,
    optimization_config=config
)

custom_config = {
    'max_steps': tune.choice([1000, 2000]),
    'learning_rate': tune.loguniform(1e-4, 1e-2)
}

auto_model = factory.create_auto_model(
    dataset=df,
    config=custom_config
)
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ¤œè¨¼é‡è¦–

```python
from validation import validate_all, print_validation_results

results = validate_all(
    backend="optuna",
    config=my_config,
    num_samples=50,
    cpus=4,
    gpus=1,
    model_class_name="NHITS",
    dataset=df,
    h=24
)

if results['overall_valid']:
    auto_model = create_auto_model(...)
else:
    print_validation_results(results)
```

---

## ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§

### v1.0.0
- åˆå›ãƒªãƒªãƒ¼ã‚¹
- Optuna/Ray Tuneçµ±åˆ
- MLflowã‚µãƒãƒ¼ãƒˆ
- è‡ªå‹•æ¤œè¨¼æ©Ÿèƒ½

---

## æ³¨æ„äº‹é …

1. **GPUä½¿ç”¨æ™‚**: CUDAäº’æ›ã®PyTorchãŒå¿…è¦
2. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«æ³¨æ„
3. **ä¸¦åˆ—å®Ÿè¡Œ**: Rayä½¿ç”¨æ™‚ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãŒå¿…è¦
4. **å†ç¾æ€§**: `random_seed`ã‚’è¨­å®šã™ã‚‹ã“ã¨

---

## ã‚µãƒãƒ¼ãƒˆã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

APIä»•æ§˜ã«é–¢ã™ã‚‹è³ªå•ã‚„ææ¡ˆã¯ã€GitHubã®Issuesã§ãŠé¡˜ã„ã—ã¾ã™ã€‚

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0  
**æœ€çµ‚æ›´æ–°**: 2025å¹´11æœˆ12æ—¥
