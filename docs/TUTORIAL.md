# è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ - ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

## ğŸ“˜ ã¯ã˜ã‚ã«

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã‚’æ®µéšçš„ã«å­¦ã³ã¾ã™ã€‚å„ãƒ¬ãƒƒã‚¹ãƒ³ã¯ç‹¬ç«‹ã—ã¦ãŠã‚Šã€å¿…è¦ãªéƒ¨åˆ†ã‹ã‚‰å­¦ç¿’ã§ãã¾ã™ã€‚

**æ‰€è¦æ™‚é–“**: å…¨ä½“ã§ç´„2-3æ™‚é–“ï¼ˆå®Ÿè·µã‚’å«ã‚€ï¼‰

---

## ğŸ“‹ ç›®æ¬¡

- [ãƒ¬ãƒƒã‚¹ãƒ³0: ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#ãƒ¬ãƒƒã‚¹ãƒ³0-ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)
- [ãƒ¬ãƒƒã‚¹ãƒ³1: æœ€åˆã®äºˆæ¸¬ï¼ˆ10åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³1-æœ€åˆã®äºˆæ¸¬)
- [ãƒ¬ãƒƒã‚¹ãƒ³2: ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨æ¤œè¨¼ï¼ˆ15åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³2-ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨æ¤œè¨¼)
- [ãƒ¬ãƒƒã‚¹ãƒ³3: ãƒ¢ãƒ‡ãƒ«ã®é¸æŠï¼ˆ15åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³3-ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ)
- [ãƒ¬ãƒƒã‚¹ãƒ³4: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼ˆ20åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³4-ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º)
- [ãƒ¬ãƒƒã‚¹ãƒ³5: MLflowã§ã®å®Ÿé¨“ç®¡ç†ï¼ˆ20åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³5-mlflowã§ã®å®Ÿé¨“ç®¡ç†)
- [ãƒ¬ãƒƒã‚¹ãƒ³6: é«˜åº¦ãªæœ€é©åŒ–ï¼ˆ30åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³6-é«˜åº¦ãªæœ€é©åŒ–)
- [ãƒ¬ãƒƒã‚¹ãƒ³7: æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹ï¼ˆ20åˆ†ï¼‰](#ãƒ¬ãƒƒã‚¹ãƒ³7-æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹)

---

## ãƒ¬ãƒƒã‚¹ãƒ³0: ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ã‚¹ãƒ†ãƒƒãƒ—1: å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install neuralforecast optuna 'ray[tune]' mlflow pytorch-lightning pandas numpy

# GPUä½¿ç”¨æ™‚ï¼ˆæ¨å¥¨ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

```python
# test_installation.py
import torch
import neuralforecast
import optuna
import ray
import mlflow

print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("GPU count:", torch.cuda.device_count())
print("NeuralForecast version:", neuralforecast.__version__)
print("Optuna version:", optuna.__version__)
print("Ray version:", ray.__version__)
print("MLflow version:", mlflow.__version__)

print("\nâœ… All packages installed successfully!")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

```python
# prepare_sample_data.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def create_sample_data(n_series=3, n_periods=365):
    """ã‚µãƒ³ãƒ—ãƒ«ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    data = []
    
    for series_id in range(n_series):
        start_date = datetime(2023, 1, 1)
        dates = [start_date + timedelta(days=i) for i in range(n_periods)]
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ + å­£ç¯€æ€§ + ãƒã‚¤ã‚º
        trend = np.linspace(100, 200, n_periods)
        seasonality = 20 * np.sin(np.arange(n_periods) * 2 * np.pi / 7)
        noise = np.random.normal(0, 5, n_periods)
        values = trend + seasonality + noise + (series_id * 50)
        
        for date, value in zip(dates, values):
            data.append({
                'unique_id': f'series_{series_id}',
                'ds': date,
                'y': value
            })
    
    df = pd.DataFrame(data)
    df.to_csv('sample_data.csv', index=False)
    print(f"âœ… Sample data created: {len(df)} rows, {n_series} series")
    return df

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
df = create_sample_data()
print(df.head(10))
```

**å¿…é ˆã‚«ãƒ©ãƒ **:
- `unique_id`: æ™‚ç³»åˆ—ã‚’è­˜åˆ¥ã™ã‚‹ID
- `ds`: æ—¥ä»˜ï¼ˆdatetimeå‹ï¼‰
- `y`: äºˆæ¸¬å¯¾è±¡ã®å€¤ï¼ˆæ•°å€¤ï¼‰

---

## ãƒ¬ãƒƒã‚¹ãƒ³1: æœ€åˆã®äºˆæ¸¬

**ç›®æ¨™**: æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹

### ã‚¹ãƒ†ãƒƒãƒ—1: åŸºæœ¬çš„ãªäºˆæ¸¬

```python
# lesson1_basic_forecast.py
import pandas as pd
from auto_model_factory import create_auto_model

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

print("ğŸ“Š Dataset info:")
print(f"  Rows: {len(df)}")
print(f"  Series: {df['unique_id'].nunique()}")
print(f"  Date range: {df['ds'].min()} to {df['ds'].max()}")

# ğŸš€ è‡ªå‹•æœ€é©åŒ–å®Ÿè¡Œ
print("\nğŸ” Starting optimization...")
auto_model = create_auto_model(
    model_name="NHITS",       # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    h=7,                      # 7æ—¥å…ˆã‚’äºˆæ¸¬
    dataset=df,               # ãƒ‡ãƒ¼ã‚¿
    backend="optuna",         # Optunaã‚’ä½¿ç”¨
    num_samples=10,           # 10å›è©¦è¡Œï¼ˆæœ€åˆã¯å°‘ãªã‚ï¼‰
    verbose=True              # è©³ç´°å‡ºåŠ›
)

# äºˆæ¸¬å®Ÿè¡Œ
print("\nğŸ“ˆ Making predictions...")
predictions = auto_model.predict(dataset=df)

print("\nâœ… Forecast completed!")
print(predictions.head())

# çµæœã‚’ä¿å­˜
predictions.to_csv('predictions_lesson1.csv', index=False)
print("\nğŸ’¾ Results saved to predictions_lesson1.csv")
```

### ç†è§£ã‚’æ·±ã‚ã‚‹

**ã“ã®ã‚³ãƒ¼ãƒ‰ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹**:

1. **ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿**: CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
2. **è‡ªå‹•æœ€é©åŒ–**: OptunaãŒ10å›ã®è©¦è¡Œã§æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢
3. **äºˆæ¸¬**: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§7æ—¥å…ˆã‚’äºˆæ¸¬
4. **çµæœä¿å­˜**: äºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜

**ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `model_name`: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆNHITS, TFT, DLinearãªã©ï¼‰
- `h`: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼ˆä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
- `num_samples`: è©¦è¡Œå›æ•°ï¼ˆå¤šã„ã»ã©ç²¾åº¦å‘ä¸Šã€æ™‚é–“å¢—ï¼‰

### æ¼”ç¿’

1. `num_samples`ã‚’5, 10, 20ã«å¤‰æ›´ã—ã¦å®Ÿè¡Œæ™‚é–“ã¨ç²¾åº¦ã®é•ã„ã‚’ç¢ºèª
2. `h`ã‚’3, 7, 14ã«å¤‰æ›´ã—ã¦ç•°ãªã‚‹äºˆæ¸¬æœŸé–“ã‚’è©¦ã™
3. è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ

---

## ãƒ¬ãƒƒã‚¹ãƒ³2: ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨æ¤œè¨¼

**ç›®æ¨™**: ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ç¢ºä¿ã—ã€å•é¡Œã‚’æ—©æœŸç™ºè¦‹ã™ã‚‹

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼

```python
# lesson2_data_validation.py
import pandas as pd
from validation import DataValidator, print_validation_results

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
print("ğŸ” Validating dataset...")
validator = DataValidator()
result = validator.validate_dataset(df)

# æ¤œè¨¼çµæœã®è¡¨ç¤º
print_validation_results(result)

if result.is_valid:
    print("\nâœ… Dataset is valid!")
else:
    print("\nâŒ Dataset has issues:")
    for error in result.errors:
        print(f"  - {error}")

# äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ã®æ¤œè¨¼
h = 7
horizon_result = validator.validate_forecast_horizon(df, h)
print_validation_results(horizon_result)
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å“è³ªã®åˆ†æ

```python
# lesson2_data_analysis.py
import pandas as pd
import numpy as np

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

print("ğŸ“Š Data Quality Analysis\n")

# 1. åŸºæœ¬çµ±è¨ˆ
print("=" * 50)
print("1. Basic Statistics")
print("=" * 50)
print(f"Total rows: {len(df)}")
print(f"Unique series: {df['unique_id'].nunique()}")
print(f"Date range: {df['ds'].min()} to {df['ds'].max()}")
print(f"Value range: {df['y'].min():.2f} to {df['y'].max():.2f}")

# 2. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
print("\n" + "=" * 50)
print("2. Missing Values")
print("=" * 50)
missing = df.isnull().sum()
print(missing)

# 3. å„ç³»åˆ—ã®çµ±è¨ˆ
print("\n" + "=" * 50)
print("3. Per-Series Statistics")
print("=" * 50)
series_stats = df.groupby('unique_id').agg({
    'y': ['count', 'mean', 'std', 'min', 'max']
})
print(series_stats)

# 4. æ™‚ç³»åˆ—ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
print("\n" + "=" * 50)
print("4. Time Series Continuity")
print("=" * 50)
for series_id in df['unique_id'].unique():
    series_df = df[df['unique_id'] == series_id].sort_values('ds')
    date_diffs = series_df['ds'].diff().dt.days.dropna()
    
    print(f"\n{series_id}:")
    print(f"  Expected frequency: 1 day")
    print(f"  Actual min: {date_diffs.min()} days")
    print(f"  Actual max: {date_diffs.max()} days")
    
    if date_diffs.std() > 0:
        print(f"  âš ï¸  Warning: Irregular time intervals detected")

# 5. å¤–ã‚Œå€¤æ¤œå‡º
print("\n" + "=" * 50)
print("5. Outlier Detection")
print("=" * 50)
Q1 = df['y'].quantile(0.25)
Q3 = df['y'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['y'] < Q1 - 1.5*IQR) | (df['y'] > Q3 + 1.5*IQR)]
print(f"Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)")

print("\nâœ… Analysis complete!")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

```python
# lesson2_data_cleaning.py
import pandas as pd
import numpy as np

def clean_timeseries_data(df):
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    print("ğŸ§¹ Cleaning data...")
    
    df_clean = df.copy()
    
    # 1. æ¬ æå€¤ã®å‡¦ç†
    print("\n1. Handling missing values...")
    before = len(df_clean)
    df_clean = df_clean.dropna(subset=['unique_id', 'ds', 'y'])
    after = len(df_clean)
    print(f"  Removed {before - after} rows with missing values")
    
    # 2. é‡è¤‡ã®å‰Šé™¤
    print("\n2. Removing duplicates...")
    before = len(df_clean)
    df_clean = df_clean.drop_duplicates(subset=['unique_id', 'ds'])
    after = len(df_clean)
    print(f"  Removed {before - after} duplicate rows")
    
    # 3. æ—¥ä»˜ã®ä¸¦ã³æ›¿ãˆ
    print("\n3. Sorting by date...")
    df_clean = df_clean.sort_values(['unique_id', 'ds'])
    
    # 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
    df_clean = df_clean.reset_index(drop=True)
    
    print("\nâœ… Cleaning complete!")
    return df_clean

# å®Ÿè¡Œ
df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])
df_clean = clean_timeseries_data(df)
df_clean.to_csv('sample_data_cleaned.csv', index=False)
```

### æ¼”ç¿’

1. æ„å›³çš„ã«æ¬ æå€¤ã‚’å…¥ã‚Œã¦ã€æ¤œè¨¼ãŒãã‚Œã‚’æ¤œå‡ºã™ã‚‹ã“ã¨ã‚’ç¢ºèª
2. ç•°ãªã‚‹é »åº¦ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚é–“åˆ¥ã€é€±åˆ¥ï¼‰ã‚’æº–å‚™ã—ã¦æ¤œè¨¼
3. è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’åˆ†æ

---

## ãƒ¬ãƒƒã‚¹ãƒ³3: ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

**ç›®æ¨™**: é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã‚’ç†è§£ã™ã‚‹

```python
# lesson3_model_characteristics.py
from model_characteristics import MODEL_CATALOG, ModelComplexity

print("ğŸ“‹ Available Models\n")
print("=" * 80)

for model_name, char in MODEL_CATALOG.items():
    print(f"\nğŸ”¹ {model_name}")
    print(f"   Complexity: {char.complexity.value}")
    print(f"   Recommended input_size: {char.recommended_input_size_range}")
    print(f"   Training time: ~{char.typical_training_time_minutes} minutes")
    print(f"   Memory footprint: ~{char.memory_footprint_mb} MB")
    print(f"   Supports exogenous: {char.supports_exogenous}")
    print(f"   Supports static: {char.supports_static}")

print("\n" + "=" * 80)
print("\nğŸ’¡ Selection Guide:")
print("   Simple models (MLP): Fast, good for simple patterns")
print("   Moderate models (NHITS, DLinear): Balance of speed and accuracy")
print("   Complex models (TFT, Transformer): Best accuracy, slower training")
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ

```python
# lesson3_model_selection.py
import pandas as pd
from search_algorithm_selector import DatasetSize, recommend_num_samples
from model_characteristics import MODEL_CATALOG, ModelComplexity

def select_model_for_data(df, h, time_budget_hours=1.0):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ"""
    
    n_rows = len(df)
    n_series = df['unique_id'].nunique()
    
    print("ğŸ“Š Dataset Analysis")
    print(f"   Rows: {n_rows:,}")
    print(f"   Series: {n_series}")
    print(f"   Forecast horizon: {h}")
    print(f"   Time budget: {time_budget_hours} hours")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®åˆ¤å®š
    if n_rows < 10000:
        dataset_size = DatasetSize.SMALL
        print(f"   Dataset size: SMALL")
    elif n_rows < 100000:
        dataset_size = DatasetSize.MEDIUM
        print(f"   Dataset size: MEDIUM")
    else:
        dataset_size = DatasetSize.LARGE
        print(f"   Dataset size: LARGE")
    
    # ãƒ¢ãƒ‡ãƒ«ã®æ¨å¥¨
    print("\nğŸ¯ Recommended Models:")
    
    if dataset_size == DatasetSize.SMALL:
        recommendations = [
            ("NHITS", "Good balance for small data"),
            ("NBEATS", "Interpretable, works well"),
            ("MLP", "Fast baseline")
        ]
    elif dataset_size == DatasetSize.MEDIUM:
        recommendations = [
            ("TFT", "Best for complex patterns"),
            ("TSMixer", "Modern architecture"),
            ("NHITS", "Reliable choice")
        ]
    else:  # LARGE
        recommendations = [
            ("DLinear", "Efficient for large data"),
            ("PatchTST", "State-of-the-art"),
            ("TSMixer", "Scalable")
        ]
    
    for model, reason in recommendations:
        char = MODEL_CATALOG[model]
        print(f"\n   {model}")
        print(f"      Reason: {reason}")
        print(f"      Training time: ~{char.typical_training_time_minutes}min")
        
        # æ¨å¥¨è©¦è¡Œå›æ•°ã‚’è¨ˆç®—
        num_samples, _ = recommend_num_samples(
            model_complexity=char.complexity,
            dataset_size=dataset_size,
            time_budget_hours=time_budget_hours
        )
        print(f"      Recommended trials: {num_samples}")
    
    return recommendations[0][0]  # æœ€ã‚‚æ¨å¥¨ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™

# å®Ÿè¡Œ
df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

recommended_model = select_model_for_data(df, h=7, time_budget_hours=1.0)
print(f"\nâœ… Best choice: {recommended_model}")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ

```python
# lesson3_model_comparison.py
import pandas as pd
from auto_model_factory import create_auto_model
import time

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

models_to_test = ["MLP", "NHITS", "DLinear"]
results = {}

print("ğŸ”¬ Comparing Models\n")

for model_name in models_to_test:
    print(f"Testing {model_name}...")
    start_time = time.time()
    
    try:
        auto_model = create_auto_model(
            model_name=model_name,
            h=7,
            dataset=df,
            backend="optuna",
            num_samples=10,  # å°‘ãªã‚ã§æ¯”è¼ƒ
            verbose=False
        )
        
        predictions = auto_model.predict(dataset=df)
        elapsed_time = time.time() - start_time
        
        results[model_name] = {
            'status': 'Success',
            'time_seconds': elapsed_time,
            'predictions': predictions
        }
        
        print(f"  âœ… Complete in {elapsed_time:.1f}s\n")
        
    except Exception as e:
        results[model_name] = {
            'status': 'Failed',
            'error': str(e)
        }
        print(f"  âŒ Failed: {e}\n")

# çµæœã®ã‚µãƒãƒªãƒ¼
print("\nğŸ“Š Comparison Summary")
print("=" * 60)
for model, result in results.items():
    if result['status'] == 'Success':
        print(f"{model:15s} âœ… {result['time_seconds']:6.1f}s")
    else:
        print(f"{model:15s} âŒ Failed")
```

### æ¼”ç¿’

1. ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ãŒã©ã†å¤‰ã‚ã‚‹ã‹ç¢ºèª
2. 3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«æ¯”è¼ƒã—ã¦ã€é€Ÿåº¦ã¨ç²¾åº¦ã‚’è©•ä¾¡
3. è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«(TFT)ã¨å˜ç´”ãªãƒ¢ãƒ‡ãƒ«(MLP)ã®é•ã„ã‚’ä½“æ„Ÿ

---

## ãƒ¬ãƒƒã‚¹ãƒ³4: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

**ç›®æ¨™**: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§æœ€é©åŒ–ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ç†è§£

```python
# lesson4_default_config.py
from ray import tune
from auto_model_factory import AutoModelFactory

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ç¢ºèª
factory = AutoModelFactory(model_name="NHITS", h=7)
default_config = factory._create_default_config()

print("ğŸ”§ Default Hyperparameter Configuration\n")
for param, value in default_config.items():
    print(f"{param:25s}: {value}")
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®ä½œæˆ

```python
# lesson4_custom_config.py
import pandas as pd
from auto_model_factory import create_auto_model
from ray import tune

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ã‚«ã‚¹ã‚¿ãƒ æ¢ç´¢ç©ºé–“ã®å®šç¾©
custom_config = {
    # å­¦ç¿’é–¢é€£
    'max_steps': tune.choice([500, 1000, 2000]),
    'learning_rate': tune.loguniform(1e-4, 1e-2),
    'batch_size': tune.choice([32, 64, 128]),
    
    # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é–¢é€£
    'input_size': tune.choice([7, 14, 28]),
    'hidden_size': tune.choice([256, 512]),
    
    # æ­£å‰‡åŒ–
    'dropout_prob_theta': tune.uniform(0.0, 0.5),
    
    # æ—©æœŸåœæ­¢
    'early_stop_patience_steps': 3
}

print("ğŸ¯ Custom Configuration:")
for param, value in custom_config.items():
    print(f"  {param}: {value}")

print("\nğŸ” Starting optimization with custom config...")

auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    config=custom_config,  # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ä½¿ç”¨
    backend="optuna",
    num_samples=20,
    verbose=True
)

predictions = auto_model.predict(dataset=df)
print("\nâœ… Optimization complete!")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: æ¢ç´¢ç©ºé–“ã®è¨­è¨ˆ

```python
# lesson4_search_space_design.py
from ray import tune

def create_narrow_search_space():
    """ç‹­ã„æ¢ç´¢ç©ºé–“: ç´ æ—©ãåæŸ"""
    return {
        'max_steps': tune.choice([1000]),  # å›ºå®š
        'learning_rate': tune.loguniform(5e-4, 2e-3),  # ç‹­ã„ç¯„å›²
        'batch_size': tune.choice([64, 128]),  # å°‘ãªã„é¸æŠè‚¢
        'input_size': tune.choice([14])  # å›ºå®š
    }

def create_wide_search_space():
    """åºƒã„æ¢ç´¢ç©ºé–“: ã‚ˆã‚Šè‰¯ã„è§£ã‚’æ¢ç´¢"""
    return {
        'max_steps': tune.choice([500, 1000, 2000, 3000]),
        'learning_rate': tune.loguniform(1e-5, 1e-2),  # åºƒã„ç¯„å›²
        'batch_size': tune.choice([16, 32, 64, 128, 256]),
        'input_size': tune.choice([7, 14, 28, 56]),
        'hidden_size': tune.choice([128, 256, 512, 1024])
    }

def create_focused_search_space():
    """é›†ä¸­çš„ãªæ¢ç´¢ç©ºé–“: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ç„¦ç‚¹"""
    return {
        'max_steps': 1000,  # å›ºå®š
        'learning_rate': tune.loguniform(1e-4, 1e-2),  # æœ€é‡è¦
        'batch_size': tune.choice([64, 128]),
        'input_size': tune.choice([14, 28]),  # ã‚„ã‚„é‡è¦
        'hidden_size': 512  # å›ºå®š
    }

# ä½¿ç”¨ä¾‹
print("ğŸ“ Search Space Examples\n")

print("1. Narrow (Fast):")
narrow = create_narrow_search_space()
combinations_narrow = 1 * 10 * 2 * 1  # å¤§ã¾ã‹ãªçµ„ã¿åˆã‚ã›æ•°
print(f"   Approximate combinations: {combinations_narrow}")

print("\n2. Wide (Thorough):")
wide = create_wide_search_space()
combinations_wide = 4 * 20 * 5 * 4 * 4
print(f"   Approximate combinations: {combinations_wide}")

print("\n3. Focused (Balanced):")
focused = create_focused_search_space()
combinations_focused = 1 * 15 * 2 * 2 * 1
print(f"   Approximate combinations: {combinations_focused}")

print("\nğŸ’¡ Recommendation:")
print("   - Time limited? Use Narrow")
print("   - Best accuracy? Use Wide")
print("   - Balanced? Use Focused")
```

### ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’ç†è§£ã™ã‚‹

```python
# lesson4_parameter_impact.py
import pandas as pd
from auto_model_factory import create_auto_model
from ray import tune
import time

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ç•°ãªã‚‹ learning_rate ã§æ¯”è¼ƒ
learning_rates = [1e-4, 1e-3, 1e-2]
results = {}

print("ğŸ”¬ Testing different learning rates\n")

for lr in learning_rates:
    print(f"Testing lr={lr}...")
    
    config = {
        'learning_rate': lr,  # å›ºå®šå€¤ã§ãƒ†ã‚¹ãƒˆ
        'max_steps': 500,
        'batch_size': 64
    }
    
    start_time = time.time()
    
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,
        config=config,
        backend="optuna",
        num_samples=3,  # å°‘æ•°ã®è©¦è¡Œ
        verbose=False
    )
    
    elapsed = time.time() - start_time
    results[lr] = {'time': elapsed}
    
    print(f"  Complete in {elapsed:.1f}s")

print("\nğŸ“Š Results:")
for lr, result in results.items():
    print(f"  lr={lr}: {result['time']:.1f}s")
```

### æ¼”ç¿’

1. æ¢ç´¢ç©ºé–“ã‚’æ®µéšçš„ã«åºƒã’ã¦ã€åæŸæ™‚é–“ã®é•ã„ã‚’è¦³å¯Ÿ
2. æœ€ã‚‚å½±éŸ¿ã®å¤§ãã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰¹å®š
3. è‡ªåˆ†ã®ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªæ¢ç´¢ç©ºé–“ã‚’è¨­è¨ˆ

---

## ãƒ¬ãƒƒã‚¹ãƒ³5: MLflowã§ã®å®Ÿé¨“ç®¡ç†

**ç›®æ¨™**: MLflowã‚’ä½¿ã£ã¦å®Ÿé¨“ã‚’è¿½è·¡ãƒ»ç®¡ç†ã™ã‚‹

### ã‚¹ãƒ†ãƒƒãƒ—1: MLflowã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# MLflowã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
mlflow ui --host 0.0.0.0 --port 5000
```

ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:5000 ã‚’é–‹ã

### ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬çš„ãªå®Ÿé¨“è¿½è·¡

```python
# lesson5_mlflow_basic.py
import pandas as pd
from auto_model_factory import create_auto_model
import mlflow

# MLflowã®è¨­å®š
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("lesson5_basic")

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

print("ğŸ“Š Starting experiment with MLflow tracking...")

# MLflowã‚’æœ‰åŠ¹ã«ã—ã¦å®Ÿè¡Œ
auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    backend="optuna",
    num_samples=20,
    use_mlflow=True,  # MLflowæœ‰åŠ¹åŒ–
    mlflow_experiment_name="lesson5_basic",
    verbose=True
)

predictions = auto_model.predict(dataset=df)

print("\nâœ… Experiment complete!")
print("ğŸ“Š View results at http://localhost:5000")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²

```python
# lesson5_custom_metrics.py
import pandas as pd
from auto_model_factory import create_auto_model
import mlflow
import numpy as np

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("lesson5_custom_metrics")

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¨ˆç®—
dataset_info = {
    'n_rows': len(df),
    'n_series': df['unique_id'].nunique(),
    'date_range_days': (df['ds'].max() - df['ds'].min()).days,
    'mean_value': df['y'].mean(),
    'std_value': df['y'].std()
}

print("ğŸ“Š Dataset Information:")
for key, value in dataset_info.items():
    print(f"  {key}: {value}")

# å®Ÿé¨“ã‚’å®Ÿè¡Œ
with mlflow.start_run(run_name="custom_metrics_run"):
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¨˜éŒ²
    mlflow.log_params(dataset_info)
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,
        backend="optuna",
        num_samples=20,
        use_mlflow=False,  # æ‰‹å‹•ã§MLflowç®¡ç†
        verbose=False
    )
    
    predictions = auto_model.predict(dataset=df)
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
    pred_mean = predictions['NHITS'].mean()
    pred_std = predictions['NHITS'].std()
    
    mlflow.log_metrics({
        'prediction_mean': pred_mean,
        'prediction_std': pred_std,
        'prediction_range': predictions['NHITS'].max() - predictions['NHITS'].min()
    })
    
    # äºˆæ¸¬çµæœã‚’ä¿å­˜
    predictions.to_csv('predictions_custom.csv', index=False)
    mlflow.log_artifact('predictions_custom.csv')
    
    print("\nâœ… Custom metrics logged to MLflow!")
```

### ã‚¹ãƒ†ãƒƒãƒ—4: è¤‡æ•°å®Ÿé¨“ã®æ¯”è¼ƒ

```python
# lesson5_compare_experiments.py
import pandas as pd
from auto_model_factory import create_auto_model
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("lesson5_comparison")

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ç•°ãªã‚‹è¨­å®šã§3ã¤ã®å®Ÿé¨“
configurations = [
    {"name": "fast", "num_samples": 10, "model": "MLP"},
    {"name": "balanced", "num_samples": 20, "model": "NHITS"},
    {"name": "accurate", "num_samples": 30, "model": "TFT"}
]

print("ğŸ”¬ Running multiple experiments...\n")

for config in configurations:
    print(f"Running: {config['name']}")
    
    with mlflow.start_run(run_name=config['name']):
        # è¨­å®šã‚’è¨˜éŒ²
        mlflow.log_params(config)
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        auto_model = create_auto_model(
            model_name=config['model'],
            h=7,
            dataset=df,
            backend="optuna",
            num_samples=config['num_samples'],
            use_mlflow=False,
            verbose=False
        )
        
        predictions = auto_model.predict(dataset=df)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
        mlflow.log_metric("mean_prediction", predictions[config['model']].mean())
        
        print(f"  âœ… {config['name']} complete\n")

print("ğŸ“Š Compare results at http://localhost:5000")
print("   Navigate to the 'lesson5_comparison' experiment")
print("   Select runs and click 'Compare'")
```

### ã‚¹ãƒ†ãƒƒãƒ—5: å®Ÿé¨“çµæœã®åˆ†æ

```python
# lesson5_analyze_results.py
import mlflow
import pandas as pd

mlflow.set_tracking_uri("http://localhost:5000")

# å®Ÿé¨“ã®å–å¾—
experiment = mlflow.get_experiment_by_name("lesson5_comparison")

if experiment:
    # å®Ÿé¨“ã®ã™ã¹ã¦ã®runã‚’å–å¾—
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["start_time DESC"]
    )
    
    print("ğŸ“Š Experiment Results Analysis\n")
    print("=" * 80)
    
    # åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
    print(f"Experiment: {experiment.name}")
    print(f"Total runs: {len(runs)}")
    print(f"Experiment ID: {experiment.experiment_id}")
    
    # runã®è©³ç´°
    print("\n" + "=" * 80)
    print("Run Details:")
    print("=" * 80)
    
    for idx, run in runs.iterrows():
        print(f"\nRun: {run['tags.mlflow.runName']}")
        print(f"  Status: {run['status']}")
        print(f"  Duration: {run['end_time'] - run['start_time']}")
        print(f"  Parameters:")
        for col in runs.columns:
            if col.startswith('params.'):
                param_name = col.replace('params.', '')
                print(f"    {param_name}: {run[col]}")
        print(f"  Metrics:")
        for col in runs.columns:
            if col.startswith('metrics.'):
                metric_name = col.replace('metrics.', '')
                if pd.notna(run[col]):
                    print(f"    {metric_name}: {run[col]:.4f}")
    
    # ãƒ™ã‚¹ãƒˆrunã®ç‰¹å®š
    print("\n" + "=" * 80)
    print("Best Run:")
    print("=" * 80)
    
    # mean_predictionãŒå­˜åœ¨ã™ã‚‹å ´åˆ
    if 'metrics.mean_prediction' in runs.columns:
        best_idx = runs['metrics.mean_prediction'].idxmax()
        best_run = runs.loc[best_idx]
        print(f"Run Name: {best_run['tags.mlflow.runName']}")
        print(f"Mean Prediction: {best_run['metrics.mean_prediction']:.4f}")

else:
    print("âŒ Experiment not found. Run lesson5_compare_experiments.py first.")
```

### æ¼”ç¿’

1. MLflow UIã§å®Ÿé¨“çµæœã‚’å¯è¦–åŒ–
2. è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¦ã€ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
3. ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ ã—ã¦ã€ç‹¬è‡ªã®è©•ä¾¡åŸºæº–ã‚’å®Ÿè£…

---

## ãƒ¬ãƒƒã‚¹ãƒ³6: é«˜åº¦ãªæœ€é©åŒ–

**ç›®æ¨™**: é«˜åº¦ãªæ©Ÿèƒ½ã‚’ä½¿ã„ã“ãªã™

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨

```python
# lesson6_factory_class.py
import pandas as pd
from auto_model_factory import AutoModelFactory, OptimizationConfig

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# è©³ç´°ãªæœ€é©åŒ–è¨­å®š
opt_config = OptimizationConfig(
    backend="optuna",
    num_samples=None,  # è‡ªå‹•æ¨å¥¨
    cpus=4,
    gpus=1,
    use_mlflow=True,
    mlflow_tracking_uri="http://localhost:5000",
    mlflow_experiment_name="lesson6_advanced",
    use_pruning=True,  # æ—©æœŸåœæ­¢ã‚’æœ‰åŠ¹åŒ–
    time_budget_hours=0.5,  # 30åˆ†ã®æ™‚é–“åˆ¶é™
    random_seed=42,
    verbose=True
)

print("ğŸ­ Creating factory with advanced configuration...")
print(f"  Backend: {opt_config.backend}")
print(f"  Time budget: {opt_config.time_budget_hours} hours")
print(f"  Pruning enabled: {opt_config.use_pruning}")

# ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ä½œæˆ
factory = AutoModelFactory(
    model_name="TFT",
    h=7,
    optimization_config=opt_config
)

# æœ€é©åŒ–å®Ÿè¡Œ
print("\nğŸ” Starting optimization...")
auto_model = factory.create_auto_model(dataset=df)

# æœ€é©åŒ–å±¥æ­´ã®ç¢ºèª
print("\nğŸ“Š Optimization Summary:")
summary = factory.get_optimization_summary()
for key, value in summary.items():
    print(f"  {key}: {value}")

predictions = auto_model.predict(dataset=df)
print("\nâœ… Advanced optimization complete!")
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ã®ä½¿ç”¨

```python
# lesson6_custom_loss.py
import pandas as pd
from auto_model_factory import create_auto_model
from neuralforecast.losses.pytorch import MQLoss, MAE, MSE

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ç•°ãªã‚‹æå¤±é–¢æ•°ã§æ¯”è¼ƒ
loss_functions = {
    'MAE': MAE(),
    'MSE': MSE(),
    'MQ_90': MQLoss(level=[90])  # 90%ä¿¡é ¼åŒºé–“
}

results = {}

print("ğŸ”¬ Testing different loss functions...\n")

for loss_name, loss_fn in loss_functions.items():
    print(f"Testing {loss_name}...")
    
    auto_model = create_auto_model(
        model_name="NHITS",
        h=7,
        dataset=df,
        loss=loss_fn,  # ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°
        backend="optuna",
        num_samples=15,
        verbose=False
    )
    
    predictions = auto_model.predict(dataset=df)
    results[loss_name] = predictions
    
    print(f"  âœ… Complete\n")

print("ğŸ“Š Results:")
for loss_name, preds in results.items():
    mean_pred = preds['NHITS'].mean()
    print(f"  {loss_name:10s}: mean={mean_pred:.2f}")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–

```python
# lesson6_parallel_optimization.py
import pandas as pd
from auto_model_factory import create_auto_model
from ray import tune
import torch

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª
n_cpus = 8  # åˆ©ç”¨å¯èƒ½ãªCPUæ•°ã«å¿œã˜ã¦èª¿æ•´
n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0

print(f"ğŸ’» System Resources:")
print(f"  CPUs: {n_cpus}")
print(f"  GPUs: {n_gpus}")

# ä¸¦åˆ—å®Ÿè¡Œè¨­å®š
config = {
    'max_steps': tune.choice([500, 1000, 2000]),
    'learning_rate': tune.loguniform(1e-4, 1e-2),
    'batch_size': tune.choice([64, 128, 256])
}

print(f"\nğŸš€ Running parallel optimization...")
print(f"  Parallel trials: {min(n_cpus // 2, 4)}")

auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    config=config,
    backend="ray",  # Ray Tuneã¯ä¸¦åˆ—å®Ÿè¡Œã«å„ªã‚Œã¦ã„ã‚‹
    num_samples=40,
    cpus=n_cpus,
    gpus=n_gpus,
    verbose=True
)

predictions = auto_model.predict(dataset=df)
print("\nâœ… Parallel optimization complete!")
```

### ã‚¹ãƒ†ãƒƒãƒ—4: æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠ

```python
# lesson6_algorithm_selection.py
import pandas as pd
from auto_model_factory import AutoModelFactory, OptimizationConfig
from search_algorithm_selector import (
    SearchAlgorithmSelector,
    ModelComplexity,
    DatasetSize
)

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ
n_rows = len(df)
if n_rows < 10000:
    dataset_size = DatasetSize.SMALL
elif n_rows < 100000:
    dataset_size = DatasetSize.MEDIUM
else:
    dataset_size = DatasetSize.LARGE

print(f"ğŸ“Š Dataset size: {dataset_size.value} ({n_rows} rows)")

# ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦
model_complexity = ModelComplexity.COMPLEX  # TFTã‚’æƒ³å®š

# ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ
selector = SearchAlgorithmSelector(backend="optuna")
strategy = selector.select_algorithm(
    model_complexity=model_complexity,
    dataset_size=dataset_size,
    num_samples=50,
    use_pruning=True
)

print(f"\nğŸ¯ Selected Algorithm:")
print(f"  Name: {strategy.algorithm_name}")
print(f"  Description: {strategy.description}")
print(f"  Reason: {strategy.reason}")

# ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¨ãƒ—ãƒ«ãƒ¼ãƒŠãƒ¼ã‚’å–å¾—
sampler = selector.get_optuna_sampler(strategy)
pruner = selector.get_optuna_pruner(strategy)

print(f"\nâš™ï¸ Configuration:")
print(f"  Sampler: {type(sampler).__name__}")
print(f"  Pruner: {type(pruner).__name__ if pruner else 'None'}")

# æœ€é©åŒ–å®Ÿè¡Œ
opt_config = OptimizationConfig(
    backend="optuna",
    num_samples=50,
    use_pruning=True,
    verbose=True
)

factory = AutoModelFactory(
    model_name="TFT",
    h=7,
    optimization_config=opt_config
)

auto_model = factory.create_auto_model(dataset=df)
print("\nâœ… Optimization with selected algorithm complete!")
```

### æ¼”ç¿’

1. æ™‚é–“åˆ¶é™ã‚’è¨­å®šã—ã¦ã€åˆ¶é™å†…ã§ã®æœ€é©åŒ–ã‚’ä½“é¨“
2. ç•°ãªã‚‹æå¤±é–¢æ•°ã§äºˆæ¸¬çµæœãŒã©ã†å¤‰ã‚ã‚‹ã‹è¦³å¯Ÿ
3. ä¸¦åˆ—å®Ÿè¡Œã®åŠ¹æœã‚’æ¸¬å®šï¼ˆå®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒï¼‰

---

## ãƒ¬ãƒƒã‚¹ãƒ³7: æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹

**ç›®æ¨™**: æœ¬ç•ªç’°å¢ƒã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å­¦ã¶

### ã‚¹ãƒ†ãƒƒãƒ—1: å …ç‰¢ãªæ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
# lesson7_production_pipeline.py
import pandas as pd
from auto_model_factory import create_auto_model
from validation import validate_all, print_validation_results
import mlflow
import logging
from datetime import datetime

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'forecast_{datetime.now():%Y%m%d_%H%M%S}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def production_forecast_pipeline(data_path, model_name, h, num_samples):
    """æœ¬ç•ªç’°å¢ƒç”¨ã®äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    logger.info("=" * 80)
    logger.info("PRODUCTION FORECAST PIPELINE STARTED")
    logger.info("=" * 80)
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("Step 1: Loading data...")
        df = pd.read_csv(data_path)
        df['ds'] = pd.to_datetime(df['ds'])
        logger.info(f"  Loaded {len(df)} rows, {df['unique_id'].nunique()} series")
        
        # 2. åŒ…æ‹¬çš„ãªæ¤œè¨¼
        logger.info("\nStep 2: Validating configuration...")
        validation_results = validate_all(
            backend="optuna",
            config=None,
            num_samples=num_samples,
            cpus=4,
            gpus=1,
            model_class_name=model_name,
            dataset=df,
            h=h,
            strict_mode=True  # æœ¬ç•ªç’°å¢ƒã§ã¯å³æ ¼ãƒ¢ãƒ¼ãƒ‰
        )
        
        if not validation_results['overall_valid']:
            logger.error("âŒ Validation failed!")
            for category, result in validation_results.items():
                if isinstance(result, dict) and not result.get('is_valid', True):
                    logger.error(f"  {category}: {result.get('errors', [])}")
            return None
        
        logger.info("  âœ… All validations passed")
        
        # 3. MLflowè¨­å®š
        logger.info("\nStep 3: Setting up MLflow...")
        mlflow.set_tracking_uri("http://localhost:5000")
        experiment_name = f"production_{model_name}_{datetime.now():%Y%m%d}"
        mlflow.set_experiment(experiment_name)
        logger.info(f"  Experiment: {experiment_name}")
        
        # 4. æœ€é©åŒ–å®Ÿè¡Œ
        logger.info("\nStep 4: Running optimization...")
        with mlflow.start_run(run_name=f"forecast_{datetime.now():%H%M%S}"):
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            mlflow.log_params({
                'model_name': model_name,
                'forecast_horizon': h,
                'num_samples': num_samples,
                'data_rows': len(df),
                'n_series': df['unique_id'].nunique()
            })
            
            auto_model = create_auto_model(
                model_name=model_name,
                h=h,
                dataset=df,
                backend="optuna",
                num_samples=num_samples,
                use_mlflow=True,
                verbose=False
            )
            
            logger.info("  âœ… Optimization complete")
            
            # 5. äºˆæ¸¬å®Ÿè¡Œ
            logger.info("\nStep 5: Generating predictions...")
            predictions = auto_model.predict(dataset=df)
            
            # 6. çµæœä¿å­˜
            output_file = f'predictions_{model_name}_{datetime.now():%Y%m%d_%H%M%S}.csv'
            predictions.to_csv(output_file, index=False)
            mlflow.log_artifact(output_file)
            logger.info(f"  Saved to {output_file}")
            
            # 7. äºˆæ¸¬çµ±è¨ˆ
            pred_stats = {
                'mean': predictions[model_name].mean(),
                'std': predictions[model_name].std(),
                'min': predictions[model_name].min(),
                'max': predictions[model_name].max()
            }
            mlflow.log_metrics(pred_stats)
            
            logger.info("\nPrediction Statistics:")
            for key, value in pred_stats.items():
                logger.info(f"  {key}: {value:.2f}")
        
        logger.info("\n" + "=" * 80)
        logger.info("âœ… PIPELINE COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        
        return predictions
        
    except Exception as e:
        logger.error(f"\nâŒ Pipeline failed: {e}", exc_info=True)
        return None

# å®Ÿè¡Œ
if __name__ == "__main__":
    predictions = production_forecast_pipeline(
        data_path='sample_data.csv',
        model_name='NHITS',
        h=7,
        num_samples=30
    )
    
    if predictions is not None:
        print("\nâœ… Production pipeline successful!")
    else:
        print("\nâŒ Production pipeline failed!")
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

```python
# lesson7_model_versioning.py
import pandas as pd
from auto_model_factory import create_auto_model
import mlflow
import pickle
from datetime import datetime
import json

def save_model_version(auto_model, metadata, version_dir='models'):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã—ã¦ä¿å­˜"""
    import os
    os.makedirs(version_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    version_name = f"v_{timestamp}"
    version_path = os.path.join(version_dir, version_name)
    os.makedirs(version_path, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_file = os.path.join(version_path, 'model.pkl')
    with open(model_file, 'wb') as f:
        pickle.dump(auto_model, f)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata['version'] = version_name
    metadata['timestamp'] = timestamp
    metadata_file = os.path.join(version_path, 'metadata.json')
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… Model saved as version: {version_name}")
    print(f"   Location: {version_path}")
    
    return version_name, version_path

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

print("ğŸ”§ Creating and versioning model...\n")

auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    backend="optuna",
    num_samples=20,
    verbose=False
)

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
metadata = {
    'model_name': 'NHITS',
    'forecast_horizon': 7,
    'num_samples': 20,
    'backend': 'optuna',
    'training_data_rows': len(df),
    'n_series': df['unique_id'].nunique()
}

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¿å­˜
version_name, version_path = save_model_version(auto_model, metadata)

print(f"\nğŸ“¦ Model Version Info:")
with open(f'{version_path}/metadata.json', 'r') as f:
    metadata = json.load(f)
    for key, value in metadata.items():
        print(f"  {key}: {value}")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªã‚«ãƒãƒªãƒ¼

```python
# lesson7_error_handling.py
import pandas as pd
from auto_model_factory import create_auto_model
import mlflow
import logging
from typing import Optional

logger = logging.getLogger(__name__)

class ForecastError(Exception):
    """äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ã®ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–"""
    pass

def robust_forecast(
    df: pd.DataFrame,
    model_name: str,
    h: int,
    num_samples: int,
    fallback_model: Optional[str] = "MLP"
) -> pd.DataFrame:
    """
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’æŒã¤äºˆæ¸¬
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        model_name: ä¸»è¦ãƒ¢ãƒ‡ãƒ«å
        h: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚¾ãƒ³
        num_samples: è©¦è¡Œå›æ•°
        fallback_model: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«å
    
    Returns:
        äºˆæ¸¬çµæœ
    """
    
    # è©¦è¡Œ1: ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
    try:
        logger.info(f"Attempting forecast with {model_name}...")
        
        auto_model = create_auto_model(
            model_name=model_name,
            h=h,
            dataset=df,
            backend="optuna",
            num_samples=num_samples,
            verbose=False
        )
        
        predictions = auto_model.predict(dataset=df)
        logger.info(f"âœ… Forecast successful with {model_name}")
        return predictions
        
    except Exception as e:
        logger.warning(f"âš ï¸ {model_name} failed: {e}")
        
        # è©¦è¡Œ2: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
        if fallback_model:
            try:
                logger.info(f"Trying fallback model {fallback_model}...")
                
                auto_model = create_auto_model(
                    model_name=fallback_model,
                    h=h,
                    dataset=df,
                    backend="optuna",
                    num_samples=max(10, num_samples // 2),  # åŠåˆ†ã®è©¦è¡Œå›æ•°
                    verbose=False
                )
                
                predictions = auto_model.predict(dataset=df)
                logger.info(f"âœ… Forecast successful with fallback {fallback_model}")
                return predictions
                
            except Exception as e2:
                logger.error(f"âŒ Fallback {fallback_model} also failed: {e2}")
                raise ForecastError(f"Both {model_name} and {fallback_model} failed")
        else:
            raise ForecastError(f"{model_name} failed and no fallback specified")

# ä½¿ç”¨ä¾‹
logging.basicConfig(level=logging.INFO)

df = pd.read_csv('sample_data.csv')
df['ds'] = pd.to_datetime(df['ds'])

print("ğŸ›¡ï¸ Testing robust forecast with error handling...\n")

try:
    predictions = robust_forecast(
        df=df,
        model_name="TFT",  # è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ï¼ˆå¤±æ•—ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
        h=7,
        num_samples=20,
        fallback_model="NHITS"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    )
    print("\nâœ… Forecast completed successfully!")
    print(predictions.head())
    
except ForecastError as e:
    print(f"\nâŒ Forecast failed: {e}")
```

### ã‚¹ãƒ†ãƒƒãƒ—4: å®šæœŸå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# lesson7_scheduled_forecast.py
import pandas as pd
from auto_model_factory import create_auto_model
import mlflow
from datetime import datetime, timedelta
import logging
import schedule
import time

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def daily_forecast_job():
    """æ¯æ—¥å®Ÿè¡Œã•ã‚Œã‚‹äºˆæ¸¬ã‚¸ãƒ§ãƒ–"""
    
    logger.info("=" * 80)
    logger.info(f"DAILY FORECAST JOB STARTED: {datetime.now()}")
    logger.info("=" * 80)
    
    try:
        # 1. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv('sample_data.csv')
        df['ds'] = pd.to_datetime(df['ds'])
        
        # éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        cutoff_date = datetime.now() - timedelta(days=30)
        df = df[df['ds'] >= cutoff_date]
        
        logger.info(f"Using data from {cutoff_date.date()} onwards")
        logger.info(f"Total rows: {len(df)}")
        
        # 2. äºˆæ¸¬å®Ÿè¡Œ
        mlflow.set_experiment("daily_forecast")
        
        with mlflow.start_run(run_name=f"daily_{datetime.now():%Y%m%d}"):
            auto_model = create_auto_model(
                model_name="NHITS",
                h=7,
                dataset=df,
                backend="optuna",
                num_samples=20,
                use_mlflow=True,
                verbose=False
            )
            
            predictions = auto_model.predict(dataset=df)
            
            # 3. çµæœä¿å­˜
            output_file = f'daily_forecast_{datetime.now():%Y%m%d}.csv'
            predictions.to_csv(output_file, index=False)
            mlflow.log_artifact(output_file)
            
            logger.info(f"Forecast saved to {output_file}")
        
        logger.info("âœ… Daily forecast job completed successfully")
        
    except Exception as e:
        logger.error(f"âŒ Daily forecast job failed: {e}", exc_info=True)
    
    logger.info("=" * 80 + "\n")

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
def setup_scheduler():
    """å®šæœŸå®Ÿè¡Œã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
    
    # æ¯æ—¥åˆå‰2æ™‚ã«å®Ÿè¡Œ
    schedule.every().day.at("02:00").do(daily_forecast_job)
    
    # ãƒ†ã‚¹ãƒˆç”¨: 1åˆ†ã”ã¨ã«å®Ÿè¡Œï¼ˆæœ¬ç•ªã§ã¯å‰Šé™¤ï¼‰
    # schedule.every(1).minutes.do(daily_forecast_job)
    
    logger.info("ğŸ“… Scheduler configured:")
    logger.info("  Daily forecast at 02:00")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ãƒ«ãƒ¼ãƒ—
    while True:
        schedule.run_pending()
        time.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯

if __name__ == "__main__":
    print("ğŸ• Starting forecast scheduler...")
    print("   Press Ctrl+C to stop")
    
    try:
        # åˆå›å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        print("\nğŸ§ª Running initial test forecast...")
        daily_forecast_job()
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼èµ·å‹•
        # setup_scheduler()  # æœ¬ç•ªç’°å¢ƒã§ã¯æœ‰åŠ¹åŒ–
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Scheduler stopped by user")
```

### æ¼”ç¿’

1. æœ¬ç•ªç’°å¢ƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¦ã€ã™ã¹ã¦ã®æ®µéšã‚’ç¢ºèª
2. æ„å›³çš„ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã¦ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆ
3. ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã£ã¦è¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç®¡ç†

---

## ğŸ’¡ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ç·ã¾ã¨ã‚

### 1. é–‹ç™ºãƒ•ã‚§ãƒ¼ã‚º

```python
# é–‹ç™ºæ™‚ã¯å°‘ãªã„è©¦è¡Œå›æ•°ã§ç´ æ—©ãåå¾©
auto_model = create_auto_model(
    model_name="NHITS",
    h=7,
    dataset=df,
    num_samples=5,  # å°‘ãªã‚
    verbose=True    # è©³ç´°å‡ºåŠ›
)
```

### 2. å®Ÿé¨“ãƒ•ã‚§ãƒ¼ã‚º

```python
# è¤‡æ•°è¨­å®šã‚’è©¦ã—ã¦æœ€é©ãªçµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢
opt_config = OptimizationConfig(
    backend="optuna",
    num_samples=50,
    use_mlflow=True,
    mlflow_experiment_name="experiment_phase"
)
```

### 3. æœ¬ç•ªãƒ•ã‚§ãƒ¼ã‚º

```python
# å …ç‰¢æ€§ã¨å†ç¾æ€§ã‚’é‡è¦–
opt_config = OptimizationConfig(
    backend="optuna",
    num_samples=30,
    use_mlflow=True,
    use_pruning=True,
    random_seed=42,  # å†ç¾æ€§
    strict_mode=True
)
```

---

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å®Œäº†ã—ãŸã‚‰:

1. **[API_REFERENCE.md](./API_REFERENCE.md)** ã§è©³ç´°ãªAPIã‚’ç¢ºèª
2. **[TROUBLESHOOTING.md](./TROUBLESHOOTING.md)** ã§ã‚ˆãã‚ã‚‹å•é¡Œã®è§£æ±ºæ–¹æ³•ã‚’å­¦ç¿’
3. **è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ** ã§å®Ÿéš›ã«ä½¿ç”¨

---

## â“ ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰

### Q1: æœ€åˆã«è©¦ã™ã¹ããƒ¢ãƒ‡ãƒ«ã¯ï¼Ÿ

**A**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã€œä¸­è¦æ¨¡ãªã‚‰**NHITS**ã€å¤§è¦æ¨¡ãªã‚‰**DLinear**ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

### Q2: num_samplesã¯ã„ãã¤ã«ã™ã¹ãï¼Ÿ

**A**: 
- ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ: 5-10
- é€šå¸¸ã®æœ€é©åŒ–: 20-50
- å¾¹åº•çš„ãªæœ€é©åŒ–: 50-100

### Q3: GPUã¯å¿…é ˆï¼Ÿ

**A**: å¿…é ˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€å¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚ç‰¹ã«è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ï¼ˆTFTã€Transformerãªã©ï¼‰ã§ã¯GPUä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

### Q4: ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã©ã†ã™ã‚Œã°ï¼Ÿ

**A**: ä»¥ä¸‹ã®é †ã§ç¢ºèªã—ã¦ãã ã•ã„ï¼š
1. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèª
2. [TROUBLESHOOTING.md](./TROUBLESHOOTING.md)ã§è©²å½“ã™ã‚‹å•é¡Œã‚’æ¢ã™
3. æ¤œè¨¼æ©Ÿèƒ½ã‚’ä½¿ã£ã¦å•é¡Œã‚’ç‰¹å®š

### Q5: ã©ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ã†ã¹ãï¼Ÿ

**A**:
- **Optuna**: ä¸€èˆ¬çš„ãªä½¿ç”¨ã€ä½¿ã„ã‚„ã™ã„
- **Ray Tune**: ä¸¦åˆ—å®Ÿè¡Œã€å¤§è¦æ¨¡ãªå®Ÿé¨“

---

## ğŸ‰ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å®Œäº†ã—ã¾ã—ãŸï¼ã“ã‚Œã§è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã„ã“ãªã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
1. è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§è©¦ã™
2. ã‚ˆã‚Šé«˜åº¦ãªæ©Ÿèƒ½ã‚’æ¢ç´¢
3. ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«è²¢çŒ®

**Happy Forecasting! ğŸš€ğŸ“ˆ**
